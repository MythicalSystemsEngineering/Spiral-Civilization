* 92f7f0a5 ggml: add `conv3d` op (#15182)
* b1ab9182 cuda : add Pad Reflect 1D support (#14659)
| * 44933f76 kv-cache : remove LLAMA_SET_ROWS checks
| * 2c2fbbd6 cont : update comments [no ci]
| * d6d5e953 kv-cache : support layer reuse
|/  
* 9ebebef6 llama : remove KV cache defragmentation logic (#15473)
* ad5c975c ggml-cpu: Support Q5_0 and Q5_1 on s390x (#15486)
* 4afb0a74 server : Support multimodal completion and embeddings prompts in JSON format (#15108)
* e2886936 readme : model : mtdm : lfm2 improvements (#15476)
* a0f98dd6 CANN: Optimize RMS_NORM using cache (#15419)
* 54a241f5 sched : fix possible use of wrong ids tensor when offloading moe prompt processing (#15488)
* cd36b5e5 llama : remove deprecated llama_kv_self API (#15472)
* 3f196be8 graph : remove build_attn_with_sinks overload (#15469)
| * 730ba007 vulkan: use prealloc intermediate reuse for mmvq path
| * fe1fe9e7 vulkan: allow all subgroup modes for mmv and mmvq
| * b034ed6e vulkan: adapt integer dot mmv to mmv small m optimization (#15355)
| * cf2ef805 vulkan: fix quantizing issue when tensor is not divisible by 128
| * 16b6caa0 vulkan: tune mul_mat_vecq performance for Intel
| * 216acb55 vulkan: do 8 calculations per invocation instead of 32 in mul_mat_vecq, similar to mul_mat_vec
| * d77f3e4b vulkan: use q8_1_x4 blocks in mul_mmq shader
| * 05843b61 vulkan: add q8_1_x4 type with 128-bit alignment, use in mul_mat_vecq shader
| * 33307617 vulkan: use subgroup operations for quantize_q8_1 shader
| * cb006c87 vulkan: Add Integer Dot Product mul_mat_vec shader for legacy quants
|/  
* 97ae5961 vulkan : support conv_2d_dw with f16 weights (#15392)
* 20c2dac8 vulkan: add exp operation (#15456)
* 96452a3f vulkan: Reuse conversion results in prealloc_y (#15410)
* 9ad5e60d examples : fix some typos in examples/model-conversion/README.md (#15477)
* 715a6db0 kv-cache : drop the "unified" prefix (#15467)
* ad294df0 examples : install torch-cpu for model conversion tool/example (#15475)
* 029bb39e ci : enable RVV1.0 native build (#15386)
* 30649cab ci : continue file download with wget (#15471)
* 2758fa10 examples : add model conversion tool/example (#15455)
* b108e429 ci : fix -Werror=return-type in clip.cpp so ci/run.sh can run without issue (#15221)
* 245be739 ci : add copilot-instructions.md (#15286)
* b2caf67d convert : make Mistral community chat templates optional via parameter (#15420)
* 2f3dbffb common : fix incorrect print of non-ascii characters in the logging (#15466)
* 945e1f12 ggml : fix condition of im2col on Metal backend (#15460)
* 1b0db8f6 server : fix webui (#15462)
* 29f538ac examples : remove references to `make` in examples [no ci] (#15457)
* 8ad038c0 musa: add GGML_UNUSED_VARS (#15446)
* 5682a374 sched : copy only the used experts when offloading prompt processing (#15346)
* 1bc664a2 server: fix OpenAI API compatibility for usage statistics in chat streams (#15444)
* 13aeb7ae CUDA: refactor FA support/selection code (#15454)
* 7a6e91ad CUDA: replace GGML_CUDA_F16 with CUDA arch checks (#15433)
* fec95198 vulkan: shorten pipeline name strings (#15431)
* 657b8a77 chat: handle gpt-oss return/end token inconsistency (#15421)
* ec5ab1a3 common : fix context shift help message (#15448)
* 1a99c2d9 cmake : fix target include directories (#15450)
* 37f10f95 make : remove make in favor of CMake (#15449)
* 2f370140 lookahead : add sample command to readme (#15447)
* a094f381 musa: fix build warnings (#15258)
* fb22dd07 opencl: mark `argsort` unsupported if cols exceed workgroup limit (#15375)
* 9ef6b0b8 model : add gpt-oss type strings (#15424)
* 1e19f5d4 common : Add top-nsigma sampler to help globally (#15428)
* d2fcd91c server : disable context shift by default (#15416)
* a6d3cfe7 CANN: optimize rope operator (#15335)
* 67f09a3a musa: handle __hgt2_mask, available starting from MUSA SDK rc4.3.0 (#15413)
* 6424594c ggml-cpu: add mxfp4 VSX intrinsics for Power9+ (ppc64le) hardware (#15385)
* e9288e88 chat : clarify the meaning of reasoning_format (#15408)
* 9d262f4b server : remove swa_full warning (#15399)
* f0d3c740 batched-bench : use rand tokens (#15398)
| * 89939827 convert : fix conversion from FP8 for Deepseek-V3.1-Base
| *   1ae6ab76 Merge branch 'master' into compilade/convert-prequant
| |\  
| * | de12f8ac convert : begin handling pre-quantized models
| | | * a178e442 add link to this PR
| | | * 58447922 chat : clarify the meaning of reasoning_format
| |_|/  
|/| |   
* | | f08c4c0d mtmd : clean up clip_n_output_tokens (#15391)
* | | 6d7f1117 codeowners : remove mmv.*
* | | 60212f1e sync : ggml
* | | f0c541d3 scripts : update sync scripts
* | | baa9255a llama : merge conts and reshapes and remove unnecessary cont (#15380)
* | | 3007baf2 readme : update hot topics (#15397)
* | | d1d82416 server : fix incoming tasks not process in order (#15395)
* | | 618575c5 Fix broken build: require updated pip to support --break-system-packages (#15357)
* | | f44f7931 ggml-quants : fix make_qp_quants NANs and IQ1 assertion errors (#15379)
* | | ae532eac vulkan: disable spirv-opt for bfloat16 shaders (#15352)
* | | e5155e69 server : export max observed n_past value (#15361)
* | | 21c17b5b vulkan: Use larger workgroups for mul_mat_vec when M is small (#15355)
| | | * fb573f44 ggml-quants : avoid division by zero in make_q3_quants
| | | * 184cdc6b ggml-quants : fix make_qp_quants NANs and IQ1 assertion errors
| |_|/  
|/| |   
* | | 19f4deca vulkan: support sqrt (#15370)
* | | 4d196981 convert : force patch_embd weights to F16 or F32 to avoid broken GGUFs (#15367)
* | | b143fbc8 ci : fix hang in windows-hip build/release (#15365)
* | | de562791 vulkan: Optimize argsort (#15354)
* | | 65349f26 model : support vision LiquidAI LFM2-VL family (#15347)
| | | *   43955380 Merge branch 'master' into cisc/jina-embeddings-v3
| | | |\  
| |_|_|/  
|/| | |   
* | | | 1fe00296 vulkan: fuse adds (#15252)
* | | | de219279 vulkan: Support mul_mat_id with f32 accumulators (#15337)
* | | | 2e2b22ba vulkan: Add missing bounds checking to scalar/coopmat1 mul_mat_id (#15334)
* | | | 912ff8c1 OpenCL: add initial FA support (#14987)
* | | | 5e6229a8 common : fix double bos, use common_chat_templates for add_bos and add_eos (#15326)
* | | | e2c1bfff opencl: add initial mxfp4 support via mv (#15270)
* | | | 5edf1592 vulkan : fix out-of-bounds access in argmax kernel (#15342)
* | | | db3010bd vulkan : fix compile warnings on macos (#15340)
* | | | ff27f80a ggml: initial IBM zDNN backend (#14975)
* | | | d3248d9b ci : fix ios-xcode-build (#15324)
* | | | 7aeee88c ci : move ccache action to ggml-org fork (#15328)
* | | | b07791aa test-opt: fix backend support check (#15317)
* | | | 4227c9be CUDA: fix negative KV_max values in FA (#15321)
| |/ /  
|/| |   
* | | df36bce6 eval-callback : stop on first NaN (#15320)
* | | f75b8306 chat : include kwargs in template example (#15309)
* | | 7a0de960 llama : add 18-layer model type for Gemma 3-270m (#15319)
* | | e4e91591 devops : fix compile bug when the BASE_CUDA_DEV_CONTAINER is based on Ubuntu 24.04 (#15005)
* | | 5ba36f61 HIP: Cleanup hipification header (#15285)
* | | b204a5a2 gpt-oss: implement harmony parsing (#15181)
* | | 646944cf docker : Enable GGML_CPU_ALL_VARIANTS for ARM (#15267)
* | | 1a01899b readme : update hot topics (#15315)
* | | 863d341e vulkan: perf_logger improvements (#15246)
| | * 1cda21e4 apply suggestions from review
| | * d98c7d9e fix assert
| | *   80094033 Merge branch 'master' into cisc/jina-embeddings-v3
| | |\  
| | |/  
| |/|   
| | * e092776f convert_hf_to_lora compatibility
| | * 9b67ea26 use std::string
| | * bea6d061 add adapter metadata api
| | * 966d0e0e export separate lora ggufs instead
| | * 9a39ccb7 add lora embedding and loading (non-functional)
| | *   8ab8d363 Merge branch 'master' into cisc/jina-embeddings-v3
| | |\  
| | * | 3862d954 rope
| | * | f5d0305d merge tensor loading into general bert
| | * |   40467013 Merge branch 'master' into cisc/jina-embeddings-v3
| | |\ \  
| | * | | b17e9811 revert vocab_size() change [no ci]
| | * | | f2d876ad additional unk_token_id fallback just in case [no ci]
| | * | | 65a37fa8 set mask token lstrip attribute
| | * | | 1274c8c3 fix vocab parsing with only tokenizer.json
| | * | | 4ac13805 initial jina-embeddings-v3 support
| | * | | ba51f891 initial jina-embeddings-v3 support
| | * | | 6303ea26 initial jina-embeddings-v3 support
| | | | | * 220860aa graph : use F32 accumulators for gpt-oss
| |_|_|_|/  
|/| | | |   
* | | | | d32e03f4 server : add SWA checkpoints (#15293)
* | | | | 3973163b sync : ggml
* | | | | 5ade3000 ggml: fix ggml_conv_1d_dw bug (ggml/1323)
* | | | | 8b248373 tests : remove unused includes (ggml/0)
* | | | | 810b9fc8 perplexity : provide a helpful hint for has_cpl case in split_equal error. (#15304)
* | | | | 4ebd0c12 cuda : fix GGML_CUDA_GRAPHS=OFF (#15300)
* | | | | 5cdb27e0 finetune: SGD optimizer, more CLI args (#13873)
* | | | | 3ea913f1 perplexity: give more information about constraints on failure (#15303)
* | | | | 29c8fbe4 HIP: bump requirement to rocm 6.1 (#15296)
* | | | | 1adc9812 fix(nix): remove non-functional llama-cpp cachix cache from flake.nix (#15295)
* | | | | b3e16665 server : enable -td and -tbd parameters (#15172)
* | | | | c24f4e26 ggml : update `ggml_rope_multi` (#12665)
* | | | | d8914fc4  common : add --override-tensor-draft, --cpu-moe-draft and --n-cpu-moe-draft parameters (#15191)
* | | | | e885445b server : filter out harmony thought messages (#15278)
* | | | | 648ebcdb ci : Added CI with RISC-V RVV1.0 Hardware (#14439)
* | | | | 07aa869a ci : add more python requirements to copilot-setup-steps (#15289)
* | | | | 00f35d50 ggml : repack block_iq4_nlx8 (#14904)
* | | | | 6028bf74 CUDA: Optimize `reduce_rows_f32` kernel, leading up to 25x perf improvement on kernel-level and 10% perf increase for Gemma3n (#15132)
* | | | | bc518227 ci : add copilot-setup-steps.yml (#15214)
* | | | | e71d48e3 ggml-rpc: chunk send()/recv() to avoid EINVAL for very large tensors over RPC (macOS & others) (#15188)
* | | | | b0493156 HIP: disable sync warp shuffel operators from clr amd_warp_sync_functions.h (#15273)
* | | | | f4586ee5 sycl: Fix and disable more configurations of mul_mat (#15151)
* | | | | 60a76588 opencl: allow mixed f16/f32 `add` (#15140)
* | | | | efe3a909 CUDA cmake: add `-lineinfo` for easier debug (#15260)
* | | | | bbd57b7e CANN: GGML_OP_CPY optimization (#15070)
* | | | | 25ff6f76 musa: fix failures in test-backend-ops for mul_mat_id op (#15236)
| | | | | * d9b625ed ggml-quants : handle imatrix for MXFP4
| |_|_|_|/  
|/| | | |   
* | | | | be48528b CANN: Add broadcast for softmax and FA (#15208)
* | | | | cf9e5648 mtmd : Fix MinicpmV model converter and clip to avoid using hardcode. (#14750)
* | | | | fba5c0d6 chat : hotfix gpt-oss jinja raising an exception (#15243)
| | | | | * 94d8042e fix
| | | | | * 7cbeb4e2 chat : hotfix gpt-oss jinja raising an exception
| |_|_|_|/  
|/| | | |   
* | | | | 53d0a126 server : allow specifying reasoning_format in HTTP request (#15238)
* | | | | 27093afe readme : update infra list (#15234)
* | | | | 228f724d kv-cache : fix seq_rm with seq_id == -1 (#15226)
* | | | | cd3069df kv-cache : log (debug) all streams in find_slot (#15176)
* | | | | 50e81bdf convert : fix merge conflicts (#15229)
* | | | | 1ebbaddf perplexity : update comments/error msg to use decode [no ci] (#15227)
* | | | | a3a78742 convert : improve Mistral models integration (#14737)
* | | | | 002cb1bb kleidiai: fix unsigned overflow bug (#15150)
* | | | | 79c1160b cuda: refactored ssm_scan and use CUB (#13291)
* | | | | 34c9d765 CUDA: add attention sinks for tile and wmma (#15178)
* | | | | e54d41be gguf-py : add Numpy MXFP4 de/quantization support (#15111)
* | | | | 4850b52a server-bench: external OAI servers, sqlite (#15179)
* | | | | cd6983d5 ggml : fix field name when new ggml_backend (#14944)
* | | | | 6c7e9a54 vendor: sync minja (#15161)
* | | | | 1425f587 CUDA: attention sinks for mma FlashAttention (#15157)
* | | | | aaa3d07a opencl: support sink in `soft_max` (attn sinks) (#15152)
* | | | | 50aa9389 convert : support non-mxfp4 HF model (#15153)
* | | | | c4f53563 vulkan: support fattn sinks (#15126)
* | | | | a0552c8b vulkan: Add env var to disable host visible vidmem (#15109)
* | | | | 99acbc99 llama : Support intern-s1 (#14875)
* | | | | 7ad67ba9 HIP: add cmake option to enable compiler output of kernel resource usage metrics (#15103)
* | | | | 9a963895 ggml: Skip backend library linking code when GGML_BACKEND_DL=ON (#15094)
* | | | | 1d72c841 CUDA: GEMM for FP32/FP16/BF16 and ne11 <= 16 (#15131)
* | | | | 20638e4f scripts: fix crash when --tool is not set (#15133)
* | | | | 36d3f00e requirements : fix PyTorch uint64 compatibility (#15134)
* | | | | 5fd160bb ggml: Add basic SET_ROWS support in WebGPU (#15137)
* | | | | 756cfea8 fix profiling crash (#15072)
* | | | | e725a1a9 opencl: add `swiglu_oai` and  `add_id` (#15121)
* | | | | 3db4da56 chat : support Granite model reasoning and tool call (#14864)
* | | | | 476aa3fd Fixed name `-override-tensors` to `-override-tensor` (#15129)
* | | | | 0d883154 ggml : fix fallback to CPU for ununsupported ops (#15118)
* | | | | 65c797c4 chat : fix yandex chat template (#15116)
* | | | | 25726898 chat : fix hunyuan auto-detection (#15114)
* | | | | 22414532 CANN: add support for ACL Graph (#15065)
* | | | | 9515c613 ggml: WebGPU disable SET_ROWS for now (#15078)
| | | | | * 2763dc8b ggml-quants : handle zero amax for MXFP4
| | | | | * 141cab13 gguf-py : add MXFP4 de/quantization support
| |_|_|_|/  
|/| | | |   
* | | | | fd1234cb llama : add gpt-oss (#15091)
* | | | | f324a3b7 chat : only remove double bos/eos if added (#15086)
| | | | | *   ea5e55d0 Merge branch 'master' into compilade/imatrix-neutral-prior
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | be426425 readme : update hot topics (#15097)
* | | | | | 3306ceab sycl: fix mul_mat selection (#15092)
* | | | | | c81de6e1 Fix `glm4moe` bug (#15088)
* | | | | | 22f060c9 webui: fix markdown table (#15081)
* | | | | | ee3a9fcf context : fix index overflow on huge outputs (#15080)
| | | | | * 46a86011 quantize : assume the neutral prior is equal imatrix weights
| | | | | * 92383bfa quantize : store metadata for prior weight used for imatrix
| | | | | * 0416ed2b quantize : configurable neutral imatrix prior
| | | | | | * 2ec70c96 tests: Fix OPT_STEP_SGD test-backend-ops
| | | | | | * 9d031242 Vulkan: Implement GGML_OP_OPT_STEP_SGD
| | | | | | *   50e83eae Merge branch 'master' into finelayer
| | | | | | |\  
| | | | | | * | bc39aa67 examples/finetune -opt SGD (stochastic gradient descent) memory opt
| | | | | | | | * 145401c9 context : fix logits size overflow for huge batches
| | | | | | | | * f16a843a context : fix overflow when re-ordering huge outputs
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | ec428b02 llama : add --n-cpu-moe option (#15077)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 19f68fa5 imatrix : warn when GGUF imatrix is saved without .gguf suffix (#15076)
* | | | | | | 41613437 cmake: Add GGML_BACKEND_DIR option (#15074)
* | | | | | | e5bebe52 gguf-py : add --chat-template-file to gguf_new_metadata (#15075)
* | | | | | | ef0144c0 model: support GLM 4.5 family of models (#14939)
| | | | | | | * 342e7014 imatrix : only warn about suffix when output format is unspecified
| | | | | | | * afa43e13 imatrix : add warning when suffix is not .gguf for GGUF imatrix
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 2721257e quantize : fix confusing error message if ftype is invalid (#15071)
* | | | | | | 587d0118 ggml: WebGPU backend host improvements and style fixing (#14978)
* | | | | | | 5aa1105d vulkan: fix build when using glslang that does not support coopmat2 (#15062)
* | | | | | | d31192b4 imatrix : use GGUF by default (#14842)
| |_|_|_|/ /  
|/| | | | |   
* | | | | | 0a2f5496 imatrix : fix 3d activation handling for hybrid and recurrent models (#14994)
* | | | | | 11a38111 memory : handle kv_unified for hybrid models (#15050)
* | | | | | 97366dc6 vocab : JetBrains Mellum pre-tokenizer (#15045)
* | | | | | 83bc2f28 model : add text-only support for Kimi-VL (and find special tokens in text_config)  (#15051)
* | | | | | 6c7a4411 vulkan: Use coopmat2 for conv2d (#14982)
| | | | | | * e549515c memory : handle kv_unified for hybrid models
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 5c0eb5ef opencl: fix adreno compiler detection logic (#15029)
* | | | | | 03d46982 CUDA: use mma FA kernel for gqa > 4 on RTX 4000 (#15035)
* | | | | | 3303c19b cuda: make im2col a little faster (#15025)
* | | | | | 4fdea540 kv-cache : skip alignment of n_stream in kv-cache log msg [no ci] (#15040)
* | | | | | a4569c41 llama : enable LLAMA_SET_ROWS=1 by default (#14959)
* | | | | | 15e92fd3 cuda, sycl : fix batched gemm when ne02 == 1 && ne03 > 1 (#15038)
* | | | | | 2bf3fbf0 ci : check that pre-tokenizer hashes are up-to-date (#15032)
* | | | | | 711d5e6f convert : fix Qwen3-Embedding pre-tokenizer hash (#15030)
* | | | | | f738989d chat : fix multiple tool_calls on hermes-2-pro (#14962)
* | | | | | 4cb208c9 vulkan: coopmat2 mul_mat optimizations (#14934)
* | | | | | 3025b621 llama-bench: rename DB table name from test to llama_bench (#15003)
* | | | | | ec0b1880 vulkan: Support ne[3]>1 in noncontig matrix-vector multiply (#15015)
* | | | | | 339bd026 model : support Qwen3-Embedding (#15023)
* | | | | | f9062755 server: enable token array inputs for OAI API (#15001)
* | | | | | a9f7541e vulkan: optimizations for direct convolution (#14933)
* | | | | | 9c35706b CUDA: fix MMQ nwarps for AMD with warp_size==32 (#15014)
* | | | | | c76b420e vendor : update vendored copy of google/minja (#15011)
* | | | | | 0f5ccd6f model : add hunyuan dense (#14878)
* | | | | | 1c872f71 opencl: add f16 for `add`, `sub`, `mul`, `div` (#14984)
* | | | | | baad9488 ggml : Q2k interleaving implementation - x86/x64 SIMD (#14373)
* | | | | | ba42794c graph : fix equal_seq() check (#14986)
* | | | | | 2860d479 docker : add cann build pipline (#14591)
* | | | | | 484b2091 compare-commits.sh: support both llama-bench and test-backend-ops (#14392)
* | | | | | daf2dd78 quantize : skip tensor override when in fallback mode (#14995)
* | | | | | a06ed5fe llama : add simple option to enable CPU for MoE weights (--cpu-moe) (#14992)
* | | | | | 78452405 Fix params bug in diffusion example (#14993)
* | | | | | d6818d06 llama : allow other bufts when overriding to CPU, add --no-repack option (#14990)
* | | | | | e08a9882 Vulkan: Fix minor debug mode issues (#14899)
| | | | | | * 91e67b85 imatrix : fix 3d tensor counts
| | | | | | *   05beb070 Merge branch 'master' into compilade/imatrix-saner-3d
| | | | | | |\  
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 952a47f4 mtmd : support MiniCPM-V 4.0 (#14983)
* | | | | | | 36e5fe7b MODEL_TENSOR.SSM_DT_NORM has defined twice (#14991)
* | | | | | | 94933c8c server : implement universal assisted decoding (#12635)
* | | | | | | c1dacaa9 llama : merge build_moe_ffn_from_probs function into build_moe_ffn (#14968)
* | | | | | | a9f77a8b server : add openai-style logit_bias support (#14946)
* | | | | | | 8a4a8562 Add LLaDA 8b Diffusion model (#14771)
* | | | | | | 11490b36 CANN: Improve loading efficiency after converting weights to NZ format. (#14985)
* | | | | | | 66625a59 graph : reduce splits for recurrent and hybrid models (#14825)
* | | | | | | 6e672545 opencl: add `mul_mat_f32_f32_l4_lm` and `mul_mat_f16_f32_l4_lm` (#14809)
* | | | | | | e9192bec quantize : fix using combined imatrix GGUFs (multiple datasets) (#14973)
* | | | | | | 41e78c56 server : add support for `embd_normalize` parameter (#14964)
* | | | | | | ad4a7001 HIP: enable mfma mmq on gfx908 and gfx90a for select datatypes and shapes (#14949)
* | | | | | | e32a4ec6 sync : ggml
* | | | | | | e228de94 cmake : Fix BLAS link interface (ggml/1316)
* | | | | | | 73a8e5ca vulkan : fix 32-bit builds (ggml/1313)
* | | | | | | 92b8810e CUDA: skip masked KV slices for all FA kernels (#14924)
* | | | | | | 00131d6e tests : update for LLAMA_SET_ROWS=1 (#14961)
* | | | | | | 1e15bfd4 graph : fix stack-use-after-return (#14960)
* | | | | | | a118d802 embeddings: fix extraction of CLS pooling results (#14927)
* | | | | | | 61550f82 CANN: update ops docs (#14935)
* | | | | | | aa79524c HIP: remove the use of __HIP_PLATFORM_AMD__, explicitly support only AMD targets (#14945)
| | | | | | * d4f36e5e imatrix : fix 3d activations when model tensor is 2d
| | | | | | * 73439beb imatrix : use a single count for dense 3d tensors
| | | | | | | * b98f80a6 server : test alternative LRU logic
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | b77d1117 HIP: add GGML_HIP_MMQ_MFMA option to allow disableing the MFMA path. (#14930)
* | | | | | | c7aa1364 HIP: Ignore unsupported unroll transformation in fattn-vec (#14931)
* | | | | | | 1a67fcc3 common : avoid logging partial messages (which can contain broken UTF-8 sequences) (#14937)
* | | | | | | 204f2cf1 CANN: Add ggml_set_rows (#14943)
* | | | | | | 138b288b cuda : add softcap fusion (#14907)
* | | | | | | bbd0f917 server-bench: make seed choice configurable (#14929)
| | | | | | | * 0591b39e ops: add MUSA
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 0a5036be CUDA: add roll (#14919)
| | | | | | | * 381879e0 cont : tmp
| | | | | | | * 477d4398 repack : optimize mul_mat_id path
| | | | | | | * e2661edd ggml : repack block_iq4_nlx8
| | | | | | | | * fb371c18 bench,common : add CPU extra buffer types
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 8ad7b3e6 opencl : add ops docs (#14910)
* | | | | | | | bda62193 test-backend-ops : extend test case filtering (#14865)
* | | | | | | | c556418b llama-bench : use local GPUs along with RPC servers (#14917)
* | | | | | | | db16e283 ggml-cpu : deduplicate scalar implementations (#14897)
* | | | | | | | cd1fce6d SYCL: Add set_rows support for quantized types  (#14883)
* | | | | | | | 00fa15fe mtmd : add support for Voxtral (#14862)
* | | | | | | | 946b1f68 CUDA: fix pointer incrementation in FA (#14916)
* | | | | | | | 6c6e397a model : add support for SmallThinker series (#14898)
* | | | | | | | afc0e896 sycl: refactor quantization to q8_1  (#14815)
* | | | | | | | a5771c9e ops : update BLAS (#14914)
| | | | | | | | * e9f7e7cc ops : update BLAS
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | c35f9eaf ops : update Metal (#14912)
* | | | | | | | 1f45f289 sync : ggml
* | | | | | | | 613c5095 cmake : Indent ggml-config.cmake (ggml/1310)
* | | | | | | | 7f975995 quantize : update README.md (#14905)
* | | | | | | | bf78f543 vulkan: add ops docs (#14900)
* | | | | | | | bbfc8492 SYCL: add ops doc (#14901)
* | | | | | | | ca0ef2dd llama : clarify comment about pp and tg graphs [no ci] (#14895)
* | | | | | | | 89d10295 vulkan : add fp16 support for the conv_2d kernel (#14872)
* | | | | | | | f1a4e72d vulkan: skip empty set_rows to avoid invalid API usage (#14860)
* | | | | | | | 4762ad73 model : make rope_yarn_log_mul optional for deepseek2 (#14896)
* | | | | | | | 1dc9614e llama : fix kq_scale for the attention layers of PLaMo2 (#14892)
* | | | | | | | 446595b9 Docs: add instructions for adding backends (#14889)
* | | | | | | | 66906cd8 HIP: Enable Matrix cores for MMQ Kernels, Enable stream-K for CDNA 3 (#14624)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 11dd5a44 CANN: Implement GLU ops (#14884)
* | | | | | | 9b8f3c6c musa: fix build warnings (unused variable) (#14869)
* | | | | | | c7f3169c ggml-cpu : disable GGML_NNPA by default due to instability (#14880)
* | | | | | | 793c0d7f metal: SSM_SCAN performance (#14743)
* | | | | | | ce111d39 opencl: add fused `rms_norm_mul` (#14841)
* | | | | | | e7fecba9 docs : update HOWTO‑add‑model.md for ModelBase and new model classes (#14874)
| | | | | | | * a5801f40 sync : ggml
| | | | | | | * 2c1f8101 cmake : Indent ggml-config.cmake (ggml/1310)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | e2b7621e ggml : remove invalid portPos specifiers from dot files (#14838)
* | | | | | | c1dbea75 context : restore preemptive sched reset when LLAMA_SET_ROWS=0 (#14870)
* | | | | | | 749e0d27 mtmd : fix 32-bit narrowing issue in export-lora and mtmd clip (#14503)
* | | | | | | 64bf1c37 rpc : check for null buffers in get/set/copy tensor endpoints (#14868)
* | | | | | | c12bbde3 sched : fix multiple evaluations of the same graph with pipeline parallelism (#14855)
| | | | | | | * 6f4c5723 server : fix vision test regex
| | | | | | | | * aa5a7c6d profiler: output all tensor names
| | | | | | | | * dd0b9aab profiler: initial support for profiling graph ops
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 3f4fc97f musa: upgrade musa sdk to rc4.2.0 (#14498)
* | | | | | | | 2df255da sync : ggml
* | | | | | | | 60f816a7 cmake : fix usage issues (ggml/1257)
* | | | | | | | 5592f278 ggml-cpu : remove stdlib include from repack.cpp (ggml/1276)
| | | | | | | | * e65aa694 context : only sort outputs when needed
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | * a124399f sched : fix multiple evaluations of the same graph with pipeline parallelism
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | e4868d16 context : perform output reorder lazily upon access after sync (#14853)
* | | | | | | | 820de57d chat : fix kimi-k2 chat template (#14852)
| | | | | | | | * 978c88ba cont : add TODO
| | | | | | | | * 5e58711c context : perform output reorder after lazily upon access after sync
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | cb4a63aa sycl: fixed semantics of block offset calculation (#14814)
* | | | | | | | 86f5623d llama : fix MiniCPM inference after Granite Four changes (#14850)
* | | | | | | | 39cffdf1 docs: add libcurl-dev install hint for Linux distros (#14801)
* | | | | | | | 065908cb metal : fix fusion across different encoders (#14849)
* | | | | | | | 4ec6291a sycl: fix undefined variable in work group size check (#14843)
| | | | | | | | * 1ef3cc1a imatrix : use GGUF regardless of the output filename
| | | | | | | | * 53f65c35 imatrix : use GGUF by default
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | a12363bb convert : text-only support for GLM-4.1V-9B-Thinking (#14823)
* | | | | | | | a86f52b2 CUDA: fix overflow in FA, tune performance (#14840)
* | | | | | | | b284197d CUDA: fix compilation with GGML_CUDA_F16 (#14837)
* | | | | | | | 221c0e0c ci : correct label refactor->refactoring (#14832)
* | | | | | | | 07a19e27 CUDA: fix quantized KV cache + multiple sequences (#14822)
* | | | | | | | 18f3b5ff tests : add non-cont K,V FA tests
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 7233358d memory : handle saving/loading null layers in recurrent memory (#14675)
* | | | | | | 6c88b3bb ggml: fix loongarch quantize_row_q8_1 error (#14827)
* | | | | | | 14c28dfc CANN: weight format to NZ for Ascend310P3 (#14407)
* | | | | | | 8c988fa4 CUDA: add fused rms norm (#14800)
| |_|_|_|/ /  
|/| | | | |   
* | | | | | acd6cb1c ggml : model card yaml tab->2xspace (#14819)
* | | | | | 84712b60 vulkan: fix rms_norm_mul to handle broadcasting dim0 (#14817)
* | | | | | d4d1522b llama : add model type detection for rwkv7 7B&14B (#14816)
* | | | | | d1aa0cc5 imatrix: add option to display importance score statistics for a given imatrix file (#12718)
* | | | | | c8ade300 Mtmd: add a way to select device for vision encoder (#14236)
* | | | | | e28c0b80 cuda : implement bf16 cpy ops and enable bf16 cont (#14763)
* | | | | | 8e6f8bc8 opencl: remove unreachable `return` (#14806)
* | | | | | adef8178 server : allow setting `--reverse-prompt` arg (#14799)
* | | | | | 48b86c4f cuda: remove linking to cublasLt (#14790)
* | | | | | 38d3af1b opencl: fix `im2col` when `KW!=KH` (#14803)
* | | | | | 6c9ee3b1 opencl: add conv2d kernel (#14403)
* | | | | | cd465d82 sycl: Fix im2col (#14797)
* | | | | | 92204260 kleidiai: add support for get_rows (#14676)
* | | | | | 2ba1333b docs : fix backends table in README.md (#14796)
* | | | | | c2e058f1 vulkan/cuda: Fix im2col when KW!=KH (#14789)
* | | | | | c82d48ec llama : fix `--reverse-prompt` crashing issue (#14794)
* | | | | | b4efd77f server : add parse_special option to /tokenize endpoint (#14783)
|/ / / / /  
* | | | | 2be60cbc docs : fix link for tools/perplexity in README.md (#14780)
* | | | | b526ad26 Documentation: Further revisions to the Vulkan section in build.md (#14785)
* | | | | 938b7857 Clang-format: local files first + fix BinPacking (#14779)
* | | | | 36c15324 Contrib: add 0cc4m as codeowner for Vulkan backend (#14775)
* | | | | a979ca22 ggml: adds CONV_2D op and direct GEMM Vulkan implementation (#14316)
| |_|_|/  
|/| | |   
* | | | 90083283 imatrix : use GGUF to store importance matrices (#9400)
* | | | d4b91ea7 vulkan: Add logging for bf16 features to ggml_vk_print_gpu_info (#13274) (#14707)
* | | | 83f58724 Vulkan: Fix fprintf format-security warning (#14770)
* | | | f0d4d176 Documentation: Update build.md's Vulkan section (#14736)
| | | | * 55cf48de cuda : fix multi-seq, quantized FA
| | | | * a856a566 tests : add non-cont K,V FA tests
| | | | | * 0a0af0db Vulkan: Fix fprintf format-security warning
| |_|_|_|/  
|/| | | |   
* | | | | b1723091 sync : ggml
| | | | | * 386892ec sync : ggml
| |_|_|_|/  
|/| | | |   
* | | | | bf9087f5 metal : fuse add, mul + add tests (#14596)
* | | | | 9fb1042c graph : fix graph reuse reset of params (#14760)
* | | | | 2adf8d83 parallel : add option for different RNG seeds (#14757)
| | | | | * cfe5e984 graph : fix graph reuse reset of params
| |_|_|_|/  
|/| | | |   
* | | | | 021cc28b cuda : Fix Gemma3n not executed as CUDA_GRAPH on NVGPUs (#14741)
* | | | | d498af3d graph : avoid huge warm-up graphs for MoE models (#14753)
| |_|_|/  
|/| | |   
* | | | eacdeb5b model : fix build after merge conflict (#14754)
| | | | * 9106d759 model : fix build after merge conflict
| |_|_|/  
|/| | |   
* | | | e0cb5c5c model : add EXAONE 4.0 support (#14630)
* | | | f9a31eea CUDA: set_rows + cpy.cu refactor (#14712)
* | | | 8f974bc1 graph : refactor context to not pass gf explicitly (#14629)
* | | | 09651d09 graph : Pass the graph placeholder message in debug mode (#14748)
* | | | 349ea79f use max work group size for device to replace the magic number (#14732)
* | | | 670e1360 convert : fix Ernie4.5 MoE without shared experts (#14746)
* | | | 760b4484 nix : use optionalAttrs for env mkDerivation attrset argument (#14726)
* | | | cb887f1b model: add Ernie 4.5 MoE support (#14658)
* | | | d6fb3f6b kv-cache : fix k-shift for multiple streams (#14742)
| | | | * 05baa62a kv-cache : fix k-shift for multiple streams
| |_|_|/  
|/| | |   
* | | | 01612b74 llama : reuse compute graphs (#14482)
* | | | 086cf81e llama : fix parallel processing for lfm2 (#14705)
* | | | d9b69108 kv-cache : opt mask set input (#14600)
* | | | ad57d3ed batch : fix uninitialized has_cpl flag (#14733)
* | | | 1ba45d49 ci : disable failing vulkan crossbuilds (#14723)
* | | | 19e5943d convert : make hf token optional (#14717)
* | | | 496957e1 llama : fix parameter order for hybrid memory initialization (#14725)
* | | | 21c02174 ggml: Add initial WebGPU backend (#14521)
* | | | b0f0ecc3 model : support output bias for qwen2 (#14711)
* | | | 225e7a14 llama : add high-throughput mode (#14363)
* | | | ab140198 Support diffusion models: Add Dream 7B (#14644)
* | | | 64978340 ggml : add asserts (#14720)
* | | | 6ffd4e9c server : pre-calculate EOG logit biases (#14721)
| | | | * 07908a82 server : pre-calculate EOG logit biases
| |_|_|/  
|/| | |   
* | | | e4841d24 llama : fix parallel processing for plamo2 (#14716)
* | | | 538cc77f server : fix handling of the ignore_eos flag (#14710)
* | | | 5cae7665 scripts: synthetic prompt mode for server-bench.py (#14695)
* | | | 4b91d6f7 convert : only check for tokenizer folder if we need it (#14704)
* | | | cf91f217 convert : add pre-computed hashes first to prevent order mishaps (#14701)
| | | | * 9f8d2859 server : fix handling of the ignore_eos flag
| |_|_|/  
|/| | |   
* | | | 79e0b68c llama: add LLAMA_API to deprecated llama_kv_self_seq_div (#14708)
* | | | c81f4192 gguf-py : dump bpw per layer and model in markdown mode (#14703)
* | | | 4a4f4269 model : add Kimi-K2 support (#14654)
* | | | ba1ceb34 vulkan: fix noncontig check for mat_mul_id splitting (#14683)
* | | | 10a0351a vulkan: add RTE variants for glu/add/sub/mul/div (#14653)
* | | | 68e37a61 model : add PLaMo-2 support (#14560)
* | | | cbc68be5 cuda: fix build warnings in set-rows.cu (unused variable) (#14687)
* | | | bdca3837 sycl: Hotfix for non dnnl codepath (#14677)
* | | | 55c509da ggml : refactor llamafile_sgemm PPC code (#14673)
* | | | 9c9e4fc6 llama-context: add ability to get logits (#14672)
* | | | 494c5899 scripts: benchmark for HTTP server throughput (#14668)
* | | | 0f4c6ec0 SYCL: use 1D kernel for set_rows (#14618)
* | | | 65a3ebb0 sycl: Batched mulmat rework for oneDNN dispatch (#14617)
* | | | 0d922676 llama : add jinja template for rwkv-world (#14665)
* | | | 982e3472 quantize : fix minor logic flaw in --tensor-type (#14572)
* | | | 923e3ea2 cuda : add set rows for bf16 (#14664)
* | | | e743cddb cuda : add ELU support (#14657)
* | | | 05fec5bd ggml : add build-time message to remind about ggml_set_rows (#14661)
* | | | dcf7f2ea metal : Add missing unary ops Metal support (#14660)
* | | | 84b396e0 cmake : Add CMake presets for Linux and GCC (#14656)
| | | | * f68669d5 fix and opt kernel launch
| | | | * 18310bf2 fix trailing whitespace
| | | | * 418606e8 add mul_mat_f16_f32_image kernel
| | | | | * 942c55cd imatrix : avoid using imatrix.dat in README
| | | | | * 183eeb55 imatrix : avoid loading model to convert or combine imatrix
| | | | | * 50f53b3e imatrix : warn when writing partial data, to help guess dataset coverage
| | | | | * 42423ec4 imatrix : add warning when legacy format is written
| | | | | *   0ee322cd Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | c31e6064 tests : cover lfm2 cases in test_ssm_conv (#14651)
* | | | | | 67eade1b docs : add LFM2 to models section (#14650)
* | | | | | 7de5c7ca CUDA: add set rows for f32 and f16 (#14551)
* | | | | | 8eff9554 sync : ggml
* | | | | | 3120413c vulkan : remove unused vars (#0)
* | | | | | 21553570 sync : ggml
* | | | | | 74bb2945 vulkan : implement bilinear interpolation (ggml/1291)
* | | | | | 3e303b11 vulkan : implement ggml_roll (ggml/1290)
* | | | | | 0c1df14b server : fix pooled embedding output (#14645)
* | | | | | b3ad3a01 vulkan: support SET_ROWS (#14587)
* | | | | | 98197e5c vulkan: optimizations for deepseek prompt processing (#14555)
* | | | | | f5e96b36 model : support LiquidAI LFM2 hybrid family (#14620)
* | | | | | 756aa102 HIP : Add HIP 7.0+ compatibility for hipBLAS compute types (#14634)
* | | | | | aaa088d8 readme : add hot PRs (#14636)
* | | | | | 0d5375d5 llama : move enum llama_vocab_pre_type to implementation (#14631)
| |_|_|/ /  
|/| | | |   
* | | | | 576c82ed vocab : add midm-2.0 model pre-tokenizer (#14626)
* | | | | 0aedae00 model : Granite Four (#13550)
* | | | | 6bdda139 opencl: add tiled mul_mat_f16_f32 (#14535)
* | | | | 0b885577 opencl: add `set_rows` for `f16` and `f32` (#14547)
* | | | | 4bb625b7 Smoldocling support (#14597)
* | | | | 11ee0fea Docs: script to auto-generate ggml operations docs (#14598)
* | | | | a4575513 cmake : do not search for curl libraries by ourselves (#14613)
* | | | | 704bb7a7 SYCL: Initial set_rows kernel implementation (#14562)
* | | | | 435a6d10 llama : minor coding style fix for smollm3 (#14605)
* | | | | f9a867f5 cmake : bump llguidance version to v1.0.1 (#14609)
* | | | | ac44eb6c cmake : llguidance build parser library only (#14608)
* | | | | a57d1bcb cuda : support Falcon-H1 state size for SSM_SCAN (#14602)
* | | | | cb9178f8 llama : remove llm_graph_input_one (#14603)
* | | | | 4a5686da llama : support Jamba hybrid Transformer-Mamba models (#7531)
* | | | | 98bab638 ggml : add ggml_scale_bias (#14417)
| | | | * e33de128 common : move string_remove_suffix from quantize and imatrix
| | | | *   118d52fe Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | |\  
| | | | * | 0e793550 quantize : fix dataset name loading from gguf imatrix
| | | | * | 43cd2b3e imatrix : support 3d tensors with MUL_MAT
| | | | * | 1a9454a3 imatrix : avoid returning from void function save_imatrix
| | | | * | ba6f6be6 imatrix : don't use FMA explicitly
| | | | * |   2c094502 Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | |\ \  
| | | | * | | 1d190259 imatrix : use the function name in more error messages
| | | | * | | 635f945e convert : remove imatrix to gguf python script
| | | | * | | a5165a6c imatrix : two-way conversion between old format and GGUF
| | | | * | |   16202d6f Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | |\ \ \  
| | | | * \ \ \   1be357d9 Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | |\ \ \ \  
| | | | * \ \ \ \   db502ddd Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | |\ \ \ \ \  
| | | | * | | | | | c7a32e76 common : use GGUF for imatrix output by default
| | | | * | | | | | 2d79a707 quantize : use unused imatrix chunk_size with LLAMA_TRACE
| | | | * | | | | | 8c13e16b imatrix : allow loading mis-ordered tensors
| | | | * | | | | | 22172470 imatrix : remove unused n_entries
| | | | * | | | | | efa9186d imatrix : avoid using designated initializers in C++
| | | | * | | | | | 894ed8d7 py : include imatrix converter requirements in toplevel requirements
| | | | * | | | | | 9e6b0e94 perplexity : revert changes
| | | | * | | | | | 503630e8 py : add requirements for legacy imatrix convert script
| | | | * | | | | | d19101c9 imatrix : use FMA and sort tensor names
| | | | * | | | | |   3ad0603c Merge branch 'master' into compilade/imatrix-batched-chunks
| | | | |\ \ \ \ \ \  
| | | | * | | | | | | c8ab6a3b imatrix : fix conversion problems
| | | | * | | | | | | 3de9300c imatrix : use GGUF to store imatrix data
| | | | * | | | | | | 347247a2 imatrix : fix segfault when using a single chunk per batch
| | | | * | | | | | | bce54642 imatrix : allow processing multiple chunks per batch
| | | | | | | | | | | * 11807528 cuda : support Falcon-H1 state size for SSM_SCAN
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
| | | | | | | | | | | * 4d6a179c gguf-py : avoid adding duplicate tensor mappings for Jamba
| | | | | | | | | | | * 452207f3 memory : avoid referring to KV in recurrent cache logs
| | | | | | | | | | | * 7f3955a0 model : make falcon-h1 use shared mamba2 layer builder
| | | | | | | | | | | *   a60a24be Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 26a48ad6 ggml : prevent integer overflow in gguf tensor size calculation (#14595)
* | | | | | | | | | | | ffd59e7d model : add skt/A.X-4.0 model vocabulary (#14589)
* | | | | | | | | | | | 10555459 llama : remove unintended whitespace (#14592)
* | | | | | | | | | | | 04655063 model : add support for Falcon-H1 family (#14534)
* | | | | | | | | | | | 20b7bf8a convert : fix smollm3 jinja template (#14586)
| | | | | | | | | | | * f7c7a926 model : use ggml_swiglu_split for Mamba
| | | | | | | | | | | * 2f39cd7b model : remove unnecessary prefix for tensor loading constants
| | | | | | | | | | | * db5ff0cc jamba : remove redundant nullptr initializations
| | | | | | | | | | | *   b0b280ea Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
| | | | | | | | | | | *   f7163582 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\  
| | | | | | | | | | | * | 07c252f0 model : add Jamba to Mamba-specific hparams printing
| | | | | | | | | | | * | 20f8e43e graph : add back hybrid memory graph input
| | | | | | | | | | | * |   4682e21c Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \  
| | | | | | | | | | | * | | 908e6559 convert : fix jamba conv1d shape squeezing
| | | | | | | | | | | * | |   2bcaf64e Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \  
| | | | | | | | | | | * | | | e3fe6120 llama : partially apply clang-format style
| | | | | | | | | | | * | | |   691698e1 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | * | | | | 8006f3b3 llama : remove implicit recurrent state rollbacks
| | | | | | | | | | | * | | | |   124c222f Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \  
| | | | | | | | | | | * \ \ \ \ \   63ac36b2 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | 4bb4b22a llama : begin renaming llama_past back to llama_kv_cache
| | | | | | | | | | | * | | | | | | 375de5b1 llama : use unused n_embd_k_gqa in k_shift
| | | | | | | | | | | * | | | | | | 5f62db79 llama : fix mixed signedness comparison
| | | | | | | | | | | * | | | | | | 9d3f44da convert_hf : fix Jamba conversion
| | | | | | | | | | | * | | | | | |   a03e32a3 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | | fcb889cf llama : session saving and reloading for hybrid models
| | | | | | | | | | | * | | | | | | |   bc320ef6 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \  
| | | | | | | | | | | * \ \ \ \ \ \ \ \   9b38f8bf Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \ \  
| | | | | | | | | | | * \ \ \ \ \ \ \ \ \   10c3c419 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | | | | | 33425a7e mamba : fix non-contiguous usage of ggml_silu
| | | | | | | | | | | * | | | | | | | | | |   ff794f55 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | | | | | | 43d8d4bf examples : replace llama_kv_cache_seq_* with llama_past_seq_*
| | | | | | | | | | | * | | | | | | | | | | | 372482df llama : rename llama_cache to llama_past
| | | | | | | | | | | * | | | | | | | | | | |   6840ac0b Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | | | | | | | fee3c1d7 llama : allow doing the equivalent of SSM_CONV with SUM_ROWS and MUL
| | | | | | | | | | | * | | | | | | | | | | | | 17f6c1ef llama : fix .base() compilation error on Windows
| | | | | | | | | | | * | | | | | | | | | | | | 8fb57ac0 llama : use im2col and mul_mat to perform convolution for Mamba
| | | | | | | | | | | * | | | | | | | | | | | | eb589d5e llama : avoid copies for simple batch splits
| | | | | | | | | | | * | | | | | | | | | | | | 61200ef2 llama : fix edge case finding batch seq_id of split recurrent cell
| | | | | | | | | | | * | | | | | | | | | | | | 18d1c140 llama : minimize swaps when reordering logits
| | | | | | | | | | | * | | | | | | | | | | | | 72eea492 llama : fix batch split output count for embeddings
| | | | | | | | | | | * | | | | | | | | | | | |   5d3c7b95 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | | | | | | | | 3587a949 llama : use equal-sequence-length sub-batches for recurrent models
| | | | | | | | | | | * | | | | | | | | | | | | |   4e4c41e5 Merge branch 'master' into compilade/refactor-kv-cache
| | | | | | | | | | | |\ \ \ \ \ \ \ \ \ \ \ \ \ \  
| | | | | | | | | | | * | | | | | | | | | | | | | | 3a414b0b llama : sequence-length-aware batch splitting
| | | | | | | | | | | * | | | | | | | | | | | | | | 181dadf2 llama : fix Jamba quantization sanity checks
| | | | | | | | | | | | | | | | | | | | | | | | | | * b7c6ece5 ggml-ci
| | | | | | | | | | | | | | | | | | | | | | | | | | *   a67685e0 Merge commit 'refs/pull/14417/head' of github.com:ggerganov/llama.cpp into xsn/ggml_scale_bias
| | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | | | | * ebbad779 add x param to ggml_vec_mad1_f32
| | | | | | | | | | | | | | | | | | | | | | | | | | * | 60b03ff9 ggml-ci
| | | | | | | | | | | | | | | | | | | | | | | | | | * | 533016ef Merge commit 'refs/pull/14417/head' of github.com:ggerganov/llama.cpp into xsn/ggml_scale_bias
| | | | | | | | | | | | | | | | | | | | | | | | | | |\| 
| | | | | | | | | | | | | | | | | | | | | | | | | | | * cd1703a3 use scalar for __ARM_FEATURE_SVE
| | | | | | | | | | | | | | | | | | | | | | | | | | * | 34bacc83 ggml-ci
| | | | | | | | | | | | | | | | | | | | | | | | | | |/  
| | | | | | | | | | | | | | | | | | | | | | | | | | * 4ea74b04 make code looks more consistent
| | | | | | | | | | | | | | | | | | | | | | | | | | * 0d70ca81 use memcpy for op params
| | | | | | | | | | | | | | | | | | | | | | | | | | * 50c678f6 rm __ARM_FEATURE_SVE
| | | | | | | | | | | | | | | | | | | | | | | | | | * 563aca0b vDSP_vsmsa
| | | | | | | | | | | | | | | | | | | | | | | | | | * 265cb435 fix cann compile error
| | | | | | | | | | | | | | | | | | | | | | | | | | * c8d89317 suggestions from coderabbit
| | | | | | | | | | | | | | | | | | | | | | | | | | * b22708fd fix cuda
| | | | | | | | | | | | | | | | | | | | | | | | | | * 4d019532 will this fix cpu?
| | | | | | | | | | | | | | | | | | | | | | | | | | * 0e51a0a8 opencl
| | | | | | | | | | | | | | | | | | | | | | | | | | * 477a97ad cann (placeholder)
| | | | | | | | | | | | | | | | | | | | | | | | | | * 782b58fa vulkan
| | | | | | | | | | | | | | | | | | | | | | | | | | * a28df6f0 sycl
| | | | | | | | | | | | | | | | | | | | | | | | | | * 92a87384 add CUDA
| | | | | | | | | | | | | | | | | | | | | | | | | | * e427af75 add more simd
| | | | | | | | | | | | | | | | | | | | | | | | | | * a5ccf168 ggml_vec_mad1_f32
| | | | | | | | | | | | | | | | | | | | | | | | | | *   7af3fd98 Merge branch 'master' into xsn/ggml_scale_bias
| | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | * 50f88fc4 ggml : add ggml_scale_bias
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 7634d14d test-model-random : fix seq_id buffer overflow
| | | | | | | | | | | | | | | | | | | | | | | | | | | * a17c4f7d test-model-random : add shared prompt test variant
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 4e58ca46 test-model-random : avoid testing too many sequences for now
| | | | | | | | | | | | | | | | | | | | | | | | | | | *   18d20551 Merge branch 'master' into compilade/test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6efcd659 vulkan: optimize flash attention split_k_reduce (#14554)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 699f4392 model : fix hunyuan moe chat template (#14584)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 08382869 model : add SmolLM3 (#14581)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | bb4f7a9e memory : fix broken batch splits for recurrent cache (#14575)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b8eeb874 vulkan : fix rope with partial rotation and non-cont src (#14582)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 17a1f0d2 server: Add ability to mount server at prefix (#14544)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 362cf542 test-model-random : configurable model n_ctx, and smaller seq lengths
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 6b38c7a0 memory : fix broken batch splits for recurrent cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 985cda6c test-model-random : add Mamba2
| | | | | | | | | | | | | | | | | | | | | | | | | | | *   48a5eba5 Merge branch 'master' into compilade/test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | | | | * \   7c3f9c22 Merge branch 'master' into compilade/test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | ccb2bb99 test-model-random : show max error
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | 9d873d75 test-model-random : shuffle across sequences but not within
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | |   04b8f514 Merge branch 'master' into compilade/test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | | 352703b0 test-model-random : better default tensor initialization distribution
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | | dfa3c182 tests : add LLAMA, LLAMA4, and GEMMA2 to test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | |   61f64294 Merge branch 'master' into compilade/test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | | | 8fe213af tests : avoid sprintf in test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | | | 7657835b tests : fix overflow and memory leaks in test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | * | | | | 9cd402cb tests : add test-model-random
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 6837130c rm redundant code
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c9ccbec8 cgraph ok
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * a986efb3 wip
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | *   b4985945 Merge branch 'master' into vb/add-smollm3
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8f22dc0a model : add hunyuan moe (#14425)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 53903ae6 vulkan: increase timeout for CI (#14574)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4d0dcd4a cuda : fix rope with partial rotation and non-cont src (#14580)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 75c91de6 CUDA: add bilinear interpolation for upscale (#14563)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 68155c66 musa: fix build warnings (unused variable) (#14561)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 99619529 up.
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 97c64a09 up.
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 6201b438 Update the graph.
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 02ff0850 fix errors in conversion.
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 32ea9c5f Model -> ModelBase.
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 024bd294 Init - first pass.
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |/  
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |/|   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 2ff3354c memory : fix broken batch splits for recurrent cache
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e1a70590 llama : fix incorrect minicpm3 v_states shape (#14571)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 12f55c30 llama : remove ggml_cont where possible (#14568)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b9c3eefd CUDA: add bf16 and i32 to getrows (#14529)
| |_|/ / / / / / / / / / / / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6491d6e4 vulkan: increase LOAD_VEC_A to 8 (IQ1/IQ2) or 4 (IQ3) (#14485)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e592be15 vulkan: fix rms_norm+mul fusion (#14545)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | a0374a67 vulkan: Handle updated FA dim2/3 definition (#14518)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ddef9952 server : fix assistant prefilling when content is an array (#14360)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 66816881 opencl: add GELU_ERF (#14476)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * bf8b3901 metal : reuse graphs
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 0d2038f9 llama-bench : add graph reuse parameter
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 76681e3c llama : reuse compute graphs
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | bac8bed2 eval-callback : check for empty input (#14539)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b81510a7 test-backend-ops: add support for specifying output format (#14368)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ef797db3 metal : disable fast math in all quantize kernels (#14528)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 67d1ef23 batch : add optional for sequential equal split (#14511)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7b50f7c0 graph : prepare for 4D mask (#14515)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c79184d2 batch : add n_used count (#14512)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 499a8f5a CANN: Replace aclrtMemsetSync with aclnnInplaceZero operator (#14002)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 28657a82 ggml : implement GEGLU_ERF and GEGLU_QUICK ops (#14445)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | bee28421 opencl : broadcast for soft_max (#14510)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 2b72bede vulkan: support mixed/deepseekR1 FA head sizes (#14509)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c8c4495b ggml: backward pass for split swiglu (#14483)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 886da0a2 kv-cache : prepare K/V buffers for separation
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 7b63a71a Fix conditional enabling following arch checks for ggml-sycl (#14504)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 0c2ee38a convert : correct gemma 3n conversion (#14450)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a70c8a0c kv-cache : use ggml_set_rows (#14285)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 9067487c ggml : fix FA mask dim 2 and 3 (#14505)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d4cdd9c1 ggml : remove kompute backend (#14501)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 55c2646b CUDA: add dynamic shared mem to softmax, refactor general usage (#14497)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | e75ba4c0 gguf-py : add support for chat template jinja files (#14508)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5d46babd llama : initial Mamba-2 support (#9126)
* | | | | | | | | | | | | | | | | | | | | | | | | | | e17991c4 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | c46944aa ggml : add version function to get lib version (ggml/1286)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f3ed38d7 Set RPATH to "@loader_path" / "$ORIGIN" to ensure executables and dynamic libraries search for dependencies in their origin directory. (#14309)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * dfceb012 llama : add "virtual sequences"
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 30b4d4e1 ggml : add TODOs for adding GGML_OP_SET_ROWS support in the backends
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 5495ea96 kv-cache : add comments
| | | | | | | | | | | | | | | | | | | | | | | | | | | * f3da97e6 kv-cache : bounds-check when accessing slot_info indices
| | | | | | | | | | | | | | | | | | | | | | | | | | | * a70293bc kv-cache : improve find_slot impl
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 2ac5be3a cont : remove redundant ifs
| | | | | | | | | | | | | | | | | | | | | | | | | | | * ac8f3474 graph : separate k and v indices
| | | | | | | | | | | | | | | | | | | | | | | | | | | * cd811b7a kv-cache : use ggml_set_rows
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 55a1c5a5 CUDA: add softmax broadcast (#14475)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 12a81af4 CUDA: broadcasting for FlashAttention mask (#14500)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8875523e vulkan: support softmax/FA batch and broadcast (#14449)
* | | | | | | | | | | | | | | | | | | | | | | | | | | ec68e84c ggml : support bcast ggml_soft_max_ext, ggml_flash_attn_ext (#14435)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 307e79d3 opencl : fix possible buffer overflow in dump_tensor (#14490)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d7f5f4e5 simple-chat : fix context-exceeded condition (#14494)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c8a4e470 opencl : skip empty nodes on cgraph compute (#14491)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 603e43dc opencl : update upscale to support align corners (#14488)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 611ba4b2 ci : add OpenCL to labeler workflow (#14496)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 71bef665 cuda : graceful fallback for Mamba-1 models with weird embd size
| | | | | | | | | | | | | | | | | | | | | | | | | | | *   73de1fd1 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 85841e12 github : add OpenCL backend to issue templates (#14492)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 68b3cd65 ggml : Callback before abort (#14481)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | de569441 ci : disable fast-math for Metal GHA CI (#14478)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1b2aaf28 Add Vulkan images to docker.md (#14472)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 343b6e94 CANN: update aclnnGroupedMatmulV2 to aclnnGroupedMatmulV3 (#14411)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6a746cf9 vulkan: Split large mul_mat_id to fit in shared memory (#14451)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | eff5e454 add GELU_ERF (#14455)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a6a47958 ggml : remove trailing whitespace (#0)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f61c05d4 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 431b2c24 ggml-cpu : "align corners" for bilinear upscale/downscale (ggml/1285)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 497be7c0 ggml-quants : rename best_mad to best_error (ggml/1283)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 79b33b23 opencl : add GEGLU, REGLU, SWIGLU (#14456)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 0a5a3b5c Add Conv2d for CPU (#14388)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 745f11fe memory : correctly handle failure in apply() (#14438)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 5dd942de metal : disable fast-math for some cpy kernels (#14460)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a7417f55 ggml-cpu: sycl: Re-enable exp f16 (#14462)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | eb3fa291 test-backend-ops : disable llama test (#14461)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c839a2da cmake : Remove redundant include path in CMakeLists.txt (#14452)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | e9b6350e scripts : make the shell scripts cross-platform (#14341)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | caf5681f server : support jinja extra template kwargs (Qwen3 enable_thinking feature), from command line and from client (#13196)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 83790b0e server : fix appearance of the chats list context menu for Safari (#14322)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f47c1d71 SYCL: disable faulty fp16 exp kernel (#14395)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a5d1fb62 ggml : fix unmerged GGML_FPxx_TO_FPxx refactoring (#14443)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a0535ffa ggml : implement REGLU/GEGLU/SWIGLU ops (#14158)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | bd9c981d vulkan: Add fusion support for RMS_NORM+MUL (#14366)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 27208bf6 CUDA: add bf16 and f32 support to cublas_mul_mat_batched (#14361)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 63a7bb3c vulkan: handle noncontig in the final case of ggml_vk_get_cpy_pipeline (#14378)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 00d5282c vulkan: lock accesses of pinned_memory vector (#14333)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 566c16fc model : add support for ERNIE 4.5 0.3B model (#14408)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b25e9277 fix async_mode bug (#14432)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6609507a ci : fix windows build and release (#14431)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | ceb1bf5a vulkan: Fix GGML_VULKAN_SHADER_DEBUG_INFO (#14427)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 72babea5 graph : make llm_graph_context destructor virtual (#14410)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 43678060 recurrent : call balloc split_reset() in init_batch() (#14414)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 8d94219a ggml : add ggml_set_rows (#14274)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | f667f1e6 convert : fix broken sentencepiece vocab (#14416)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | 8846aace model : gemma3n text-only (#14400)
* | | | | | | | | | | | | | | | | | | | | | | | | | a01047b0 cmake: regen vulkan shaders when shaders-gen sources change (#14398)
* | | | | | | | | | | | | | | | | | | | | | | | | | b2534622 llama : return mistral-v7-tekken as default template only (#14390)
* | | | | | | | | | | | | | | | | | | | | | | | | | e8215dbb metal : add special-case mat-vec mul for ne00 == 4 (#14385)
* | | | | | | | | | | | | | | | | | | | | | | | | | 5783ae43 metal : batch rows copy in a single threadgroup (#14384)
* | | | | | | | | | | | | | | | | | | | | | | | | | bf5bcd0b docs: update s390x documentation + add faq (#14389)
* | | | | | | | | | | | | | | | | | | | | | | | | | 716301d1 musa: enable fp16 mma (all) and cublas on qy2 (#13842)
* | | | | | | | | | | | | | | | | | | | | | | | | | 60ef23d6 ggml-cpu: enable IBM NNPA Vector Intrinsics (#14317)
* | | | | | | | | | | | | | | | | | | | | | | | | | b193d530 ggml : do not output unprintable characters on GGUF load failure (#14381)
* | | | | | | | | | | | | | | | | | | | | | | | | | 2bf9d539 sycl: GGML_SYCL_DISABLE_OPT on by default for all Intel Devices (#13973)
* | | | | | | | | | | | | | | | | | | | | | | | | | 73e53dc8 opencl: ref count `ggml_backend_opencl_context` and refactor profiling (#14254)
* | | | | | | | | | | | | | | | | | | | | | | | | | 62af4642 batch : fix check for empty sequences in memory (#14364)
* | | | | | | | | | | | | | | | | | | | | | | | | | c148cf19 cmake : use LLAMA_BUILD_NUMBER when defining LLAMA_INSTALL_VERSION (#14362)
* | | | | | | | | | | | | | | | | | | | | | | | | | 1b809cee server : move no API key doc to /health (#14352)
* | | | | | | | | | | | | | | | | | | | | | | | | | abf24104 main : honor --verbose-prompt on interactive prompts (#14350)
* | | | | | | | | | | | | | | | | | | | | | | | | | 901e20bb jinja : Add Mistral-Small-3.2-24B-Instruct-2506.jinja (#14349)
* | | | | | | | | | | | | | | | | | | | | | | | | | 0142961a CUDA/HIP: optimize mmv paths taken for HIP devices (#14324)
* | | | | | | | | | | | | | | | | | | | | | | | | | ce82bd01 ci: add workflow for relocatable cmake package (#14346)
| |_|_|_|_|_|_|_|/ / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | * dc1d109d mamba : fix mismatched new and delete size for llm_build_mamba
| | | | | | | | | | | | | | | | | | | | | | | | *   afdb6692 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | bf2a99e3 vulkan: update windows SDK in release.yml (#14344)
* | | | | | | | | | | | | | | | | | | | | | | | | 72c6bc3f llama : better rwkv chat template and add missing `inputs.use_jinja` setting (#14336)
* | | | | | | | | | | | | | | | | | | | | | | | | defe2158 CUDA: mul_mat_v support for batch sizes > 1 (#14262)
| | | | | | | | | | | | | | | | | | | | | | | | *   830e5542 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | * | f8c7caee cuda : implement ssm scan for Mamba2
| | | | | | | | | | | | | | | | | | | | | | | | * |   a42f2394 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \  
| | | | | | | | | |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
| | | | | | | | |/| | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | * | 0b6f6bec ggml-cpu : reorder SVE FMA for consistency with other SIMD arches
| | | | | | | | | | | | | | | | | | | | | | | | * | 757aa623 ggml : fix mamba2 ssm scan when compiled with SVE
| | | | | | | | | | | | | | | | | | | | | | | | * | 2fa5f2ce graph : fix recurrent state copies when avoiding copies
| | | | | | | | | | | | | | | | | | | | | | | | * |   9864bfcd Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | e94f3932 kv-cache : allow context shift for recurrent models
| | | | | | | | | | | | | | | | | | | | | | | | * | | d55b0d06 convert : avoid AutoConfig for Mamba and Mamba2 hparams
| | | | | | | | | | | | | | | | | | | | | | | | * | |   929fe85d Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | | 94c3d530 kv-cache : remove const_cast when setting inputs for s_copy
| | | | | | | | | | | | | | | | | | | | | | | | * | | | 791998b4 metal : single-user mamba2 inference works
| | | | | | | | | | | | | | | | | | | | | | | | * | | | 6def5cd7 metal : add missing args for nb references in ssm_scan_f32_group
| | | | | | | | | | | | | | | | | | | | | | | | * | | | cf4f0a41 metal : fix confusion between ; and ,
| | | | | | | | | | | | | | | | | | | | | | | | * | | |   35d06fac Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * \ \ \ \   c9ecf620 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * \ \ \ \ \   1ee6c482 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | b4e9c599 convert : fix flake8 lint
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | |   8d8f0657 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | 3bc7103d ggml : avoid multiply by D in GGML_OP_SSM_SCAN
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | |   7d16e1bc Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 805512a7 ggml : remove unused fast broadcast path in GGML_MUL
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | |   038d9583 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | |_|_|_|_|/ / / / / / / / /  
| | | | | | | | | | | | | | | | | | | |/| | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 62b09b34 metal : fix wrong number of tokens per sequence in SSM_SCAN
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 5b8ec2b9 metal : fix SSM_SCAN state head offset
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 8b15bc6f metal : add back n_seqs to SSM_SCAN args
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 7a351abc metal : remove unused arguments for SSM_SCAN
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 03d0e6ea metal : use log and exp instead of log1pf and expf in SSM_SCAN
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 87b97d08 metal : fix SSM_SCAN pipeline scope
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 2c77d799 metal : attempt to adapt SSM_SCAN for Mamba-2
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | |   7d6cb368 Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 273e7a49 llama : avoid redundant state copy for Mamba 1 and 2
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | |   0e601caf Merge branch 'master' into compilade/mamba2
| | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | 38913dc8 convert_hf : prefer SentencePiece tokenizer for Mamba-2 when present
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | fa358e70 llama : add missing break
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | e04910dc llama : remove unused variable
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | aff96920 llama : fix Mamba-2 conv state saving
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | 2bfe9de6 llama : support running Mamba-Codestral-7B-v0.1
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | dceff23f ggml : SIMD ggml_ssm_scan for Mamba-2
| | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | | 1f0fea70 llama : initial Mamba-2 support
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 61795789 batch : require non-coupled batch with sequential split_equal
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 5eb1a88d batch : optional requirement for sequential sequence ids
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 66631284 kv-cache : rework kv_idxs, support seq_cp
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 0bb1da58 kv-cache : simplify set_rows logic
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 165d8220 graph : support iSWA virtual sequences
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 1b74b9d7 ggml : extend support for n_seq for soft_max and fattn
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 8c682198 kv-cache : fix non-FA path with virutal sequences
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 7c6487b2 metal : extend ggml_soft_max_ext() to support n_seq dim
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 401c13e3 cont : fix build
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 13214393 tools : tmp adjustments (TMP)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 52b90071 llama : add "virtual sequences"
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 37bdfbef wip 3
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * efc33ea6 wip 2
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 7664390b wip
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |/  
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 36f8e20d kv-cache : utilize ggml_set_rows broadcast
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 332f0735 cont : support non-continuous slots
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 39d0b1e8 cont : kv-cells cp/set for non-cont slots
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f875d6cb cont : migrate to using set of indices instead of slot head
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * db2bb378 cont : gate the ggml_set_rows usage with env var
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 79dac3c8 kv-cache : use ggml_set_rows
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 1f647b59 ggml : fix supports_op
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * eba97574 ggml : simplify forward_dup_f32
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c0cfc2f7 metal : add ggml_set_rows implementation
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 828e5d2f tests : add ggml_set_rows
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * e73690a6 ggml : ggml_set_rows update comment + better index name
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * e8970972 ggml : support GGML_TYPE_F32 ".from_float" trait
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 630c84a2 ggml : ggml_set_rows support quantized dst
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * df71c803 ggml : ggml_set_rows support broadcast
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 313a444b ggml : add ggml_is_contiguous_rows
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 695b6b70 ggml : add repeat impl for i64
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f2cd962f use I64 for indices
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c1a581a1 ggml : add ggml_set_rows
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7b50d589 kv-cells : fix tracking of seq_pos (#14339)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3a9457df vulkan: update windows SDK in CI (#14334)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fa4a9f2a quantize : handle user-defined pruning of whole layers (blocks) (#13037)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 238005c2 gguf-py : fix SpecialVocab parsing when post_processor is null (#14330)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 66aba7ac run : avoid double tokenization (#14327)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | f1f5e82d examples : fix is_first logic for tokenization (#14329)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | af3373f1 HIP: enable vec fattn on RDNA4 (#14323)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5d5c066d mtmd : fix Pixtral OOM with large images by capping image_size to 1024 (#14326)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 40bfa04c common : use std::string_view now that we target c++17 (#14319)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | aa064b2e CUDA: add mean operation (#14313)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | aa0ef5c5 gguf-py : fix Qwen3-Embedding eos token (#14314)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bb16041c Add support for VK_EXT_debug_utils to add labels to Vulkan objects. (#13792)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 58cba76a gguf-py : fix TemplateProcessing pair when bos/eos is missing (#14312)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 67ae5312 metal : fix thread-safety (#14300)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 692e3cdd memory : rename interface to llama_memory_context_i (#14296)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b23fa0b3 convert : fix Llama 4 conversion (#14311)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 06cbedfc sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b7147673 Add `ggml_roll` (ggml/1274)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d860dd99 docs : fix the link to llama.h (#14293)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c959f462 CUDA: add conv_2d_transpose (#14287)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 22015b20 lint : remove trailing whitepace (#14304)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | dd6e6d0b vocab : prevent tokenizer overflow (#14301)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8308f98c sycl: add usage of enqueue_functions extension (#14244)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6369be07 Implement GGML_CPU_ALL_VARIANTS for PowerPC (#14286)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 88fc854b llama : improve sep token handling (#14272)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e28c1b93 cuda : synchronize graph capture and cublas handle destruction (#14288)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * ae963339 metal : fix thread-safety
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d27b3ca1 ggml : fix repack work size for mul_mat_id (#14292)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9230dbe2 ggml: Update KleidiAI to v1.9.0 (#14277)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 812939a9 model : more uniform output id handling (#14275)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 6fb2f2e8 ggml : fix repack work size for mul_mat_id
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4c9fdfbe ubatch : new splitting logic (#14217)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9eaa51e7 CUDA: add conv_2d_dw (#14265)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8f71d0f3 ggml-cpu : remove unnecesary arm feature detection (#14281)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 381174bb gguf-py : make sentencepiece optional (#14200)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d67341dc server : add server parameters for draft model cache type (#13782)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 456af35e build : suppress gcc15 compile warnings (#14261)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 600e3e9b sycl: Cleanup codepaths in Get Rows in sycl backend (#14215)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fffcce53 llama-bench : add --no-warmup flag (#14224) (#14270)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5fc78568 convert : fix remote option in Windows (#14100)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | faed5a5f llamafile : support s390x SIMD instruction set (#14273)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 10bb545c Vulkan: Set device max size for host memory to avoid OOM warning and fallback to CPU buffer (#14249)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | edc4a29e memory : Hybrid recurrent cache (#13979)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ed3290ab metal : add mean kernel (#14267)
| |_|_|_|_|_|_|/ / / / / / / / / / / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8d947136 docs: add s390x build documentation (#14264)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 50d22279 ggml-cpu: reduce asm calls for hsum (#14037)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6231c5cd ggml-cpu: fix uncaught underscore terminators (#14023)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ef035803 ggml: Add Apple support for GGML_CPU_ALL_VARIANTS (#14258)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 413977de mtmd : refactor llava-uhd preprocessing logic (#14247)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 95402553 llama-chat : fix multiple system message for gemma, orion (#14246)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3865cff4 convert : fix null head_dim AutoConfig regression (#14248)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d03172cc sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | dd8e59f4 ggml : disable warnings for tests when using MSVC (ggml/1273)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bbe98d27 ggml : remove unused ggml_context_container (ggml/1272)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c2056ed6 examples : include examples in msvc disable warn (ggml/1270)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 59fee24c recurrent : rework graph inputs + add TODOs
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * faf41199 refactor: Use a common build_recurrent_state method that is cache-agnostic
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 5046d412 fix: Fix initialization of child states
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 9db44a2a fix: Fix resize vs reserve and skip null tensors in size computation
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 11cd80d5 feat: Overhaul build_recurrent_state / build_inp_s_copy to match attention pattern
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 4ec4e6a8 refactor: Use llama_memory_state_ptr for child states in hybrid memory state
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 7ba463b3 fix: Remove llama_model_is_hybrid_Recurrent public API
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 1510016e fix: Remove logits_all after rebase
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d8c929ff feat: Allow custom layer filters for hybrid recurrent
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d5d7628b refactor: Remove n_embd_k/v_gqa from recurrent cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * b42c8b43 refactor: Remove layer index from n_embd_k/v_s
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 1dd12133 refactor: Remove n_embd_k/v_s from unified cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 833dfb54 fix: Use per-layer n_embd_k/v_s calls for mamba (1) layers
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f6d5f055 fix: Remove errant virtual destructor leftover from previous impl attempt
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 9c1a604a fix: Update clear signature for data argument after rebase
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * de9297fd fix: Add missing padding to n_ctx for hybrid cache construction
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 911e6944 fix: Fix status for init_update sig for recurrent cache state
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d3699366 fix: Update recurrent cache for changes to remove intermediate kv_cache interface
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * a9b5fe98 fix: Fix logic for initializing inputs and attn layers for hybrid caches
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * e3c16315 feat: Support hybrid recurrent in llama-graph
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * cf03d4ae fix: Fix shift logic to defer to unified cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 6c6ec000 fix: Fix wrong bool condition for split equal in hybrid cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 423c8940 feat: Construct hybrid recurrent cache for hybrid recurrent models
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c71eaa37 feat: First pass at llama_kv_cache_hybrid_recurrent
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 13332a75 fix: Use per-layer sizing everywhere in kv caches
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 40e91878 feat: Add layer filter to recurrent cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * fb26e95a refactor: rename *_is_hybrid -> *_is_hybrid_recurrent
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * fc9e0b57 feat: Auto-fill hparams.recurrent_layer_arr based on whether the model is recurrent
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 05f19580 feat: Add support for distinguishing recurrent vs non-recurrent layers in hparams
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 5e2f2c38 feat: Add c++ side constants for attention layer indices hparam
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * ec8fe17b feat: Add llama_model_is_hybrid API call
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c4650301 cmake: remove shader-gen step-targets from ggml-vulkan (#14226)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d3d06deb server : add pidfile option
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 860a9e4e ggml-cpu : remove the weak alias trick (#14221)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fe9d60e7 musa: fix build warning (unused variable) (#14231)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 4b2233be Vulkan: Set device max size for host memory to avoid OOM warning and fallback to CPU buffer
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e434e691 common : suggest --jinja when autodetection fails (#14222)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 89fea80d server : fix incorrect usage of llama_get_embeddings() (#14225)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6adc3c3e llama : add thread safety test (#14035)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0dbcabde cmake: clean up external project logic for vulkan-shaders-gen (#14179)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ad590be9 model : add NeoBERT (#14164)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7d6d91ba HIP: disable rocwmma on gfx12 by default until rocm 7.0 (#14202)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d3e64b9f llama : rework embeddings logic (#14208)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3ba0d843 ggml: Add Android support for GGML_CPU_ALL_VARIANTS (#14206)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0bf49eb6 convert : remove arcee change in convert_hf_to_gguf_update.py (#14207)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4ad24367 gguf-py : allow key override when adding value to GGUFWriter (#14194)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c89c2d1a vulkan: mutex around vkQueueSubmit (#14127)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3555b300 ggml-cpu : rework weak alias on apple targets (#14146)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d7da8dc8 model : Add support for Arcee AI's upcoming AFM model (#14185)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | cd355eda server : When listening on a unix domain socket don't print http:// and port (#14180)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 30e5b01d quantize : change int to unsigned int for KV overrides (#14197)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e54b3940 CUDA/HIP: fix ssm_scan on devices where warp size is not 32 (#14196)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2c2caa44 HIP: Replace usage of depricated preprocessor macro __AMDGCN_WAVEFRONT_SIZE__ (#14183)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5fce5f94 kv-cache : fix use-after-move of defrag info (#14189)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9ae4143b model : add dots.llm1 architecture support (#14044) (#14118)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c311ac66 cparams : rename LLAMA_MAX_PARALLEL_SEQUENCES to LLAMA_MAX_SEQ (#14188)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b9912ac5 batch : auto-gen positions + verify multi-sequence input (#14177)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 00ba7726 docs : remove WIP since PR has been merged (#13912)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3cb203c8 llama-chat : Do not throw when tool parsing fails (#14012)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2e42be42 compare-llama-bench: add option to plot (#14169)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fb85a288 vocab : fix build (#14175)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 40643edb sycl: fix docker image (#14144)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3cfbbdb4 Merge commit from fork
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 80709b70 batch : add LLAMA_BATCH_DEBUG environment variable (#14172)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 26ff3685 docs : Update multimodal.md (#14122)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 60c66634 batch : rework llama_batch_allocr (#14153)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b7cc7745 readme : remove survey link (#14168)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | cc8d0818 cmake: Add ability to pass in LLAMA_BUILD_NUMBER/COMMIT (#14167)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d714dadb pooling : make cls_b and cls_out_b optional (#14165)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ffad0439 server : fix SWA condition for full context reprocess (#14163)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0889eba5 sycl: Adding additional cpy dbg print output (#14034)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c61285e7 SYCL: Bump oneMath commit (#14152)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 09cf2c7c cmake : Improve build-info.cpp generation (#14156)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c33fe8b8 vocab : prevent heap overflow when vocab is too small (#14145)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ed52f366 sycl: Remove not needed copy f16->f32 for dnnl mul mat (#14125)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | a681b4ba readme : remove project status link (#14149)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7d516443 server : re-enable SWA speculative decoding (#14131)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 36fce982 server : re-enable swa speculative decoding
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | f6e1a7aa context : simplify output counting logic during decode (#14142)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c3ee46fa batch : remove logits_all flag (#14141)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e2c0b6e4 cmake : handle whitepsaces in path during metal build (#14126)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * ed99a8ea cont : fix comments
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * b8b8d3f3 context : simplify output counting logic during decode
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c53acda0 batch : remove logits_all flag
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 95965069 kv-cache : fix split_equal handling in unified implementation (#14130)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | a20b2b05 context : round n_tokens to next multiple of n_seqs when reserving (#14140)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2e89f76b common: fix issue with regex_escape routine on windows (#14133)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 4b6fb652 context : round n_tokens to next multiple of n_seqs when reserving
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 532802f9 Implement GGML_CPU_ALL_VARIANTS for ARM (#14080)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d4e0d95c chore : clean up relative source dir paths (#14128)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | cc66a7f7 tests : add test-tokenizers-repo (#14017)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bd248d4d vulkan: Better thread-safety for command pools/buffers (#14116)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7781e5fe webui: Wrap long numbers instead of infinite horizontal scroll (#14062)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 89a184fa kv-cache : relax SWA masking condition (#14119)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2baf0772 server : pass default --keep argument (#14120)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7ae29321 kv-cache : add LLAMA_KV_CACHE_DEBUG environment variable (#14121)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1f7d50b2 vulkan: Track descriptor pools/sets per-context (#14109)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4c763c8d opencl: add `mul_mv_id_q4_0_f32_8x_flat` (#14003)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | dad5c443 kv-cache : avoid modifying recurrent cells when setting inputs (#13834)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 55f6b9fa convert : fix duplicate key DeepSeek-R1 conversion error (#14103)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3678b838 llama : support GEGLU for jina-bert-v2 (#14090)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 652b70e6 vulkan: force device 0 in CI (#14106)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3a12db23 Fixed spec timings to: accepted/tested instead of accepted/drafted (#14104)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ae92c185 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b7ce1ad1 ggml : fix weak alias win32 (whisper/0)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 97340b4c Vulkan: Don't default to CPU device (like llvmpipe), even if no other device is available, to allow fallback to CPU backend (#14099)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2bb04670 rpc : nicer error messages for RPC server crash (#14076)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b8e2194e sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1a3b5e80 Add in-build ggml::ggml ALIAS library (ggml/1260)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1f63e75f metal : use less stack memory in FA kernel (#14088)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 40cbf571 kv-cache : fix shift and defrag logic (#14081)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7f4fbe51 llama : allow building all tests on windows when not using shared libs (#13980)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 62a9f34b llama-graph : fix recurrent state copy
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | *   dd6495dd Merge branch 'master' into compilade/readonly-recurrent-inputs
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | f470bc36 ggml-cpu : split arch-specific implementations (#13892)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8f47e25f cuda : fix device sync on buffer clear (#14033)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 201b31dc graph : fix geglu (#14077)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e21d2d4a CANN: Simplify the environment variable setting(#13104)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | dc0623fd webui: fix sidebar being covered by main content (#14082)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 87d34b38 server : fix LRU check (#14079)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b460d16a sycl: Add reorder to Q6_K mmvq implementation (#13885)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 31296394 kv-cache : avoid modifying recurrent cells when setting inputs
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c257a887 cont : fix defrag erasing cells that didn't move
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d564e04c cont : reset shift[i]
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * eee8d481 kv-cache : fix shift
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 91a8ee6a add geglu activation function (#14074)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 056eb745 CANN: Enable labeler for Ascend NPU (#13914)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 247e5c6e cuda : fix buffer type check with integrated GPUs (#14069)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5787b5da ci: add LoongArch cross-compile build (#13944)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 228f34c9 SYCL: Implement few same quantized type copy kernels (#13739)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0974ad7a llama : fix llama_model_chat_template with template name (LLM_KV with suffix) (#14050)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 745aa531 llama : deprecate llama_kv_self_ API (#14030)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 487a5e04 context : fix SWA-related warning for multiple sequences (#14045)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d17a809e llama : support multiple classifier outputs and labels (#13940)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1caae7fc gguf-py : add add_classifier_output_labels method to writer (#14031)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 669c13e0 vulkan: Enable VK_KHR_cooperative_matrix extension for Intel Xe2 GPUs (#14001)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 146b88e8 ci: fix CUDA build failure on autodl cloud machines (#14005)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7f37b6cf memory : migrate from llama_kv_cache to more generic llama_memory (#14006)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3a077146 llama : allow using mmap without PrefetchVirtualMemory, apply GGML_WIN_VER to llama.cpp sources (#14013)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d01d112a readme : add badge (#13938)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9f47fa57 vocab : warn about missing mask token (#14022)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9e31bec4 context : fix pos_min initialization upon error decode (#14008)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5a8ae305 vulkan: automatically deduce size of push constants (#13936)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0d398442 ggml-vulkan: adds support for op CONV_TRANSPOSE_1D (#13813)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3e63a58e kv-cache : refactor the update/defrag mechanism (#13988)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2589ad37 ci : remove cuda 11.7 releases, switch runner to windows 2022 (#13997)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 48254871 releases : use dl backend for linux release, remove arm64 linux release (#13996)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3ac67535 llama-graph : use ggml_repeat_4d (#13998)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0b4be4c4 CUDA: fix FTZ in FA for Gemma 3 (#13991)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e0e806f5 kv-cache : fix unified::seq_rm to work with seq_id < 0 (#13985)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7e00e60e vulkan: fix warnings in perf logger querypool code (#13937)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ea1431b0 docs : add "Quick start" section for new users (#13862)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 71e74a3a opencl: add `backend_synchronize` (#13939)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bfb1e012 OpenCL: Add concat, tsembd, upscale, tanh, pad and repeat (#13840)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 36375762 server : disable speculative decoding for SWA models (#13970)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ea394d7a metal : use F32 accumulators in FA kernels (#13975)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5582c49c gemma : more consistent attention scaling for v2 and v3 (#13951)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c9bbc779 `server`: update deepseek reasoning format (pass reasoning_content as diffs) (#13933)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bfd32279 mtmd : fix memory leak in mtmd_helper_eval_chunk_single (#13961)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 093e3f1f cmake : Handle mixed-case 'Power' strings in POWER CPU detection (#13966)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 663445b0 sycl: quantize and reorder the input to q8_1 when reorder is enabled (#13826)
| |/ / / / / / / / / / / / / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7675c555 gguf: fix failure on version == 0 (#13956)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5e1c3aed convert : fix nomic-bert-moe mask token (#13757)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c496fe0b convert : fix vocab padding code for bert models (#13954)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e57bb87c ggml: check if non-native endian model is being loaded (#13943)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | f3a4b165 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 108009f5 vulkan : Remove unexpected ; (ggml/1253)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d337252a cmake : Fix broken CMake error messages (ggml/1252)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | af6f91db ggml : remove ggml_graph_import and ggml_graph_export declarations (ggml/1247)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | a7b8d35f sync : whisper.cpp (ggml/1250)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6eba72b7 ggml : install dynamic backends (ggml/1240)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | fedf034a ggml : Print backtrace on uncaught C++ exceptions (ggml/1232)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8726392d readme : update bindings (#13950)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c0462171 parallel : fix n_junk == 0 (#13952)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0fc16b42 kv-cache : split implementation in separate sources (#13920)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 053b1539 threading: support for GGML_SCHED_PRIO_LOW, update thread info on Windows to avoid throttling (#12995)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * ac35e50c Update tools/llama-bench/llama-bench.cpp
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 9087dd26 threading: disable SetThreadInfo() calls for older Windows versions
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 199a8384 threading: support for GGML_SCHED_PRIO_LOW, update thread info on Windows to avoid throttling
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d3a2eb59 disable on windows
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 7210ebe2 revert build changes
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 05f94a0e add arch to matrix
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f9a27178 download in batches
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | *   de8ec134 Merge branch 'master' into cisc/test-tokenizers-remote
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b3a89c3d docs : Note about necessity of having libcurl installed for standard build. (#13945)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e15898d1 server: allow unclosed thinking tags (#13931)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 803f8baf llama : deprecate explicit kv_self defrag/update calls (#13921)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3600cc28 llama : use n_swa + n_ubatch cells for SWA cache (#13833)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c7e0a205 webui : Replace alert and confirm with custom modals. (#13711)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3f55f781 llama : auto-batch preparation (#13845)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 51fa76f1 mtmd : drop `_shared` from `libmtmd` name, merge helpers into libmtmd (⚠️ breaking change) (#13917)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 12d0188c kv-cache : refactor + add llama_memory_state_i (#13746)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | eb394993 CUDA: add a prop in ggml_cuda_device_infor for distinguish iGPU or dGPU in cuda (#13856) (#13895)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e562eece CUDA: fix typo in FlashAttention code (#13926)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b47ab7b8 sched : avoid changing cur_copy when a graph is already allocated (#13922)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | dd665cc9 parallel : increase the variability of the prompt lengths (#13927)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | df0c0c7d cuda : prevent using split buffers with 3d/4d matrices (#13919)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b49a8ff9 SYCL: Add mrope kernel (#13755)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 53f92507 sync : vendor (#13901)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | db38704f convert : fix rwkv bos/eos token (#13844)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 07e4351c convert : allow partial update to the chkhsh pre-tokenizer list (#13847)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 291f2b69 llama : add support for DistilBert (#13907)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2c90da4c llama : use llm_build_granite for minicpm (#13911)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ec9e0301 cmake: Guard GGML_CPU_ALL_VARIANTS by architecture (#13890)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e83ba3e4 llama : add support for jina-reranker-v2 (#13900)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2b131621 gguf-py : add support for sub_type (in arrays) in GGUFWriter add_key_value method (#13561)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 54a2c7a8 arm64: optimize q4_k_q8_k kernel with i8mm (#13886)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 21fcc21a cmake: Factor out CPU architecture detection (#13883)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | dd8ba934 ggml: aarch64: Implement SVE F32 kernels for Mamba Sequential Scan Algorithm (#13882)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 66c92061 tests : remove json.hpp from a test (#13880)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5ca82fc1 convert : workaround for AutoConfig dummy labels (#13881)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6385b843 llama : add RobertaForSequenceClassification reranker support (#13875)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1b8fb815 ggml: aarch64: Implement SVE F32 kernels for vector functions (#13843)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 53ae3064 gguf-py : fix SafetensorRemote return on undefined size (< 0) (#13841)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 763d06ed llama : fix KV shift for qwen2vl (#13870)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 10961339 mtmd : move helpers to dedicated library (⚠️ breaking change) (#13866)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d98f2a35 ci: disable LLAMA_CURL for Linux cross-builds (#13871)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e0e3aa23 llama : add support for BertForSequenceClassification reranker (#13858)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | aa6dff05 convert: small addition to support LlamaModel (#13838)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c962ae33 server: fix remove 'image_url'/'input_audio' json-object effectlly for 'llama_params' in multimodal-model-mode (#13853)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | a3938fb5 convert : fix qwen omni conversion (#13859)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | f7873fc6 tests : change umlaut test (#11600)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | a6824743 CUDA: fix FA tg at long context for CC >= 8.9 (#13852)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 26b79b6c convert : fix tensor naming conflict for llama 4 vision (#13836)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 8e1125a8 copy curl dll for tests
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 4b4843ad windows builds adds build type to runtime output
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * d97b9ade correct working directory for all builds
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 0fe7183a fix prototype for non-curl builds
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * ecbc92ac correct working directory
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 42ff1867 add test-tokenizers-remote
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 2d2e059f make common_download_file_single/multiple public
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1e8659e6 CANN: Add SOC TYPE printing in cmake configuration (#13837)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a3c30846 opencl: add new ops - `argsort`, `div`, `sub`, `addrows`, `sigmoid`, `group_norm` (#13787)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1701d4c5 opencl: mark `mul_mat` `f32f32` as supporting non-contiguous tensors (#13790)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | bef81763 vulkan: use timestamp queries for GGML_VULKAN_PERF (#13817)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 34b7c043 cmake : add llama-cparams.cpp to build (#13832)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f3101a8c SYCL: add gelu_erf kernel (#13749)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1c49c70d sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a8ea03d8 ggml : add ggml_repeat_4d (#13824)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 05f6ac62 ggml : riscv: add xtheadvector support (#13720)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | bc583e3c mtmd : support Qwen 2.5 Omni (input audio+vision, no audio output) (#13784)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 72b090da docs: remove link for llama-cli function calling (#13810)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 7fe03e74 ggml-cpu: x86 feature detection is specific to x86 (#13811)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 952f3953 ggml : allow CUDA graphs when using pipeline parallelism (#13814)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 81713121 kv-cells : track min/max used cells and per-sequence positions (#13808)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f9cd6839 sampling : make sure samplers return at least 1 token (#13822)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 9065ca71 tests : sampling tests use min_keep == 0
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 76f31abe sampling : same for typical sampling
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * fe12a5d4 sampling : min-p should always return at least one token
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 4f81b33e llama : validate seq id batch input (#13809)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | cdf94a18 server: --offline mode (#13804)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a26c4cc1 scripts : add option to compare commits in Debug (#13806)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 4265a87b cuda : avoid cuGetErrorString (#13791)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6f180b91 SYCL: Add non contiguous support in RMS_NORM and NORM kernels (#13611)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 03f582ae server: fix streaming crashes (#13786)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 88c125f2 examples/training: Fix file name in README (#13803)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d74e94c1 `server`: fix format of streamed tool call deltas (diff name, fix id location) (#13800)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f13847cf server: fix regression on streamed non-chat completion w/ stops (#13785)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 79c137f7 examples : allow extracting embeddings from decoder contexts (#13797)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 22229314 llama : clarify deprecation message (#13794)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 9012eb9b sycl: Add more debug prints (#13640)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | fef693dc vulkan: mark IM2COL as supporting non-contig (#13783)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 2d38b6e4 CANN: Add the basic supports of Flash Attention kernel (#13627)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | e121edc4 `server`: add `--reasoning-budget 0` to disable thinking (incl. qwen3 w/ enable_thinking:false) (#13771)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 2f099b51 webui : bump max upload file size to 500MB (#13779)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | aa50ba46 tests : improve UGM tokenizer test coverage (#13773)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | de2ef53a kv-cache : rework kv_cell (#13706)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c508256d rpc : Fix build on OpenBSD (#13541)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 40aaa8a4 mtmd : add support for Qwen2-Audio and SeaLLM-Audio (#13760)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a08c1d28 docs : add Moondream2 pre-quantized link (#13745)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d785f9c1 server: fix/test add_generation_prompt (#13770)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 4032ca40 llama : add support for Qwen3 MoE tied word embeddings (#13768)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 515fdbf7 SYCL: revert "sycl: simplify bin_bcast_kernel (#13383)" (#13752)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f5cd27b7 `server`: streaming of tool calls and thoughts when `--jinja` is on (#12379)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a2d02d57 releases : bundle llvm omp library in windows release (#13763)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 17fc817b releases : enable openmp in windows cpu backend build (#13756)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 2bd1b30f ggml-cpu : set openmp wait time if not set (#13758)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 259469c4 Move GLM4 f32 attention fix to the correct function (#13750)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 4c32832c ggml : add ggml_gelu_erf() CUDA kernel (#13719)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c3a26243 vocab : fix ugm tokenizer precision (#13743)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | ffd0eae6 CUDA: fix race condition in FA vector kernels (#13742)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b775345d ci : enable winget package updates (#13734)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a70a8a69 ci : add winget package updater (#13732)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d13d0f61 hparams : initialize arrays (#13728)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 8a2afb75 llama : allow custom list of swa_layers (#13726)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 9ecf3e66 server : support audio input (#13714)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | faaaff5f CANN: Support MUL_MAT_ID for q8_0 and q4_0 (#13705)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | e16c4731 ggml : fix the order of ggml_unary_op (#13718)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1dcd0196 vulkan: support CPY from any type to itself (#13695)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c10ed6cb vulkan: Disable coopmat/coopmat2/bfloat extensions if glslc doesn't support it (#13696)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a127ff17 use LOG_WARN to replace `std::cerr` (#13657)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3079e9ac release : fix windows hip release (#13707)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 8a1d206f tts : fix n_ubatch + make WavTokenizer cache-less (#13713)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 108d484a tts : fix n_ubatch + make WavTokenizer cache-less
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 797990c4 mtmd : add ultravox audio input (#13623)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | ab863357 common: Include torch package for s390x (#13699)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | cc74d5be server : pad small embedding batches (#13692)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 5be24af7 gguf-py : correct charsmap parameter typing (#13701)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d394a9ae sycl : Remove waits from function calls (#13702)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6b56a646 SYCL: Avoid using with SYCL-Graph for unsupported nodes (#13587)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a4e8912d opencl: Add support for multiple devices (#12622)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | edbf42ed opencl: fix couple crashes (#12795)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d643bb2c releases : build CPU backend separately (windows) (#13642)
|/ / / / / / / / / / / / / / / / / / / / / / / / / / /  
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8e186ef0 hparams : support models for which all layers use SWA (#13682)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5fbfe384 server : improve error reporting (#13680)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c76532e7 convert : add qwen2vl support for unsloth merges (#13686)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2aa777d8 examples : switch retrieval to llama_encode (#13685)
* | | | | | | | | | | | | | | | | | | | | | | | | | | eb0f5c28 gguf-py : display the invalid gguf type (#13687)
* | | | | | | | | | | | | | | | | | | | | | | | | | | cf4cb59e ggml : add ggml_gelu_erf() (#13667)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0d5c7421 server : Add the endpoints /api/tags and /api/chat (#13659)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 42158ae2 server : fix first message identification (#13634)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 797f2ac0 kv-cache : simplify the interface (#13660)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b44890df model : disable SWA for Phi models (#13676)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 33983057 musa: Upgrade MUSA SDK version to rc4.0.1 and use mudnn::Unary::IDENTITY op to accelerate D2D memory copy (#13647)
* | | | | | | | | | | | | | | | | | | | | | | | | | | fb1cab20 vulkan: fix warnings (#13626)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b7a17463 mtmd-helper : bug fix to token batching in mtmd (#13650)
* | | | | | | | | | | | | | | | | | | | | | | | | | | be023969 model : fix llama4 graph (#13663)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a4090d11 llama : remove llama_kv_cache_view API + remove deprecated (#13653)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b69f1647 CUDA: skip fully masked-out KV in FA vec kernel (#13584)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 759e37b0 tests : avoid github urls due to throttling (#13654)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 4245e622 sycl: disable reorder for sycl mulmat (#13536)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c9c64dee Set GLM4 blk.*.attn_output.weight, kqv_out-* matmul to GGML_PREC_F32 to fix infinity values in output (#13639)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c00a2634 metal : fix typo in FA kernel comments (#13651)
* | | | | | | | | | | | | | | | | | | | | | | | | | | e298d2fb kv-cache : add SWA support (#13194)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f0adb80b CANN: Update CANN model support (#13162)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f7c9429c sycl : Overcoming workaround for mmap() allocation on Windows (#13482)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 1dfbf2cf common : add load_progress_callback (#13617)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * b06a954b llama_encode : only force non-causal attention for enc-dec models
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8960efd0 Vulkan: Add f32 accumulator support to quantized mul mat to fix GLM4 32B incoherence (#13607)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 725f23f1 sycl : backend documentation review (#13544)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 92ecdcc0 mtmd : add vision support for llama 4 (#13282)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f71f40a2 ci : upgraded oneAPI version in SYCL workflows and dockerfile (#13532)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d30cb5a7 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6c35981a mnist: fix segmentation fault (ggml/1227)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8b5e19ae ggml : fix apple OS check in ggml_print_backtrace (ggml/1229)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 60aea028 ggml : Fix missing backtrace on Linux (ggml/1228)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9c55e5c5 fix: check model pointer validity before use (#13631)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 33d7aed4 CANN: Support MOE Model MUL_MAT_ID (#13042)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6a2bc8bf server : added --no-prefill-assistant flag (#13608)
* | | | | | | | | | | | | | | | | | | | | | | | | | | e3a7cf6c cmake: use the current build config for vulkan-shaders-gen (#13595)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 518329b2 parallel : add option for non-shared and larger prompts (#13598)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2f5a4e1e vulkan: move common FA code to flash_attn_base.comp (#13556)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 4f41ee11 vulkan: use scalar FA rather than coopmat2 when N==1 (#13554)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3e0be1ca llguidance : official v0.7.20 release (no actual changes) [noci] (#13594)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6aa892ec server : do not return error out of context (with ctx shift disabled) (#13577)
* | | | | | | | | | | | | | | | | | | | | | | | | | | aea9f8b4 webui : improve accessibility for visually impaired people (#13551)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 06c1e4ab readme : add list of dependencies and their license (#13591)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 415e40a3 releases : use arm version of curl for arm releases (#13592)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 654a6779 metal : add FA-vec kernel for head size 64 (#13583)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5364ae4b llama : print hint when loading a model when no backends are loaded (#13589)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7c07ac24 ci : add ppc64el to build-linux-cross (#13575)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0a338ed0 sycl : fixed compilation warnings (#13582)
* | | | | | | | | | | | | | | | | | | | | | | | | | | bc098c3c minja: sync (qwen3) (#13573)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c6a2c9e7 gguf : use ggml log system (#13571)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 07ad2b6d gguf-py : fix disconnect-before-connect in editor-gui (#13569)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c531edfa convert : fix conversion for llama 4 (#13567)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 02cdd2d8 sycl: simplify bin_bcast_kernel (#13383)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 64bb51cf sycl: reordered Q4_K MMVQ (#13109)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9c404ed5 sycl: use oneDNN for matrices multiplication (#12972)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6c8b9150 llama-bench : fix -ot with dl backends (#13563)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3cc1f1f1 webui : handle PDF input (as text or image) + convert pasted long content to file (#13562)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c753d7be server : proper error handling for missing elements in messages array (OpenAI compatible backend) (#13540)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b2838049 bench : handle decode errors (#13548)
* | | | | | | | | | | | | | | | | | | | | | | | | | | aa48e373 `server`: inject date_string in llama 3.x template + fix date for firefunction v2 (#12802)
* | | | | | | | | | | | | | | | | | | | | | | | | | | e3a9421b kv-cache : fix out-of-bounds view during reserve graph (#13547)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5ab5d5fb arm64: optimize q6_k_q8_k kernel with i8mm (#13519)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 8282d746 bench : handle decode errors
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3198405e `common`: add partial regex support (#12808)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f5170c1d editorconfig : fix trailing whitespace from #13542 (#13546)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 017f10b5 fix: crash when calling `llama_state_get_size` on a context without a KV cache (#13542)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 4696d567 CUDA: fix crash on large batch size for quant. MoE (#13537)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b7d26720 llama : fix quantize with dl backends (#13539)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6da34fa2 CUDA: faster Deepseek FA, add Turing support (#13435)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5e7d95e2 fix: Move build_inp_pos to the top of the graph section for build_granite (#13538)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 05317443 server : passthrough the /models endpoint during loading (#13535)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 360a9c98 server : fix cache_tokens bug with no cache_prompt (#13533)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 237acc7c server : update readme + return json for "meta" field
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 6190e1c1 server : passthrough the /models endpoint during loading
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 09d13d94 cmake: simplify vulkan shader test logic (#13263)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 24e86cae vulkan: KHR_coopmat flash attention (#13506)
* | | | | | | | | | | | | | | | | | | | | | | | | | | bb1681fb webui : use fflate for more deterministic gzip compress (#13525)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d486dd3e webui: Allow pasting file from clipboard (#13526)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 21ca987f docs: Update link to ggml-org in multimodal.md (#13513)
* | | | | | | | | | | | | | | | | | | | | | | | | | | be1d4a13 scripts : fix compare-llama-bench.py show parameter (#13514)
* | | | | | | | | | | | | | | | | | | | | | | | | | | ab3971f2 vulkan: workaround FA compile failures on macos (#13517)
* | | | | | | | | | | | | | | | | | | | | | | | | | | e5c834f7 quantize : improve tensor-type pattern matching (#13033)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 71bdbdb5 clip : clip.h become private API (⚠️ breaking change) (#13510)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f0995d28 metal : use FA-vec kernel up to batch size 20 (#13496)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c252e0c4 metal : optimize multi-sequence FA vec kernel (#13493)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 4f711afe ggml-cpu: Update KleidiAI to v1.6 and fix include directives (#13509)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b89d605a batched-bench : fix pp batch contents (#13492)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b4726345 mtmd : remove libllava, remove clip-quantize-cli (⚠️ breaking change) (#13460)
* | | | | | | | | | | | | | | | | | | | | | | | | | | bf793711 scripts : support arbitrary input file formats in compare-llama-bench.py (#13455)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d590cd4c model : Granite MoE shared (#13269)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 1e2809bc sync : ggml
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 78d70223 metal : use FA-vec kernel up to batch size 20
| | | | | | | | | | | | | | | | | | | | | | | | | | | * fdfc7de7 metal : optimize multi-sequence FA vec kernel
| | | | | | | | | | | | | | | | | | | | | | | | | | | * f078c798 batched-bench : fix pp batch contents
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | cf0a43bb llama-bench : add defrag-thold, check for invalid ranges (#13487)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f0d46ef1 opencl: remove unnecessary assert for `add` (#13257)
* | | | | | | | | | | | | | | | | | | | | | | | | | | de4c07f9 clip : cap max image size 1024 for qwen vl model (#13478)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 10d2af0e llama/ggml: add LLM training support (#10544)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 064cc596 context : fix state io for memory-less contexts (#13470)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 91159ee9 server : allow content to be null in oaicompat_completion_params_parse (#13477)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 22cdab34 llama-bench : accept ranges for integer parameters (#13410)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a71a4075 ggml-cpu: Integrate fp32=bf16xbf16 SME KleidiAI kernel (#13053)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 95e18884 CUDA: fix misaligned synchronization in FA (#13469)
* | | | | | | | | | | | | | | | | | | | | | | | | | | df849192 ggml : add mrope kernel for metal (#13457)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 14492144 enable dpcpp nightly builds with libraries (#13406)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c1040239 mtmd : Use RMS norm for InternVL 3 38B and 78B mmproj (#13459)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9a390c48 tools : fix uninitialized llama_batch in server (#13436)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 09232370 scripts : exit compare-llama-bench.py gracefully when there's nothing to compare (#13451)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7474e00b CUDA: fix crash with partial offloading of MoE (#13439)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7f323a58 Add `--no-op-offload` to improve `-ot` pp perf in MoE models like llama4 400B (#13386)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3eac2093 mtmd : support InternVL 3 38B and 78B mmproj (#13443)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a634d75d mtmd : move helpers to dedicated file (#13442)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 62d4250e docs : Fix typo in InternVL3 model name (#13440)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0208355f CUDA: fix race conditions FlashAttention kernels (#13438)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d2a4ef05 vocab : add ByteDance-Seed/Seed-Coder (#13423)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 15e6125a mtmd : add hard limit on image resolution for qwen2vl / qwen2.5vl (#13434)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3b24d26c server : update docs (#13432)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 43dfd741 llguidance : set tokenizer slices to default (#13424)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b064a51a ci: free_disk_space flag enabled for intel variant (#13426)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 053367d1 mtmd : support InternVL 2.5 and 3 (#13422)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 5c32fc3d Break down main function in llama-server
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | d8919424 CUDA: fix FlashAttention on Turing (#13415)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7fef1176 arg : add env var to control mmproj (#13416)
* | | | | | | | | | | | | | | | | | | | | | | | | | | dc1d2adf vulkan: scalar flash attention implementation (#13324)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 1cba7345 small note about -hf --mmproj
| | | | | | | | | | | | | | | | | | | | | | | | | | | * d3f7a1f9 arg : add env var to control mmproj
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7c28a74e chore(llguidance): use tagged version that does not break the build (#13413)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 33eff402  server : vision support via libmtmd (#12898)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 17512a94 sycl : implementation of reordered Q4_0 MMVQ for Intel GPUs  (#12858)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 611aa914 metal : optimize MoE for large batches (#13388)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0cf6725e CUDA: FA support for Deepseek (Ampere or newer) (#13306)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 27ebfcac llama : do not crash if there is no CPU backend (#13395)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5c86c9ed CUDA: fix crash on large batch size for MoE models (#13384)
* | | | | | | | | | | | | | | | | | | | | | | | | | | efb8b47e imatrix : Add --parse-special for enabling parsing of special tokens in imatrix calculation (#13389)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0527771d llama-run: add support for downloading models from ModelScope (#13370)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2189fd3b mtmd : fix batch_view for m-rope (#13397)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3f96aeff llama : one-off chat template fix for Mistral-Small-2503 (#13398)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b486ba05 rpc : add rpc_msg_set_tensor_hash_req (#13353)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 02115dcd vulkan: Allow up to 4096 elements for mul_mat_id row_ids (#13326)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d9c4acca server : (webui) rename has_multimodal --> modalities (#13393)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 15e03282 ci : limit write permission to only the release step + fixes (#13392)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f05a6d71 mtmd : Expose helper_decode_image_chunk (#13366)
* | | | | | | | | | | | | | | | | | | | | | | | | | | ee01d71e server : (webui) fix a very small misalignment (#13387)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8c83449c server : (webui) revamp the input area, plus many small UI improvements (#13365)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 1a844be1 convert : support rope_scaling type and rope_type (#13349)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0ccc1213 mtmd : fix the calculation of n_tokens for smolvlm (#13381)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6562e5a4 context : allow cache-less context for embeddings (#13108)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 51fb96b1 context : remove logits_all flag (#13284)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 70a6991e ci : move release workflow to a separate file (#13362)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f0610212 llama : print size and type of overridden tensors (#13364)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 6107303a llama : remove logits_all flag + reorder llama_context_params
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 6c0501ad context : remove logits_all flag
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8733e0cf sycl: addressing non-contiguous src1 mul_mats (nc and batched) (#13343)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 814f795e docker : disable arm64 and intel images (#13356)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d8794338 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | 13b0a045 whisper: remove MSVC warnings pragmas (whisper/3090)
* | | | | | | | | | | | | | | | | | | | | | | | | | | bba9d945 cmake : removed stdc++fs (whisper/3097)
* | | | | | | | | | | | | | | | | | | | | | | | | | | bc4e1128 llama : deci : support ffn-free with attention (#13296)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 39e73ae0 common : Add a warning when we can't match samplers from a string or char. (#13330)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 1f73301b cuda : remove nrows_x in mul_mat_q_process_tile (#13325)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 4773d7a0 examples : remove infill (#13283)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6c7fd67b llama : support tie embedding for chatglm models (#13328)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 141a908a CUDA: mix virt/real CUDA archs for GGML_NATIVE=OFF (#13135)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 32916a49 clip : refactor graph builder (#13321)
* | | | | | | | | | | | | | | | | | | | | | | | | | | ffc72720 sampling : make top_n_sigma no-op at <=0 or a single candidate (#13345)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 91a86a6f sampling : don't consider -infinity values in top_n_sigma (#13344)
* | | | | | | | | | | | | | | | | | | | | | | | | | | f4ed10b6 cmake : remove arm64 msvc presets (#13342)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 1e333d5b SYCL: Disable reorder optimize by default and stop setting tensor extras when optimize is disabled (#13254)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2f54e348 llama : fix build_ffn without gate (#13336)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2356fb1d CUDA: fix bad asserts for partial offload (#13337)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 764b8562 convert : qwen2/3moe : set yarn metadata if present (#13331)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 8681d3dd Revert "fix build on windows"
| | | | | | | | | | | | | | | | | | | | | | | | | | | * fc420d3c fix build on windows
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 98ce93e7 llama : fix build_ffn without gate
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 15a28ec8 CUDA: fix --split-mode row for MMQ (#13323)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a7366faa gguf-py : avoid requiring pyside6 for other scripts (#13036)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 90703650 CUDA: fix logic for clearing padding with -ngl 0 (#13320)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 233461f8 sampling : Integrate Top-nσ into main sampling chain (and add it to the server) (#13264)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b34c8591 server : Webui - change setText command from parent window to also send the message. (#13309)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9b61acf0 mtmd : rename llava directory to mtmd (#13311)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5215b91e clip :  fix confused naming ffn_up and ffn_down (#13290)
* | | | | | | | | | | | | | | | | | | | | | | | | | | ae803bfc convert : bailingmoe : set yarn metadata if present (#13312)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 66645a52 SYCL: Disable mul_mat kernels for noncontiguous tensor b (#13308)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 27aa2595 mtmd : add C public API (#13184)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9fdfcdae rpc : use backend registry, support dl backends (#13304)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 6eb7d25c ggml : activate s390x simd for Q3_K (#13301)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 86bd60d3 llava/mtmd : fixes to fully support dl backends (#13303)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9f2da587 llama : build windows releases with dl backends (#13220)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 93c4e239 CUDA: fix race condition in MMQ stream-k fixup (#13299)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8afbd968 CUDA: fix race condition in MMQ ids_dst (#13294)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 8ae5ebcf vulkan: Additional type support for unary, binary, and copy (#13266)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 16843dba metal : pad mm results
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3e959f09 imatrix: fix oob writes if src1 is not contiguous (#13286)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 36667c8e clip : revert the change of BOI/EOI token for GLM-edge (⚠️ breaking change) (#13259)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3bf785f3 llama : Llama-3_1-Nemotron-Ultra-253B-v1 support (#12843)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 1d36b367 llama : move end-user examples to tools directory (#13249)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 15dea7bb opt : remove print [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | | | | * cee751c4 opt : fix n_outputs
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 4e73b81a try CI fix
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 111c9c75 llama/ggml: add LLM training support
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | b3444392 sync : ggml (#13268)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a75cb30d context : fix reorder logic (#13267)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3f3769ba ggml : Enable MMA for BF16 in llamafile_sgemm (#13148)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2f567611 llama-model : support Qwen2 embedding models and pooling_mode_lasttoken (#13245)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7d212348 convert : use correct context length for nomic-embed-text-v2 (#13216)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 074e42ab convert : converting mmproj for Qwen2/2.5VL from convert_hf_to_gguf (#13209)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | c642bc01 kv-cache : separate recurrent vs non-recurrent impl (#12799)
* | | | | | | | | | | | | | | | | | | | | | | | | | cb06a3c3 llama : orion rope type is neox (#13261)
* | | | | | | | | | | | | | | | | | | | | | | | | | 626083fa llama : plamo rope type is neox (#13260)
* | | | | | | | | | | | | | | | | | | | | | | | | | 2af68801 llama-chat : reset glmedge chat template (#13253)
* | | | | | | | | | | | | | | | | | | | | | | | | | e84773ab mtmd-cli : fix out_of_range when input image path is empty (#13244)
* | | | | | | | | | | | | | | | | | | | | | | | | | fab647e8 server : add cache reuse card link to help (#13230)
* | | | | | | | | | | | | | | | | | | | | | | | | | dcf88600 convert : explicitly disable trust_remote_code for AutoConfig (#13246)
* | | | | | | | | | | | | | | | | | | | | | | | | | d24d5928 ci: fix cross-compile sync issues (#12804)
* | | | | | | | | | | | | | | | | | | | | | | | | | 8efbdadc rpc : avoid uninitialized memory in serialize_tensor (#13210)
* | | | | | | | | | | | | | | | | | | | | | | | | | f057808f ggml: Don't assert fail when tensor data changes (#13222)
* | | | | | | | | | | | | | | | | | | | | | | | | | d7a14c42 build : fix build info on windows (#13239)
* | | | | | | | | | | | | | | | | | | | | | | | | | b6e4ff69 clip : (minicpmv) Re-enable upscaling of images smaller than the CLIP image size (#13237)
* | | | | | | | | | | | | | | | | | | | | | | | | | e0f572c8 llama-chat : update GLM4 chat template (#13238)
* | | | | | | | | | | | | | | | | | | | | | | | | | 79f26e9e vulkan: Add bfloat16 support (#12554)
* | | | | | | | | | | | | | | | | | | | | | | | | | fc727bcd vulkan: Handle src1 batch dimension in non-contiguous mat-vec-mul shader (#13191)
* | | | | | | | | | | | | | | | | | | | | | | | | | b0ecbd43 test: non-cont. b in test-backend-ops -o MUL_MAT (#13187)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | b1dd4d08 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | 99881f77 whisper : add check that target name exists (whisper/3103)
* | | | | | | | | | | | | | | | | | | | | | | | | b5769d92 ggml : suppress Windows compiler warnings (whisper/3075)
* | | | | | | | | | | | | | | | | | | | | | | | | 8936784f mtmd : add **vision** support for Mistral Small 3.1 (#13231)
* | | | | | | | | | | | | | | | | | | | | | | | | 13c9a331 arg : remove CURLINFO_EFFECTIVE_METHOD (#13228)
* | | | | | | | | | | | | | | | | | | | | | | | | a70183eb llama-model : fix the reported size class for nomic-embed-text-v2-moe (#13223)
* | | | | | | | | | | | | | | | | | | | | | | | | 8d33d740 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | 4254bb49 ggml : fix ggml_gallocr_ptr type (ggml/1205)
* | | | | | | | | | | | | | | | | | | | | | | | | 99985401 cuda : fix unused variable compile warning (whisper/0)
| | | | | | | | | | | | | | | | | | | | | | | | | * 65202d29 sync : ggml
| | | | | | | | | | | | | | | | | | | | | | | | | * db1ff5b6 ggml : fix ggml_gallocr_ptr type (ggml/1205)
| | | | | | | | | | | | | | | | | | | | | | | | | * 610df4cc cuda : fix unused variable compile warning (whisper/0)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | e1e8e099 CUDA: batched+noncont MMQ, refactor bs>1 MoE code (#13199)
* | | | | | | | | | | | | | | | | | | | | | | | | 6f67cf1f arg : -hf do not fail if url mismatch (#13219)
* | | | | | | | | | | | | | | | | | | | | | | | | 16a457fa fix typo: `n_ctx_pre_seq` -> `n_ctx_per_seq` (#13221)
* | | | | | | | | | | | | | | | | | | | | | | | | 3e168bed convert : improve model arch handling (#13122)
* | | | | | | | | | | | | | | | | | | | | | | | | ceda28ef llava : remove duplicate include (#13207)
* | | | | | | | | | | | | | | | | | | | | | | | | 3b127c73 common : add -jf / --json-schema-file flag (#12011)
* | | | | | | | | | | | | | | | | | | | | | | | | e5007a5e vulkan: use uint array index to avoid glslang bug (#13193)
* | | | | | | | | | | | | | | | | | | | | | | | | 41631377 ggml : fix ppc64le build (#13176)
* | | | | | | | | | | | | | | | | | | | | | | | | 07c2e2f7 convert : correct typo image_mean --> image_std (#13208)
* | | | | | | | | | | | | | | | | | | | | | | | | 44cd8d91 feat(ggml-cpu): enable z17 compile (#13182)
* | | | | | | | | | | | | | | | | | | | | | | | | 5933e6fd arg : allow using -hf offline (#13202)
* | | | | | | | | | | | | | | | | | | | | | | | | da84c04d docker : do not build tests (#13204)
* | | | | | | | | | | | | | | | | | | | | | | | | a0f7016d rpc : fix cache directory initialization (#13188)
* | | | | | | | | | | | | | | | | | | | | | | | | 19e899ce scripts: n_depth for compare-llama-bench [no ci] (#13201)
* | | | | | | | | | | | | | | | | | | | | | | | | e2e1ddb9 server : Prefilling assistant message in openai compatible API (#13174)
* | | | | | | | | | | | | | | | | | | | | | | | | d9d398f8 sampling : when top-k <= 0 -> noop (#13173)
* | | | | | | | | | | | | | | | | | | | | | | | | 5a639801 llama-bench: fixed size of fields to correctly map to values (#13183)
* | | | | | | | | | | | | | | | | | | | | | | | | cdf76586 CUDA: fix non-cont. inputs for batched mat mul (#13155)
* | | | | | | | | | | | | | | | | | | | | | | | | 7d3af70b llama : llm_type order by size (#13177)
* | | | | | | | | | | | | | | | | | | | | | | | | 00e3e5a1 mtmd : add qwen2vl and qwen2.5vl (#13141)
* | | | | | | | | | | | | | | | | | | | | | | | | e98b3692 llama : set qwen3 model type sizes (#13175)
* | | | | | | | | | | | | | | | | | | | | | | | | b6ce7430 llama-graph : fix text position for mrope (#13159)
* | | | | | | | | | | | | | | | | | | | | | | | | 5f5e39e1 model : Nomic Embed Text V2 with Mixture-of-Experts (MoE) architecture (#12466)
* | | | | | | | | | | | | | | | | | | | | | | | | eaea3253 clip : fix model size display (#13153)
* | | | | | | | | | | | | | | | | | | | | | | | | 43ddab6e fix(rpc): Improve input validation and error handling (#13069)
* | | | | | | | | | | | | | | | | | | | | | | | | 1831f538 llama-bench: add `-d` depth arg (#13096)
* | | | | | | | | | | | | | | | | | | | | | | | | 4e87962e mtmd : fix glm-edge redundant token count (#13139)
* | | | | | | | | | | | | | | | | | | | | | | | | fb0471d1 context : do not clear output buffer on reserve (#13152)
* | | | | | | | | | | | | | | | | | | | | | | | | d2b2031e llama : (mrope) allow using normal 1D position for text token (#13138)
* | | | | | | | | | | | | | | | | | | | | | | | | 5fa9e63b clip : refactor set input for cgraph + fix qwen2.5vl input (#13136)
* | | | | | | | | | | | | | | | | | | | | | | | | a4c340f9 SYCL: Add all missing unary kernels (#13074)
* | | | | | | | | | | | | | | | | | | | | | | | | d0a417f3 readme : update hot topics (#13150)
* | | | | | | | | | | | | | | | | | | | | | | | | 43f2b071 common : fix noreturn compile warning (#13151)
* | | | | | | | | | | | | | | | | | | | | | | | | e5d6c255 llama-chat : fix typo GML --> GLM (#13143)
| | | | | | | | | | | | | | | | | | | | | | | | | * b7107583 readme : update hot topics
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | f0dd6a19 musa: fix typo in cc control (#13144)
* | | | | | | | | | | | | | | | | | | | | | | | | 69699be4 CUDA: fix q_nope_absorbed prec for DS 2 Lite f16 (#13137)
* | | | | | | | | | | | | | | | | | | | | | | | | 85f36e5e arg : fix unused variable (#13142)
* | | | | | | | | | | | | | | | | | | | | | | | | c0a97b76 llama-bench : Add `--override-tensors` arg (#12922)
* | | | | | | | | | | | | | | | | | | | | | | | | ced44be3 llama-chat : fix wrong template in GLM4-0414 (#13140)
* | | | | | | | | | | | | | | | | | | | | | | | | e291450b musa: fix build warning (#13129)
* | | | | | | | | | | | | | | | | | | | | | | | | 59e991c2 Fixes Qwen2.5VL segfault during inference with https://github.com/ggml-org/llama.cpp/pull/12402 as has_qwen2vl_merger migration was incomplete (#13133)
| | | | | | | | | | | | | | | | | | | | | | | | | * 37ae6a28 Fixes Qwen2.5VL segfault during inference with https://github.com/ggml-org/llama.cpp/pull/12402 as has_qwen2vl_merger migration was incomplete
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | ca2bb89e clip : Add Qwen2.5VL support (#12402)
* | | | | | | | | | | | | | | | | | | | | | | | | 2d451c80 common : add common_remote_get_content (#13123)
* | | | | | | | | | | | | | | | | | | | | | | | | 4753791e clip : improve projector naming (#13118)
* | | | | | | | | | | | | | | | | | | | | | | | | 77d5e9a7 ggml: move fp16/bf16 conversion optimizations to CPU backend + export conversion APIs (#13107)
* | | | | | | | | | | | | | | | | | | | | | | | | d5fe4e81 grammar : handle maxItems == 0 in JSON schema (#13117)
* | | | | | | | | | | | | | | | | | | | | | | | | 295354ea llama : fix K-shift with quantized K and BLAS backend (#13113)
* | | | | | | | | | | | | | | | | | | | | | | | | 558a7647 Force FP32 compute in GLM4 FFN Down (#13101)
* | | | | | | | | | | | | | | | | | | | | | | | | edb18b6e clip : fix pixtral on some GPU backends (#13097)
* | | | | | | | | | | | | | | | | | | | | | | | | 514c4560 change the reorder tensor from init to execute OP (#13003)
* | | | | | | | | | | | | | | | | | | | | | | | | 553a5c3a rpc : do not wait for response when sending RPC_CMD_SET_TENSOR (#12943)
* | | | | | | | | | | | | | | | | | | | | | | | | 13be08da clip : remove boi/eoi embeddings for GLM-edge model (#13081)
* | | | | | | | | | | | | | | | | | | | | | | | | 226251ed embeddings : fix batch sizes (#13076)
* | | | | | | | | | | | | | | | | | | | | | | | | 87616f06 ggml : fix trailing whitespaces (#0)
* | | | | | | | | | | | | | | | | | | | | | | | | 63b49114 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | c6e8cc28 ggml : Depthwise 2D convolution (ggml/1152)
* | | | | | | | | | | | | | | | | | | | | | | | | b10d8bfd CUDA: use switch statements in constexpr functions (#13095)
* | | | | | | | | | | | | | | | | | | | | | | | | 13b45488 cmake : do not include ./src as public for libllama (#13062)
* | | | | | | | | | | | | | | | | | | | | | | | | 572b3141 clang-tidy : disable warning about missing math parenthesis (#13091)
* | | | | | | | | | | | | | | | | | | | | | | | | 7c727fbe arg : add --no-mmproj-offload (#13093)
* | | | | | | | | | | | | | | | | | | | | | | | | 80982e81 arg : clean up handling --mmproj with -hf (#13082)
* | | | | | | | | | | | | | | | | | | | | | | | | 7604a7d6 metal : fix floating-point range of attention scores in FA kernels (#13090)
* | | | | | | | | | | | | | | | | | | | | | | | | b3b6d862 vulkan: matmul gcn tuning (#13016)
* | | | | | | | | | | | | | | | | | | | | | | | | 56304069 llama-mtmd-cli: Sigint rework in mtmd vision example (#13080)
| | | | | | | | | | | | | | | | | | | | | | | | | * ed68474f wip
| | | | | | | | | | | | | | | | | | | | | | | | | * a06d9a03 media : testing [no ci]
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | ecda2ec4 mtmd : Support Pixtral 12B (#13065)
* | | | | | | | | | | | | | | | | | | | | | | | | eb1776b1 convert : Append mult-eos,half-rope,bos to GLM4-0414 and Z (#13021)
* | | | | | | | | | | | | | | | | | | | | | | | | 2cca6c01 rpc : add command line option for number of threads for the CPU backend (#13060)
* | | | | | | | | | | | | | | | | | | | | | | | | 658987cf CUDA: noncont MMVQ + batched bs1 MUL_MAT_ID (#13014)
* | | | | | | | | | | | | | | | | | | | | | | | | dc39a5e7 mtmd : support SmolVLM (version 1 and 2) (#13050)
* | | | | | | | | | | | | | | | | | | | | | | | | ab47dec3 security : add note about RPC and server functionality (#13061)
* | | | | | | | | | | | | | | | | | | | | | | | | 7b53389c metal : add memory pool for temp allocs (#12850)
* | | | | | | | | | | | | | | | | | | | | | | | | 24345353 llava : update documentations (#13055)
* | | | | | | | | | | | | | | | | | | | | | | | | 1d735c0b ggml : add SSE 4.2 and x64 base variant for CPUs without AVX (#12871)
* | | | | | | | | | | | | | | | | | | | | | | | | 5368ddda SYCL: Add non-contiguous support in ROPE (#12993)
* | | | | | | | | | | | | | | | | | | | | | | | | 84a9bf2f mtmd : merge llava, gemma3 and minicpmv CLI into single `llama-mtmd-cli` (#13012)
* | | | | | | | | | | | | | | | | | | | | | | | | 2016f07b convert : experimental support for `--mmproj` flag (#13023)
* | | | | | | | | | | | | | | | | | | | | | | | | 66023048 llava: fix errors in clip.h on certain compilers (#13030)
* | | | | | | | | | | | | | | | | | | | | | | | | 66168204 vulkan: support noncontiguous rms_norm (#13031)
* | | | | | | | | | | | | | | | | | | | | | | | | 4ba9d711 metal: add neg operator (#13029)
* | | | | | | | | | | | | | | | | | | | | | | | | 00137157 Disable CI cross-compile builds (#13022)
* | | | | | | | | | | | | | | | | | | | | | | | | fb28f4f8 gguf-py : fix upload python package workflow (#13020)
* | | | | | | | | | | | | | | | | | | | | | | | | 37b9f0d2 clip : refactor, add `image_manipulation` and `llava_uhd` classes (#13011)
* | | | | | | | | | | | | | | | | | | | | | | | | 64082100 main : Fix Ctrl+D/newline handling (#12951)
* | | | | | | | | | | | | | | | | | | | | | | | | aff9d107 gguf-py : GGUF Editor GUI - Python + Qt6 (#12930)
* | | | | | | | | | | | | | | | | | | | | | | | | 35370ba9 server : use std::move whenever possible (#12936)
* | | | | | | | | | | | | | | | | | | | | | | | | 8d660057 SYCL: Refactor and enable FP16 in binary broadcast OPs (#12975)
* | | | | | | | | | | | | | | | | | | | | | | | | b9154ecf mtmd : add methods to access `mtmd_image_tokens` (#12906)
* | | | | | | | | | | | | | | | | | | | | | | | | 2db9ba14 rpc : add RPC_CMD_HELLO (#12955)
* | | | | | | | | | | | | | | | | | | | | | | | | 2f74c354 graph : make FA compatible with MLA + add initial Metal kernels (#12953)
* | | | | | | | | | | | | | | | | | | | | | | | | 207c22ec ggml: Re-enable CUDA graphs in presence of CONT and DUP nodes (#12970)
* | | | | | | | | | | | | | | | | | | | | | | | | 7a395f67 CANN: Add support for async operator submission (#12864)
* | | | | | | | | | | | | | | | | | | | | | | | | 971f245b llama : recognize IBM Granite 3.3 FIM tokens (#12988)
* | | | | | | | | | | | | | | | | | | | | | | | | 12b17501 opencl: fix incorrect local_size index in profiling log (#12868)
* | | | | | | | | | | | | | | | | | | | | | | | | 015022bb vulkan: enable coopmat2 FA gqa and split_k optimizations more often (#12931)
* | | | | | | | | | | | | | | | | | | | | | | | | b43d89e3 CANN: Add 310P operator support check (#12962)
* | | | | | | | | | | | | | | | | | | | | | | | | 80f19b41 opencl: split `ggml-opencl.cl` into multiple files and cleanup (#12886)
* | | | | | | | | | | | | | | | | | | | | | | | | f8f820cc metal : add FA-vec kernels for head size 96 (#12952)
* | | | | | | | | | | | | | | | | | | | | | | | | 54a72720 CANN: Add x86 build ci (#12950)
* | | | | | | | | | | | | | | | | | | | | | | | | 84778e97 CUDA/HIP: Share the same unified memory allocation logic. (#12934)
* | | | | | | | | | | | | | | | | | | | | | | | | 51067647 SYCL: Add ROPE vision kernel (#12887)
* | | | | | | | | | | | | | | | | | | | | | | | | daa42288 llama : DeepSeek V2/V3 MLA implementation (#12801)
* | | | | | | | | | | | | | | | | | | | | | | | | eccc7a16 ggml : Add AVX512 implementation of GEMM - Q4_Kx8 (#12829)
* | | | | | | | | | | | | | | | | | | | | | | | | 0019279b CANN: Opt ROPE optimization (#12865)
* | | | | | | | | | | | | | | | | | | | | | | | | b0c75ac9 CANN: Optimize CANN buffer pool memory management (#12875)
* | | | | | | | | | | | | | | | | | | | | | | | | d6d2c2ab Add performance print for gemma3 in example (#12929)
* | | | | | | | | | | | | | | | | | | | | | | | | 75afa0ae SYCL: Fix im2col (#12910)
* | | | | | | | | | | | | | | | | | | | | | | | | c772d549 rpc : use ggml_context_ptr (#12938)
* | | | | | | | | | | | | | | | | | | | | | | | | 81c7e64f dsiable curl lib check, this action is missed by commit bd3f59f81289b920bcc597a208c14f55e39ed37e (#12761) (#12937)
* | | | | | | | | | | | | | | | | | | | | | | | | 526739b8 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | a25355e2 cpu: fix cpu backend's supports-op for GET_ROWS_BACK. fixes a fatal when running test-backend-ops with only the CPU backend (ggml/1190)
* | | | | | | | | | | | | | | | | | | | | | | | | e959d32b ggml: use _mm[512/256]_dpbusd[_avx]_epi32 to directly accumulate into the result register (#12773)
* | | | | | | | | | | | | | | | | | | | | | | | | 307bfa25 ggml: disable CUDA graphs for unsupported DUP and CONT node types (#12891)
* | | | | | | | | | | | | | | | | | | | | | | | | 71e90e88 quantize: Handle user-defined quantization levels for additional tensors (#12511)
| |_|_|_|/ / / / / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | bc091a4d common : Define cache directory on AIX (#12915)
* | | | | | | | | | | | | | | | | | | | | | | | a4837577 vulkan: use aligned loads for flash attention mask (#12853)
* | | | | | | | | | | | | | | | | | | | | | | | e59ea539 llava: Fix cpu-only clip image encoding sefault (#12907)
* | | | | | | | | | | | | | | | | | | | | | | | c94085df server : add VSCode's Github Copilot Chat support (#12896)
* | | | | | | | | | | | | | | | | | | | | | | | e8a62631 rpc : Set cache directory in rpc-server.cpp on FreeBSD (#12903)
* | | | | | | | | | | | | | | | | | | | | | | | b6930ebc `tool-call`: fix non-tool-calling grammar crashes w/ Qwen / Hermes 2 templates (#12900)
* | | | | | | | | | | | | | | | | | | | | | | | 68b08f36 common : Define cache directory on FreeBSD (#12892)
| | | | | | | | | | | | | | | | | | | | | | | | * 3fe362fe gguf-py : use ThreadPoolExecutor when writing tensors
| | | | | | | | | | | | | | | | | | | | | | | | *   d7db1593 Merge branch 'master' into compilade/parallel-convert
| | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | 578754b3 sycl: Support sycl_ext_oneapi_limited_graph (#12873)
* | | | | | | | | | | | | | | | | | | | | | | | | b2034c2b contrib: support modelscope community (#12664)
* | | | | | | | | | | | | | | | | | | | | | | | | 06bb53ad llama-model : add Glm4Model implementation for GLM-4-0414 (#12867)
* | | | | | | | | | | | | | | | | | | | | | | | | 0c509239 clip : use smart pointer (⚠️ breaking change) (#12869)
* | | | | | | | | | | | | | | | | | | | | | | | | fccf9cae SYCL: Add fp16 type support to unary op kernels (#12788)
* | | | | | | | | | | | | | | | | | | | | | | | | ec6c09d0 convert : Llama4 RoPE fix (#12889)
* | | | | | | | | | | | | | | | | | | | | | | | | 8ac9f5d7 ci : Replace freediskspace to free_disk_space in docker.yml (#12861)
* | | | | | | | | | | | | | | | | | | | | | | | | 12e9158f xcf : add check for visionos build version (#12854)
* | | | | | | | | | | | | | | | | | | | | | | | | 5b1f13cb convert : proper tensor name mapping for llama4 (#12870)
* | | | | | | | | | | | | | | | | | | | | | | | | 8b91d535 llama : correct rms norm for llama 4 (#12882)
* | | | | | | | | | | | | | | | | | | | | | | | | 0fed24c3 ggml: fix compilation error s390x (#12848)
* | | | | | | | | | | | | | | | | | | | | | | | | 47ba87d0 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | 1d2b6134 tests : fix init order (#0)
* | | | | | | | | | | | | | | | | | | | | | | | | eb420e11 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | cb79c2e7 ggml: don't include arm_neon.h when using CUDA 12 with ARM Neon (ggml/1187)
* | | | | | | | | | | | | | | | | | | | | | | | | fe92821e ggml : add bilinear upscale support (ggml/1185)
* | | | | | | | | | | | | | | | | | | | | | | | | 459895c3 ggml : add more generic custom op, remove deprecated custom ops (ggml/1183)
* | | | | | | | | | | | | | | | | | | | | | | | | e4bf72d6 scripts : fix sync-ggml-am.sh
* | | | | | | | | | | | | | | | | | | | | | | | | 8b9cc7cd llava : introduce libmtmd (#12849)
* | | | | | | | | | | | | | | | | | | | | | | | | 64eda5de convert : ability to lazy-load safetensors remotely without downloading to disk (#12820)
| | | | | | | | | | | | | | | | | | | | | | | | * d8bab9ef gguf-py : add more clarifying comments for multi-thread writes
| | | | | | | | | | | | | | | | | | | | | | | | * 06e1d311 convert : write tensors in parallel
| | | | | | | | | | | | | | | | | | | | | | | | | * 098f0e5e test
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | fe5b78c8 CANN: Support more ops (#12841)
* | | | | | | | | | | | | | | | | | | | | | | | | 11d07e1e Fixes #12823 (#12830)
* | | | | | | | | | | | | | | | | | | | | | | | | b0091ecc docker : added all CPU to GPU images (#12749)
* | | | | | | | | | | | | | | | | | | | | | | | | 31f7803b ggml-cpu-impl.h: do not redefine bool on POWER9 (#12856)
* | | | | | | | | | | | | | | | | | | | | | | | | 2391506a ggml-impl.h: fix build on POWER9 (#12855)
* | | | | | | | | | | | | | | | | | | | | | | | | d3bd7193 llama : Support Qwen3 and Qwen3MoE (#12828)
* | | | | | | | | | | | | | | | | | | | | | | | | d9a63b2f musa: enable freediskspace for docker image build (#12839)
* | | | | | | | | | | | | | | | | | | | | | | | | 8ed71242 sycl: update documentation to use -no-cnv (#12845)
* | | | | | | | | | | | | | | | | | | | | | | | | 381603a7 ci: detach common from the library (#12827)
* | | | | | | | | | | | | | | | | | | | | | | | | 65a69e6e clip : do not print ftype (#12832)
* | | | | | | | | | | | | | | | | | | | | | | | | 47277d6d readme : add rpc backend (#12842)
* | | | | | | | | | | | | | | | | | | | | | | | | 6e1c4ceb CANN: Support Opt CONV_TRANSPOSE_1D and ELU (#12786)
* | | | | | | | | | | | | | | | | | | | | | | | | 0090950f vulkan: In coopmat2 mmq, load q4_k/q5_k scales through shared memory (#12833)
* | | | | | | | | | | | | | | | | | | | | | | | | 7ecd780b vulkan: Use fp16 for the flash attention P*V multiplication (#12783)
* | | | | | | | | | | | | | | | | | | | | | | | | 7538246e cuda : add f32 to bf16 copy op (#12806)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | b32efad2 llava: improve clip_ctx destructor to not memleak load_image_size (#12834)
* | | | | | | | | | | | | | | | | | | | | | | | a19b5cef llama : fix FA when KV cache is not used (i.e. embeddings) (#12825)
* | | | | | | | | | | | | | | | | | | | | | | | 78a1ba0a server : fix thread.join() on exit (#12831)
* | | | | | | | | | | | | | | | | | | | | | | | 2dabf759 llava: add more helper functions to check projector types in clip context (#12824)
* | | | | | | | | | | | | | | | | | | | | | | | 1d343b40 arg : Including limits file on AIX (#12822)
* | | | | | | | | | | | | | | | | | | | | | | | 8ca6e1c3 server : webui : Improve Chat Input with Auto-Sizing Textarea (#12785)
* | | | | | | | | | | | | | | | | | | | | | | | 656babd6 Revert "sycl:remove redundant memcopy in function ggml_backend_sycl_buffer_set_tensor" (#12812)
* | | | | | | | | | | | | | | | | | | | | | | | a226bc7a gguf-py : support lazy tensor splitting (#12809)
| | | | | | | | | | | | | | | | | | | | | | | | * e9e1882d rm tail space
| | | | | | | | | | | | | | | | | | | | | | | | * 76f2ed3d Update ggml/src/ggml-sycl/ggml-sycl.cpp
| | | | | | | | | | | | | | | | | | | | | | | | * d271172a Update ggml/src/ggml-sycl/ggml-sycl.cpp
| | | | | | | | | | | | | | | | | | | | | | | | * 564a05da Revert "sycl: remove redundant memcopy in function ggml_backend_sycl_buffer_s…"
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | * da140da7 gguf-py : fix flake8 lint
| | | | | | | | | | | | | | | | | | | | | | | | * 6cbbd8e1 gguf-py : support lazy tensor splitting
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 1466621e llama : Support llama 4 text-only (#12791)
* | | | | | | | | | | | | | | | | | | | | | | | 82974011 opencl: better identify Adreno GPU (#12760)
* | | | | | | | | | | | | | | | | | | | | | | | 4ccea213 hellaswag: display estimated score confidence interval (#12797)
* | | | | | | | | | | | | | | | | | | | | | | | 1a1ab7e7 cuda : fix HIP and MUSA BF16 (#0)
* | | | | | | | | | | | | | | | | | | | | | | | a4e46e28 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | ff067dbc ggml : simplify Arm fp16 CPU logic (ggml/1177)
* | | | | | | | | | | | | | | | | | | | | | | | 36ca8b36 CUDA: don't convert BF16 weights to FP32 (ggml/1174)
* | | | | | | | | | | | | | | | | | | | | | | | 995083e4 cpu: move all the operators into a separate c++ file (except mul_mat) (ggml/1167)
* | | | | | | | | | | | | | | | | | | | | | | | 518a0148 sycl: remove redundant memcopy in function ggml_backend_sycl_buffer_set_tensor (#12734)
* | | | | | | | | | | | | | | | | | | | | | | | e391d3ee ci : no curl on ggml-ci (#12796)
* | | | | | | | | | | | | | | | | | | | | | | | bd3f59f8 cmake : enable curl by default (#12761)
* | | | | | | | | | | | | | | | | | | | | | | | 52b3d71f CANN: fix typo in ggml-cann (#12733)
| | | | | | | | | | | | | | | | | | | | | | | | * ced26486 cont
| | | | | | | | | | | | | | | | | | | | | | | | * 5ef588ba test
| | | | | | | | | | | | | | | | | | | | | | | | * 6232ceec sync : ggml
| | | | | | | | | | | | | | | | | | | | | | | | * e638450a ggml : simplify Arm fp16 CPU logic (ggml/1177)
| | | | | | | | | | | | | | | | | | | | | | | | * 4683cb40 CUDA: don't convert BF16 weights to FP32 (ggml/1174)
| | | | | | | | | | | | | | | | | | | | | | | | * 53cb49e3 cpu: move all the operators into a separate c++ file (except mul_mat) (ggml/1167)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | d0d5b223 CANN: Refactor to reduce duplicate code (#12731)
* | | | | | | | | | | | | | | | | | | | | | | | 916c83bf musa: fix compilation warnings in mp_22/31 (#12780)
| | | | | | | | | | | | | | | | | | | | | | | | * ab27292b Use dot instruction to multiply two values at once, to enable dual issue instructions
| | | | | | | | | | | | | | | | | | | | | | | | * 039cc074 Vulkan: Remove dedicated aligned matrix matrix multiplication shaders
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 0c74b043 vulkan: fix NaN issue in flash attention shader (#12776)
* | | | | | | | | | | | | | | | | | | | | | | | 80b717d4 vulkan: Use unclamped loads for flash attention mask (#12720)
* | | | | | | | | | | | | | | | | | | | | | | | 6bf28f01 Vulkan: Tune Vulkan mmq int dot shader for performance (#12767)
* | | | | | | | | | | | | | | | | | | | | | | | f1e3eb42 common : fix includes in arg.cpp and gemma3-cli.cpp (#12766)
* | | | | | | | | | | | | | | | | | | | | | | | 0364178c clip : refactor clip_init, add tests (#12757)
* | | | | | | | | | | | | | | | | | | | | | | | c6ff5d2a common: custom hf endpoint support (#12769)
* | | | | | | | | | | | | | | | | | | | | | | | 7a84777f sync: minja (#12739)
* | | | | | | | | | | | | | | | | | | | | | | | 3e1d2934 kv-cache : simplify + fix warning for recurrent models (#12756)
* | | | | | | | | | | | | | | | | | | | | | | | 1be76e46 ci: add Linux cross-compile build (#12428)
* | | | | | | | | | | | | | | | | | | | | | | | b7723942 server : webui : Upgrade daisyui, tailwindcss. (#12735)
* | | | | | | | | | | | | | | | | | | | | | | | 23106f94 gguf-split : --merge now respects --dry-run option (#12681)
* | | | | | | | | | | | | | | | | | | | | | | | 94148ba3 sycl: allow ggml-sycl configuration and compilation using Visual Studio project/solution (#12625)
* | | | | | | | | | | | | | | | | | | | | | | | 9ac4d611 cmake: fix ggml-shaders-gen compiler paths containing spaces (#12747)
| | | | | | | | | | | | | | | | | | | | | | | | * fe564b0d ci : rename job MSVC -> MinGW
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | * 43ab09b8 ci : testing (wip)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 348888e0 docs : add XCFramework section to README.md [no ci] (#12746)
* | | | | | | | | | | | | | | | | | | | | | | | 74d4f5b0 vulkan: Hybrid waitForFences/getFenceStatus to reduce fence latency (#12630)
* | | | | | | | | | | | | | | | | | | | | | | | 35e592eb vulkan: set cmake minimum and project name in vulkan-shaders (#12744)
* | | | | | | | | | | | | | | | | | | | | | | | 7d7b1baf opencl: update doc for OpenCL (#12702)
* | | | | | | | | | | | | | | | | | | | | | | | c262bedd CUDA: Prefer vector flash decoding kernel for Gemma models (#12738)
* | | | | | | | | | | | | | | | | | | | | | | | 5dd5d1ab vocab : use string_view::find() to avoid unnecessary looking up beyond the fragment range (#12706)
* | | | | | | | | | | | | | | | | | | | | | | | 1c059995 vulkan: Fix missing cmake logic for dot product extension (#12721)
* | | | | | | | | | | | | | | | | | | | | | | | 2004644b ci : add env variable in ggml-ci and document the same in SYCL.md (#12736)
* | | | | | | | | | | | | | | | | | | | | | | | 5f696e88 sync : minja (inclusionAI/Ling) and update tests (#12699)
| | | | | | | | | | | | | | | | | | | | | | | | * 7a73e861 cont
| | | | | | | | | | | | | | | | | | | | | | | | * 1b07edfb ggml : trying stuff (wip)
| | | | | | | | | | | | | | | | | | | | | | | | * 819b7d7c sync : ggml
| | | | | | | | | | | | | | | | | | | | | | | | * 0c8dad10 cpu: move all the operators into a separate c++ file (except mul_mat) (ggml/1167)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 193c3e03 fix MUSA compiler warning (#12704)
* | | | | | | | | | | | | | | | | | | | | | | | 65cfe136 CANN: Support operator SIN COS ARGMAX (#12709)
* | | | | | | | | | | | | | | | | | | | | | | | 3f9da22c Simplify and improve CUDA graphs through use of indirect copy pointers (#9017)
* | | | | | | | | | | | | | | | | | | | | | | | 2a0dc97e CANN: Fix failed test cases (#12708)
* | | | | | | | | | | | | | | | | | | | | | | | 97a20c01 opencl: use `max_alloc_size` in backend ctx instead of querying again (#12705)
* | | | | | | | | | | | | | | | | | | | | | | | f01bd023 vulkan: Implement split_k for coopmat2 flash attention. (#12627)
* | | | | | | | | | | | | | | | | | | | | | | | 6f3bd386 cmake: remove caching from vulkan coopmat checks (#12719)
* | | | | | | | | | | | | | | | | | | | | | | | be0a0f8c vulkan: Implement grouped query attention in the coopmat2 FA shader (#12559)
* | | | | | | | | | | | | | | | | | | | | | | | 92e3006b Vulkan: Fix mmq int dot float cache size (#12722)
* | | | | | | | | | | | | | | | | | | | | | | | 833e2b74 model : print tensor size during load (#12711)
* | | | | | | | | | | | | | | | | | | | | | | | e0e912f4 llama : add option to override model tensor buffers (#11397)
* | | | | | | | | | | | | | | | | | | | | | | | a10b36c9 llama : refactor kv cache guard (#12695)
* | | | | | | | | | | | | | | | | | | | | | | | 83a88bd6 vocab : BailingMoE : change possessive quantifiers to greedy (#12677)
* | | | | | | | | | | | | | | | | | | | | | | | 42eb248f common : remove json.hpp from common.cpp (#12697)
* | | | | | | | | | | | | | | | | | | | | | | | 9bacd6b3 [CANN] get_rows and dup optimization (#12671)
* | | | | | | | | | | | | | | | | | | | | | | | 267c1399 common : refactor downloading system, handle mmproj with -hf option (#12694)
* | | | | | | | | | | | | | | | | | | | | | | | f423981a opencl : fix memory allocation size (#12649)
* | | | | | | | | | | | | | | | | | | | | | | | e39e727e llama : use LLM_KV_GENERAL_FILE_TYPE instead of gguf_find_key (#12672)
* | | | | | | | | | | | | | | | | | | | | | | | 5936a616 convert : BailingMoE : fix qkv split when head_dim is 0 (#12687)
* | | | | | | | | | | | | | | | | | | | | | | | 3fd072a5 metal : use F32 prec in FA kernels (#12688)
* | | | | | | | | | | | | | | | | | | | | | | | a6f32f0b Fix clang warning in gguf_check_reserved_keys (#12686)
* | | | | | | | | | | | | | | | | | | | | | | | 2bb3597e vulkan: fix build when glslc doesn't support coopmat (#12683)
* | | | | | | | | | | | | | | | | | | | | | | | 82939705 SYCL: Rename oneMKL to oneMath (#12192)
* | | | | | | | | | | | | | | | | | | | | | | | 8bbf2608 SYCL: switch to SYCL namespace (#12674)
* | | | | | | | | | | | | | | | | | | | | | | | 35782aee convert : BailingMoE : avoid setting rope_dim to 0 (#12678)
* | | | | | | | | | | | | | | | | | | | | | | | c80a7759 vocab : add special infill tokens for CodeLlama (#11850)
* | | | | | | | | | | | | | | | | | | | | | | | 250d7953 ggml : faster ssm scan (#10558)
* | | | | | | | | | | | | | | | | | | | | | | | 403fbacb convert : Qwerky : use lora_rank_tokenshift and lora_rank_decay if present (#12667)
* | | | | | | | | | | | | | | | | | | | | | | | a8a1f335 Vulkan: Add DP4A MMQ and Q8_1 quantization shader (#12135)
* | | | | | | | | | | | | | | | | | | | | | | | 1790e731 cmake : fix whitespace (#0)
* | | | | | | | | | | | | | | | | | | | | | | | 0114a32d sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | a7724480 cmake: improve Vulkan cooperative matrix support checks (whisper/2966)
* | | | | | | | | | | | | | | | | | | | | | | | 1a859490 llava : proper description fix (#12668)
* | | | | | | | | | | | | | | | | | | | | | | | 6c02a032 SYCL: Remove misleading ggml_sycl_op_flatten function (#12387)
* | | | | | | | | | | | | | | | | | | | | | | | f52d59d7 llava : fix clip loading GGUFs with missing description (#12660)
* | | | | | | | | | | | | | | | | | | | | | | | 52de2e59 tts : remove printfs (#12640)
* | | | | | | | | | | | | | | | | | | | | | | | 2c3f8b85 llama : support BailingMoE (Ling) (#12634)
* | | | | | | | | | | | | | | | | | | | | | | | 4663bd35 metal : use constexpr in FA kernels + fix typedef (#12659)
* | | | | | | | | | | | | | | | | | | | | | | | b3de7cac llama : add Trillion 7B model support (#12556)
* | | | | | | | | | | | | | | | | | | | | | | | 7242dd96 llama-chat : Add Yandex instruct model template support (#12621)
* | | | | | | | | | | | | | | | | | | | | | | | 492d7f1f musa: fix all warnings, re-enable `-DLLAMA_FATAL_WARNINGS=ON` in ci and update doc (#12611)
* | | | | | | | | | | | | | | | | | | | | | | | d3f1f0ac sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | 360dc22c cpu : rm unused variable (ggml/1166)
* | | | | | | | | | | | | | | | | | | | | | | | a62d7fa7 cpu: de-duplicate some of the operators and refactor (ggml/1144)
* | | | | | | | | | | | | | | | | | | | | | | | e408d435 ggml : add logging for native build options/vars (whisper/2935)
* | | | | | | | | | | | | | | | | | | | | | | | 3891e183 examples : command.wasm updates (whisper/2904)
* | | | | | | | | | | | | | | | | | | | | | | | af6ae1ef llama : fix non-causal mask for gemma 3 (#12615)
* | | | | | | | | | | | | | | | | | | | | | | | 0bb29193 llama : change cpu_buft_list order: ACCEL -> GPU host -> CPU extra -> CPU (#12632)
* | | | | | | | | | | | | | | | | | | | | | | | a69f8463 cmake : fix ccache conflict (#12522)
* | | | | | | | | | | | | | | | | | | | | | | | d07a0d7a CANN : remove clang-format in ggml-cann (#12607)
* | | | | | | | | | | | | | | | | | | | | | | | 3714c3ee llama : fix incorrect Qwen2Moe ffn_moe_out graph callback (#12631)
* | | | | | | | | | | | | | | | | | | | | | | | b4ae5081 metal : improve FA + improve MoE (#12612)
* | | | | | | | | | | | | | | | | | | | | | | | b86f6007 vulkan: fix coopmat shader generation when cross-compiling (#12272)
* | | | | | | | | | | | | | | | | | | | | | | | dd373dd3 llama: fix error on bad grammar (#12628)
* | | | | | | | | | | | | | | | | | | | | | | | 5d016702 server : include speculative decoding stats when timings_per_token is enabled (#12603)
* | | | | | | | | | | | | | | | | | | | | | | | ef03229f rpc : update README for cache usage (#12620)
* | | | | | | | | | | | | | | | | | | | | | | | 13731766 llamafile : ppc64le GEMV forwarding for FP32. (#12594)
| | | | | | | | | | | | | | | | | | | | | | | | * c875e03f rpc : update README for cache usage
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | ab6ab8f8 rpc : send hash when tensor data is above some fixed threshold (#12496)
* | | | | | | | | | | | | | | | | | | | | | | | 2099a9d5 server : Support listening on a unix socket (#12613)
* | | | | | | | | | | | | | | | | | | | | | | | 29690198 media : add SVG logo [no ci] (#12616)
| | | | | | | | | | | | | | | | | | | | | | | | * efe02221 media : add SVG logo [no ci]
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 5dec47dc opencl: add multi and vision rope, `gelu_quick` and `im2col` (#12600)
* | | | | | | | | | | | | | | | | | | | | | | | f125b8dc llama : add PLM GGUF Conversion & Inference Support (#12457)
* | | | | | | | | | | | | | | | | | | | | | | | 953c2a62 model : restore support for T5Encoder (#12590)
* | | | | | | | | | | | | | | | | | | | | | | | d5c6309d convert : Support Qwen2_5_VLForConditionalGeneration (#12595)
* | | | | | | | | | | | | | | | | | | | | | | | 029c693f sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | 771d8437 scripts : update sync + fix cmake merge
* | | | | | | | | | | | | | | | | | | | | | | | df0665a4 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | 0306aad1 cmake : sync/merge PowerPC build commands (#0)
* | | | | | | | | | | | | | | | | | | | | | | | c7b43ab6 llamafile : ppc64le MMA implementation for Q4_0. (#12489)
* | | | | | | | | | | | | | | | | | | | | | | | 24feaec0 ggml : riscv: add 128-bit RVV support (#12530)
* | | | | | | | | | | | | | | | | | | | | | | | f28bc4c2 llama : make loras compatible with repacking (#12593)
* | | | | | | | | | | | | | | | | | | | | | | | f17a3bb4 SYCL: implement memset ggml backend buffer interface (#12580)
* | | | | | | | | | | | | | | | | | | | | | | | bd40678d HIP: Add support for RDNA4 targets (#12372)
* | | | | | | | | | | | | | | | | | | | | | | | b3298fa4 metal : refactor mat-vec code (#12569)
* | | | | | | | | | | | | | | | | | | | | | | | 2447ad8a upgrade to llguidance 0.7.10 (#12576)
* | | | | | | | | | | | | | | | | | | | | | | | 02082f15 clip: Fix llama-llava-clip-quantize-cli quantization error under CUDA backend (#12566)
* | | | | | | | | | | | | | | | | | | | | | | | df4d20cd convert : fix squeeze for ssm_conv tensors (#12573)
* | | | | | | | | | | | | | | | | | | | | | | | 5ed38b68 ggml : fix MUL_MAT_ID repack with Q8_K (#12544)
* | | | | | | | | | | | | | | | | | | | | | | | fd7855f8 doc: [MUSA] minor changes (#12583)
* | | | | | | | | | | | | | | | | | | | | | | | 53af4dba convert: fix Mistral3/Gemma3 model hparams init (#12571)
* | | | | | | | | | | | | | | | | | | | | | | | ef19c717 run: de-duplicate fmt and format functions and optimize (#11596)
| | | | | | | | | | | | | | | | | | | | | | | | * 70b063a5 metal : reduce register pressure
| | | | | | | | | | | | | | | | | | | | | | | | * c6a1be6c metal : fix typo [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | * e7d14ab2 metal : reduce register pressure
| | | | | | | | | | | | | | | | | | | | | | | | * fe12e20a metal : mv q6_K support nr0 > 1
| | | | | | | | | | | | | | | | | | | | | | | | * 51dea768 metal : fix nr constant [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | * 982c82f1 metal : fix comments [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | * 24a9ea8b metal : rename all_sum -> sum_all
| | | | | | | | | | | | | | | | | | | | | | | | * fcca45c0 metal : refactor mat-vec code
| | | | | | | | | | | | | | | | | | | | | | | | | * 20b256e0 convert : match ssm_conv tensors by type
| | | | | | | | | | | | | | | | | | | | | | | | | * 9c60fc4c convert : fix squeeze for ssm_conv tensors
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | 053b3f9a ggml-cpu : update KleidiAI to v1.5.0 (#12568)
* | | | | | | | | | | | | | | | | | | | | | | | | e2f56017 SYCL: disable Q4_0 reorder optimization (#12560)
* | | | | | | | | | | | | | | | | | | | | | | | | 36ee06dd docs : add build instructions for KleidiAI (#12563)
* | | | | | | | | | | | | | | | | | | | | | | | | 3cd3a395 ci: [MUSA] add CI and update doc (#12562)
* | | | | | | | | | | | | | | | | | | | | | | | | 2d77d88e context : fix worst-case reserve outputs (#12545)
| | | | | | | | | | | | | | | | | | | | | | | | | * e94c2bd3 ggml : improve repack templates
| | | | | | | | | | | | | | | | | | | | | | | | | * 87cd537a ggml : fix MUL_MAT_ID repack with Q8_K
| | | | | | | | | | | | | | | | | | | | | | | | | | * b8b78854 SYCL: disable Q4_0 reorder optimization
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | c95fa362 ci: [SYCL] ggml-ci Use main GPU and enable sysman (#12547)
* | | | | | | | | | | | | | | | | | | | | | | | | | 2b65ae30 opencl: simplify kernel embedding logic in cmakefile (#12503)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | 48d7021c CI: fix SYCL build (#12546)
* | | | | | | | | | | | | | | | | | | | | | | | | 3361e2de docs: update: improve the Fedoa CUDA guide (#12536)
* | | | | | | | | | | | | | | | | | | | | | | | | 00d53800 llama-vocab : add SuperBPE pre-tokenizer (#12532)
* | | | | | | | | | | | | | | | | | | | | | | | | 7ea75035 CUDA: Fix clang warnings (#12540)
* | | | | | | | | | | | | | | | | | | | | | | | | c54f6b79 mmap : skip resource limit checks on AIX (#12541)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 9b169a4d vulkan: fix mul_mat_vec failure in backend tests (#12529)
| | | | | | | | | | | | | | | | | | | | | | | | * a5b19439 ggml-quants : fix some edge cases in make_qkxh_nl_quants
| | | | | | | | | | | | | | | | | | | | | | | | * 8b8b88f3 ggml-quants : restore Q2_K use of make_qp_quants
| | | | | | | | | | | | | | | | | | | | | | | | *   a4113972 Merge branch 'master' into compilade/optimal-rounding
| | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | * | af23abd3 ggml-quants : remove slower qsort-based cumulative search
| | | | | | | | | | | | | | | | | | | | | | | | * | 3e4b675c ggml-quants : use a max-heap for TQ1_0 and TQ2_0 quantization
| | | | | | | | | | | | | | | | | | | | | | | | * | f86b8ff2 ggml-quants : use qkxh in more places
| | | | | | | | | | | | | | | | | | | | | | | | * | 3be11510 ggml-quants : use a max-heap for linear quants like Q3_K
| | | | | | | | | | | | | | | | | | | | | | | | * | 30ad9c28 ggml-quants : faster exhaustive IQ4_NL rounding with k_heap
| | | | | | | | | | | | | | | | | | | | | | | | * | 0c9e4424 ggml-quants : remove some commented code
| | | | | | | | | | | | | | | | | | | | | | | | * | f27c1afc ggml-quants : improve TQ2_0 imatrix
| | | | | | | | | | | | | | | | | | | | | | | | * | 6f7fe749 ggml-quants : improve imatrix behavior for TQ1_0, TQ2_0, Q4_0, Q5_0
| | | | | | | | | | | | | | | | | | | | | | | | * | d0060fc4 ggml-quants : better and faster make_qkxs_quants
| | | | | | | | | | | | | | | | | | | | | | | | * | dd6b8408 ggml-quants : improve IQ4_NL, IQ4_XS, and Q3_K
| | | | | | | | | | | | | | | | | | | | | | | | | | * 35c2f8b9 llama-vocab : add SuperBPE pre-tokenizer
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | 77f9c6bb server : Add verbose output to OAI compatible chat endpoint. (#12246)
* | | | | | | | | | | | | | | | | | | | | | | | | | 18b663d8 install : add macports (#12518)
* | | | | | | | | | | | | | | | | | | | | | | | | | fbdfefe7 llama : gemma3 : use output tensor if it exists in model weight (#12506)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | ba932dfb ggml : fix quantized cpy op (#12310)
* | | | | | | | | | | | | | | | | | | | | | | | | fac63a3d musa: refine compute capability (#12493)
* | | | | | | | | | | | | | | | | | | | | | | | | eddfb438 vulkan: Optimize mul_mat_vec p021 and nc shaders (#12505)
* | | | | | | | | | | | | | | | | | | | | | | | | 4375415b Vulkan: RTE rounding for cpy to quant (#12480)
* | | | | | | | | | | | | | | | | | | | | | | | | 30c42ef5 vulkan: workaround for AMD Windows driver 16 bit unpack8 bug (#12472)
* | | | | | | | | | | | | | | | | | | | | | | | | af04481e model : do not repack if a GPU device is present (#12498)
* | | | | | | | | | | | | | | | | | | | | | | | | 960e7260 chore : cleanup llama_model_loader::TENSOR_ usage (#12492)
* | | | | | | | | | | | | | | | | | | | | | | | | ea1518e8 llama-tts : avoid crashes related to bad model file paths (#12482)
* | | | | | | | | | | | | | | | | | | | | | | | | 1aa87ee5 [SYCL] Fix build on Windows when ccache enabled (#9954) (#9976)
* | | | | | | | | | | | | | | | | | | | | | | | | 9ffcc9e3 sycl: cleanup oneDNN related code (#12097)
* | | | | | | | | | | | | | | | | | | | | | | | | e0464306 webui : Prevent rerendering on textarea input (#12299)
* | | | | | | | | | | | | | | | | | | | | | | | | dbb3a473 llama : make Qwen2MoE QKV bias optional (#12477)
* | | | | | | | | | | | | | | | | | | | | | | | | 3d82dbcb ggml : block interleaving support for Q4_K quantization for x86 AVX2 architecture (#12332)
* | | | | | | | | | | | | | | | | | | | | | | | | 732b5fbf convert : avoid calls to tokenizer.added_tokens_decoder (#12473)
* | | | | | | | | | | | | | | | | | | | | | | | | 568013d0 context : clear sets containing encoder output sequence ids before storing new values (#12470)
* | | | | | | | | | | | | | | | | | | | | | | | | 517b5ddb CUDA: Improve flash decoding kernel GPU occupancy for BS=1 case (#12183)
* | | | | | | | | | | | | | | | | | | | | | | | | a9b59288 vulkan: optimize iq1 coopmat2 dequant functions (#12427)
* | | | | | | | | | | | | | | | | | | | | | | | | 0fd8487b Fix visionOS build and add CI (#12415)
* | | | | | | | | | | | | | | | | | | | | | | | | 108e53c2 llama : add support for GPT2, Bloom and CodeShell tied word embeddings (#12456)
* | | | | | | | | | | | | | | | | | | | | | | | | a686171e convert : Support chat_template.json (#12460)
* | | | | | | | | | | | | | | | | | | | | | | | | c446b2ed vulkan: Submit once enough matmul work has been recorded (#12406)
* | | | | | | | | | | | | | | | | | | | | | | | | d84635b1 opencl: improve profiling (#12442)
* | | | | | | | | | | | | | | | | | | | | | | | | 75422e8b graph : normalize Q, K, V shapes + sync cross attention (#12449)
* | | | | | | | | | | | | | | | | | | | | | | | | bb115d2b musa: override warp_size of musa device to 32 (#12445)
* | | | | | | | | | | | | | | | | | | | | | | | | 29fff308 llama : support converting Mistral Small text-only (#12450)
* | | | | | | | | | | | | | | | | | | | | | | | | c6af2161 speculative : fix seg fault in certain cases (#12454)
* | | | | | | | | | | | | | | | | | | | | | | | | 99aa304f llama : add support for EXAONE tied word embeddings (#12451)
| | | | | | | | | | | | | | | | | | | | | | | | | * b8b17327 server : remove old commented code [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | | * 8a23b4a5 server : avoid common_batch
| | | | | | | | | | | | | | | | | | | | | | | | | * 76fd7d6f perplexity : avoid common_batch
| | | | | | | | | | | | | | | | | | | | | | | | | * 8b80d683 embedding : avoid common_batch
| | | | | | | | | | | | | | | | | | | | | | | | | * 6f54ee66 retrieval : avoid common_batch
| | | | | | | | | | | | | | | | | | | | | | | | | * 32c2c41d android : fix permission
| | | | | | | | | | | | | | | | | | | | | | | | | * 96ca6e8d swift : adapt to new API
| | | | | | | | | | | | | | | | | | | | | | | | | * b0db7fc2 android : adapt to new API
| | | | | | | | | | | | | | | | | | | | | | | | | *   23d74073 Merge pull request #15 from ggml-org/xsn/private_batch_api
| | | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | | | * 7a3c178d speculative : adapt to new llama API
| | | | | | | | | | | | | | | | | | | | | | | | | |/  
| | | | | | | | | | | | | | | | | | | | | | | | | *   dc4bb642 Merge branch 'master' into xsn/private_batch_api
| | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | 8551c44d context : always use non-causal attention for encoder graphs (#12447)
* | | | | | | | | | | | | | | | | | | | | | | | | | 35cae5ba SYCL: using graphs is configurable by environment variable and compile option (#12371)
* | | | | | | | | | | | | | | | | | | | | | | | | | 810e0af3 server : fix warmup draft cache type (#12446)
* | | | | | | | | | | | | | | | | | | | | | | | | | eba92d64 cmake : fix PowerPC build (#12241)
| | | | | | | | | | | | | | | | | | | | | | | | | * eab5606d Apply suggestions from code review
| | | | | | | | | | | | | | | | | | | | | | | | | * de788e07 Update examples/tts/tts.cpp
| | | | | | | | | | | | | | | | | | | | | | | | | * 624a683c fix compile
| | | | | | | | | | | | | | | | | | | | | | | | | * 116b9a16 rename to init_from_text
| | | | | | | | | | | | | | | | | | | | | | | | | * eaffba0f llama_batch_ext_ptr::from_text/embd
| | | | | | | | | | | | | | | | | | | | | | | | | * 8e7714fa fix compile
| | | | | | | | | | | | | | | | | | | | | | | | | * a363251f qwen2vl: use llama_batch_ext_set_pos
| | | | | | | | | | | | | | | | | | | | | | | | | * ba793696 fix llama_batch_ext_init_from_embd
| | | | | | | | | | | | | | | | | | | | | | | | | * 07d84fa3 fix missing n_past in various places
| | | | | | | | | | | | | | | | | | | | | | | | | * 32940369 fix gemma3-cli
| | | | | | | | | | | | | | | | | | | | | | | | | * 5e6a6d4e fix llama-run n_past
| | | | | | | | | | | | | | | | | | | | | | | | | * bfdddbc1 bring back mistakenly deleted llama_batch_init/free
| | | | | | | | | | | | | | | | | | | | | | | | | * 54566ad9 correct comment
| | | | | | | | | | | | | | | | | | | | | | | | | * 04f86418 rm redundant llama_batch_ext_set_output_last
| | | | | | | | | | | | | | | | | | | | | | | | | * c3dd7900 fix llama_batch_ext_init_from_text
| | | | | | | | | | | | | | | | | | | | | | | | | * 65f01845 compile ok
| | | | | | | | | | | | | | | | | | | | | | | | | * 9fb2d81e fix common_batch missing seq_id
| | | | | | | | | | | | | | | | | | | | | | | | | * 47086fa8 apply to the rest
| | | | | | | | | | | | | | | | | | | | | | | | | * 4aabf4e8 return output ID from llama_batch_ext_add/set
| | | | | | | | | | | | | | | | | | | | | | | | | * 86973cb1 fix merge errors
| | | | | | | | | | | | | | | | | | | | | | | | | *   17f954c8 Merge branch 'master' into xsn/private_batch_api
| | | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | | * | 46596caf apply various in places
| | | | | | | | | | | | | | | | | | | | | | | | | * | 1d6ba977 remove token_info API
| | | | | | | | | | | | | | | | | | | | | | | | | * | 1170135d llama_batch_ext_add_text
| | | | | | | | | | | | | | | | | | | | | | | | | * | 40989f41 correct llama_decode_ext
| | | | | | | | | | | | | | | | | | | | | | | | | * |   9e75c49d Merge branch 'master' into xsn/private_batch_api
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | f0ffd811 adapt common
| | | | | | | | | | | | | | | | | | | | | | | | | * | |   a1b1dea3 Merge branch 'master' into xsn/private_batch_api
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | 4bf7ca39 llama_decode_ext
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | aed4a8e9 fix server
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | 85ef80cb server : use llama_batch_ext
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | 17d3658b move to llama_batch_ext
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | f2e59a8e rework, targeting llama-server
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | 4ed4fe75 first proposal for private llama_batch
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 29acf2cf context : move the change to llama_context::encode()
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * a0554c3c context : always use non-causal attention for encoder graphs
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d9a14523 ggml : add SVE support for q6_K_q8_K (#12361)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | fd123cfe Vulkan: Default to 1GB allocations instead of 4GB to avoid fragmentation and driver issues (#12434)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | a53f7f7b fixed compilation warnings in ggml-sycl (#12424)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7dfad387 llama: Add support for RWKV v7 architecture (#12412)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 60c90292 docs : bring llama-cli conversation/template docs up-to-date (#12426)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 90f17bba Vulkan: Default to 1GB allocations instead of 4GB to avoid fragmentation and driver issues
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b1b132ef cuda : enable CUDA Graph on CUDA Toolkit < 12.x (#12394)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 01e8f213 ggml-vulkan: remove unused find_program(glslc) (#12416)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 484a8ab5 vulkan: Add N/2 and N/4 optimized paths in coopmat2 shader (#12312)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | cf2270e4 vulkan: subgroup size tuning (#12087)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | f07690c9 vulkan: use fp32 in coopmat2 q4_k dequant function (#12309)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 891c6395 vulkan: Pad N dimension of B matrix for coopmat2 perf, to avoid bounds checking (#12273)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2f21123c vulkan: Adjust coopmat2 tile sizes and selection heuristic (#12258)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 374101fd cmake : enable building llama.cpp using system libggml (#12321)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b3c9a656 SYCL: set extras only on GGML_TYPE_Q4_0 (#12366)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8ba95dca llama : fix OLMo-2-0325-32B-Instruct K-norm size (#12400)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | dc079cfd context : fix init of n_outputs (#12397)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7b61bcc8 ci : add --symlinks to xcframework zip command (#12409)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | f4c3dd5d llama-tts : add '-o' option (#12398)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3d35d87b SYCL: Delete redundant plus sign and space (#12391)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b19bd064 SYCL : support non-contiguous tensors in binary ops (add, sub, etc) (#12399)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 92a39132 [CANN]MUL_MAT optimization (#12382)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9f2250ba Add CLI arg to llama-run to adjust the number of threads used (#12370)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 774973b8 main : add -sysf / --system-prompt-file (#12249) (#12250)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8fcb5636 Load all MoE experts during warmup (#11571)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | add2a3aa server: fix "--grammar-file" parameter (#12285)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c522ce41 graph : simplify attn input build for unified KV cache (#12381)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 081bee8c hparams : add SWA rope parameters (#12374)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f6711cef CUDA: determine FA parallel blocks at runtime
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c4aca655 hparams : add SWA rope parameters
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 84d54755 llama : fix Gemma3 SWA KV cache shift (#12373)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 21fe0ce4 hparams : add comment [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * de9d18fa llama : fix Gemma3 SWA KV cache shift
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | be7c3034 arg : no n_predict = -2 for examples except for main and infill (#12364)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e0dbec0b llama : refactor llama_context, llama_kv_cache, llm_build_context (#12181)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2048b591 server : fix crash when using verbose output with input tokens that are not in printable range (#12178) (#12338)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | f08f4b31 Update build.yml for Windows Vulkan builder to use Vulkan 1.4.304 SDK for VK_NV_cooperative_matrix2 support (#12301)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 80a02aa8 llama.swiftui : fix xcframework dir in README [no ci] (#12353)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 363f8c5d sycl : variable sg_size support for mmvq kernels (#12336)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 34c961b1 CUDA/HIP: Fix fattn-vec-* when device warp size is not 32 (#12315)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7841fc72 llama : Add Gemma 3 support (+ experimental vision capability) (#12343)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | bf69cfe6 vulkan: fix bug in coopmat1 mul_mat_id (#12316)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 10f2e818 CUDA/HIP: refractor mmqv to unify the calculation of nwarps and rows per block between host and device code. (#12177)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ba765438 ggml-backend : fix backend search path (#12330)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * ed58975f server : improve infill stop criteria
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6ab2e476 metal : Cache the Metal library at the device context level (#12265)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 96e12808 clip : bring back GPU support (#12322)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2c9f833d mat vec double buffer (#12188)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 25136454 musa: support new arch mp_31 and update doc (#12296)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8acdacb3 opencl: use OpenCL C standard supported by the device (#12221)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 89b2b56e readme: added Sidekick to available UIs (#12311)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e128a1bf tests : fix test-quantize-fns to init the CPU backend (#12306)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6ef79a67 common : refactor '-o' option (#12278)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4e39a3c3 `server`: extract <think> tags from qwq outputs (#12297)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | be421fc4 `tool-call`: ensure there's always a non-empty tool call id (#12292)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 87c26305 allow missing content in message if tool_calls provided (#12293)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2b3a25c2 `sampler`: fixes trigger tokens + lazy grammars (fix typo cast from token to string) (#12291)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8352cdc8 llava : fix bug in minicpm-v code (#11513)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1e2f78a0 server : add speculative decoding presets for FIM (#12287)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 87dae2fd Vulkan: Print coopmat shapes, then exit
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0fd7ca7a authors : update (#12271)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6fefc05a ggml-backend : make path_str compatible with C++20 (#12269)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7ab36439 server : infill gen ends on new line (#12254)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 25840747 Vulkan: Add device architecture enum and logic to recognize AMD generations
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 7037e948 vulkan: subgroup size test
| | | | | |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
| | | | |/| | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * c75753a0 server : infill gen ends on new line
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7c7f3b7f ggml : skip intermediate .air file when compiling .metallib (#12247)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 102ac189 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d6ae2fa0 ggml : ggml_compute_forward_concat() for arbitrary tensor type (ggml/1118)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 68d0027f ggml-cpu: faster AVX2 variant for IQ1_M (#12216)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ea002810 ci : fix save-load test invocations (#12245)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * aefa65e4 ci : fix save-load test invokations
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8fad3c7a server : Log original chat template parsing error (#12233)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * aae2903e clang-tidy : disable bugprone-branch-clone
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7cf64f6b sync: minja - support QwQ-32B (#12235)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5e2d57b2 metal : simplify kernel arguments using a struct (#3229) (#12194)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | f1648e91 HIP: fix rocWMMA build flags under Windows (#12230)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d6c95b07 metal : fix default.metallib build (#12224)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d76a86d9 opencl: Noncontiguous `norm`, `rms_norm`, disable `fp16` for some ops (#12217)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 776f9e59 cmake : fix undefined reference errors for std::filesystem in ggml (#12092) (#12094)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3d652bfd readme : update bindings (#12229)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 5220a16d CUDA: fix FA logic for PTX 7.0 and CC >= 7.5 (#12222)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3ffbbd5c HIP: rocWMMA documentation and enabling in workflow builds (#12179)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 42994048 update function-calling.md w/ template override for functionary-small-v3.2 (#12214)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | e9b2f84f llava: add big-endian conversion for image encoder (#12218)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | e721c05c HIP/CUDA: set the paramerter value in maintain_cuda_graph instead of replaceing it. (#12209)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 57b6abf8 android : fix KV cache log message condition (#12212)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 94bb63e4 opencl : fix buffer alignment (#12197)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f7924399 opencl : fix `ulong` kernel args were set from `int` variables (#12174)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | ed4ce0dd opencl : fix profile-related errors (#12095)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 07d15723 ggml-cpu: Faster IQ1 mul_mat_vec on AVX2 using BMI2 instructions (#12154)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 5e43f104 SYCL: Disable f16 Unary OPs as not supported by the kernels (#12201)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 16e4b22c ggml : fix GGMLMetalClass ODR (#12200)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 074c4fd3 ci : add fetch-depth to xcframework upload (#12195)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 669912d9 `tool-call`: fix Qwen 2.5 Coder support, add micro benchmarks, support trigger patterns for lazy grammars (#12034)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | fa31c438 ci : fix xcframework artifact tag (#12191)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3ccbfe5a ci : remove xframework upload (#12190)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 06a92a19 server : fix cache reuse logic (#12161)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | a057897a llama : add xcframework build script (#11996)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 5bbe6a9f ggml : portability fixes for VS 2017 (#12150)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 20a9b8f5 readme : fix roadmap link (#12185)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 56d7a9f8 main: allow preloading conversation with -p and add -st / --single-turn (#12145)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1a24c462 `server`: fix deadly typo in response_format.json_schema.schema handling (#12168)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | becade5d HIP: implement FlashAttention via rocWMMA for CDNA and RDNA3+ (#12032)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | dfd6b2c0 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b64d7cc2 cuda: unary ops as float + de-duplicate (ggml/1130)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3d1cf3cf sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 0cbee131 cuda/vulkan: specify fp32-only support for some operations in supports_op (ggml/1129)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 8371d445 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 87abb7e9 cuda/cpu: Increase support for fp16 unary operations (ggml/1125)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6d4c23b8 whisper : support GGML_BACKEND_DL (whisper/2843)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6512a900 cmake : fix compile assumptions for power9/etc (whisper/2777)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 45120557 Told cmake to install ggml-cpp.h as a public header file. (ggml/1126)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f54a4ba1 Support pure float16 add/sub/mul/div operations in the CUDA (and CPU) backend (ggml/1121)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | aede2074 scripts : sync-ggml-am.sh fix
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 2679c3b5 ci : set GITHUB_ACTION env var for server tests (#12162)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c43af927 tts: add speaker file support (#12048)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d5c63cd7 test-backend-ops : add option -p to filter by op params (#12155)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 9660ffef ggml : fix kleidiai build (#12159)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | c950a1f6 Adding UTF-8 support to llama.cpp (#12111)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 7b69003a webui : add ?m=... and ?q=... params (#12148)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | ece9745b SYCL: Move CPY kernels to a separate file and add few missing kernels (#12133)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | cc473cac ggml-backend : keep paths in native string type when possible (#12144)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 14dec0c2 main: use jinja chat template system prompt by default (#12118)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1782cdfe main: update outdated system prompt message (followup to #12131) (#12132)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 45a8e767 common : add --system-prompt parameter, replace behavior of -p in conversation mode (#12131)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 80c41ddd CUDA: compress mode option and default to size (#12029)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | 2cc4a5e4 webui : minor typo fixes (#12116)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 06c2b156 convert : fix Norway problem when parsing YAML (#12114)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 70680c48 ggml : upgrade init_tensor API to return a ggml_status (#11854)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c43a3e79 llama : add Phi-4-mini support (supersede #12099) (#12108)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 84d5f4bc Update granite vision docs for 3.2 model (#12105)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 438a8392 vulkan: add specific MMV kernels for IQ2 and IQ3 quants + optimizations (#11595)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 9c42b171 CUDA: fix logic for V100 + GGML_CUDA_FORCE_MMQ (#12098)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 05e6f5aa ggml: aarch64: implement SVE kernels for q2_k_q8_k vector dot (#12064)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 673cfef9 CANN: Fix build error with GCC 13 (#11990)
* | | | | | | | | | | | | | | | | | | | | | | | | | | fbeda900 vulkan: matmul dequantization improvements (#12015)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 581650b7 vulkan: improve im2col (#11826)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 624f7bd0 graph : add comments
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 0f7daa9d graph : move non-context related logic to llm_build_context
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 9cab53c7 cont : migrate the rest of the inputs out of llama_context
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 7f02ee56 context : decouple inputs, llama_graph_i become const (WIP)
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 38db8a58 llama : introduce concept of llama_memory
| | | | | | | | | | | | | | | | | | | | | | | | | | | * 828effd9 kv-cache : basic abstraction
| | | | | | | | | | | | | | | | | | | | | | | | | | | *   82675a01 Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b95c8af3 cmake: Fix ggml backend dependencies and installation (#11818)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | a800ae46 llava : add struct for FFI bindgen (#12079)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 69050a11 Refactor gguf scripts to improve metadata handling (#11909)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3567ee3a gguf-py: enable reading non-native endian files (#12081)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 53e4db10 readme : update infra list (#9096)
* | | | | | | | | | | | | | | | | | | | | | | | | | | d7cfe1ff docs: add docs/function-calling.md to lighten server/README.md's plight (#12069)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a82c9e7c vulkan: fix assertion when qy_needs_dequant (#12068)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 401af80b server: handle echo=false on /v1/completions (#12060)
* | | | | | | | | | | | | | | | | | | | | | | | | | | c132239b add OP sigmoid (#12056)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 393fca62 ggml-cpu: Fix build with sve (#12059)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 61d4f39d vulkan: implement more backpropagation operators (#11914)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0b527456 server: support add_generation_prompt query param (#12062)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 4d1051a4 Add Doc for Converting Granite Vision -> GGUF (#12006)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 3e9a2860 llama : expose llama_model_n_head_kv in the API (#11997)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 58d07a80 metal : copy kernels for quant to F32/F16 conversions (#12017)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 34a846b5 opencl: fix for small models (#11950)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 7a2c913e llava : Add Granite Vision Support (#11794)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | 08d59862 [SYCL] Optimize mul_mat for Q4_0 on Intel GPU (#12035)
* | | | | | | | | | | | | | | | | | | | | | | | | | 651adf4b gguf_convert_endian.py: implement byteswapping for q4_k and q6_k (#11349)
* | | | | | | | | | | | | | | | | | | | | | | | | | 8303e8b0 SYCL: Fix GGML_SYCL_DEBUG macro (#11995)
* | | | | | | | | | | | | | | | | | | | | | | | | | 7ad0779f run: allow to customize prompt by env var LLAMA_PROMPT_PREFIX (#12041)
* | | | | | | | | | | | | | | | | | | | | | | | | | f777a73e Some llama-run cleanups (#11973)
* | | | | | | | | | | | | | | | | | | | | | | | | | af7747c9 ggml-cpu: Support s390x SIMD Instruction Set (#12019)
* | | | | | | | | | | | | | | | | | | | | | | | | | a28e0d5e CUDA: app option to compile without FlashAttention (#12025)
* | | | | | | | | | | | | | | | | | | | | | | | | | 36c258ee llava: build clip image from pixels (#11999)
* | | | | | | | | | | | | | | | | | | | | | | | | | f3e64859 ci : fix arm upload artifacts (#12024)
| | | | | | | | | | | | | | | | | | | | | | | | | * 952feedf context : disable encoder embd tensor for now
| | | | | | | | | | | | | | | | | | | | | | | | | * 4efe9898 context : pass embeddings tensor from encoder to decoder
| | | | | | | | | | | | | | | | | | | | | | | | | * e2b3294f context : fix enc-dec state save/load
| | | | | | | | | | | | | | | | | | | | | | | | | * e5bc5f8e context : enc-dec is now working
| | | | | | | | | | | | | | | | | | | | | | | | | * be58e300 enc-dec : compose wip
| | | | | | | | | | | | | | | | | | | | | | | | | * 9cd78f11 context : explicit llama_context_i abstract interface
| | | | | | | | | | | | | | | | | | | | | | | | | * 4a1054b5 context : reuse built_attn_mha
| | | | | | | | | | | | | | | | | | | | | | | | | * a5a85a3b context : fix recurrent reserve
| | | | | | | | | | | | | | | | | | | | | | | | | * 0699a44c context : remove redundant virtual, protected -> private
| | | | | | | | | | | | | | | | | | | | | | | | | * 6378112c graph : remove the build_kv_... API from llama_graph_i
| | | | | | | | | | | | | | | | | | | | | | | | | * 372fa3a8 cont : enc should work now, next is dec
| | | | | | | | | | | | | | | | | | | | | | | | | * f5e80208 wip enc-dec
| | | | | | | | | | | | | | | | | | | | | | | | | *   c4c0a4d1 Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | |_|/  
| | | | | | | | | | | | | | | | | | | | | | | |/| |   
| | | | | | | | | | | | | | | | | | | | | | | | | * 3753b30d context : fix n_outputs init
| | | | | | | | | | | | | | | | | | | | | | | | | * f588a70d context : wrap input tensors in struct
| | | | | | | | | | | | | | | | | | | | | | | | | * ebf1bdf9 context : add logs
| | | | | | | | | | | | | | | | | | | | | | | | | * 548c230d graph : remove worst_case from the API
| | | | | | | | | | | | | | | | | | | | | | | | | * 2645a7d9 context : add save/load for recurrent context
| | | | | | | | | | | | | | | | | | | | | | | | | * 08011c2c context : add llama_kv_cache_recurrent prototype
| | | | | | | | | | | | | | | | | | | | | | | | | * ad870c49 context : fix causal input for cache-less case
| | | | | | | | | | | | | | | | | | | | | | | | | * b1554be1 context : add cache-less llama_context
| | | | | | | | | | | | | | | | | | | | | | | | | *   072280ea Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | | | | | * | f95b04a2 model : fix order kvq -> qkv
| | | | | | | | | | | | | | | | | | | | | | | | | * | 2eacb4c1 graph : simplify attention api
| | | | | | | | | | | | | | | | | | | | | | | | | * | e17e4b72 context : add llama_context_recurrent
| | | | | | | | | | | | | | | | | | | | | | | | | * | 5f11a550 kv-cache : remove llama_kv_cache_i
| | | | | | | | | | | | | | | | | | | | | | | | | * | f5cedbca kv-cache : prepare for abstraction
| | | | | | | | | | | | | | | | | | | | | | | | | * | 2bffc2d5 model : pass llama_graph_i as ptr
| | | | | | | | | | | | | | | | | | | | | | | | | * | 9e50456e context : minor simplify
| | | | | | | | | | | | | | | | | | | | | | | | | * | befe14f0 llama : reorder encode/decode in sources
| | | | | | | | | | | | | | | | | | | | | | | | | * | bc6f187e cont : use returend tensors from the graph build
| | | | | | | | | | | | | | | | | | | | | | | | | * | 172f6169 cont : return important tensors
| | | | | | | | | | | | | | | | | | | | | | | | | * | c2359031 graph : add llama_graph_result
| | | | | | | | | | | | | | | | | | | | | | | | | * |   f0d3ff23 Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 1d801d27 graph : update attn/kv_self names
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 82806456 context : move common inputs to base class
| | | | | | | | | | | | | | | | | | | | | | | | | * | | d5e8e1a2 context : remove batch_manager
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 131743ff context : abstract constructor and init
| | | | | | | | | | | | | | | | | | | | | | | | | * | | ed3cb55a context : abstract input
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 107d1e2c context : move output functionality to base class
| | | | | | | | | | | | | | | | | | | | | | | | | * | | e08f38df context : minor cleanup
| | | | | | | | | | | | | | | | | | | | | | | | | * | | f7c7757b context : abstract state read/write
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 3a504d9a llama : introduce llama_io interfaces
| | | | | | | | | | | | | | | | | | | | | | | | | * | | fbe6a072 context : rename to llama_context_kv_self
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 6ee86e5e graph : restore ubatch in build_cb
| | | | | | | | | | | | | | | | | | | | | | | | | * | | f63aeecc llama : models now build their graphs using llama_graph_i
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 0ab50f1b context : prepare llama_model graph build
| | | | | | | | | | | | | | | | | | | | | | | | | * | | e633dc17 context : introduce llama_graph_i
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 5eae8e51 context : move build_rope_factors to base class
| | | | | | | | | | | | | | | | | | | | | | | | | * | | d146a14f context : minor naming fix
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 8da7f612 context : improve llama_context encapsulation
| | | | | | | | | | | | | | | | | | | | | | | | | * | | b52b79b0 context : move encode/decode to llama-context.cpp
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 02ef4be9 context : initial abstraction
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 2cd8a903 context : make output functions members
| | | | | | | | | | | | | | | | | | | | | | | | | * | | d1d8d530 bman : remove ubatch member
| | | | | | | | | | | | | | | | | | | | | | | | | * | | ef358ee7 context : add decode/encode
| | | | | | | | | | | | | | | | | | | | | | | | | * | | 879ba827 server : increase context size for the tests
| | | | | | | | | | | | | | | | | | | | | | | | | * | | f9971ef2 llama : dedup reserve code
| | | | | | | | | | | | | | | | | | | | | | | | | * | |   972f91c7 Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | b15fede7 kv-cache : fix defrag condition
| | | | | | | | | | | | | | | | | | | | | | | | | * | | |   0f1c1cab Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | e0d913fc llama : clear whitespaces
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | 1eca8916 llama : fix rwkv inference (#11618)
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | |   74b08072 Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | 3e23be79 context : store graph build function callback
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | |   5d3491e7 Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * \ \ \ \ \ \   a40ba49f Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * \ \ \ \ \ \ \   c30e34cd Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | 91888569 llama : resolve rwkv conflict
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | |   e665b57f Merge branch 'master' into gg/llama-kv-cache
| | | | | | | | | | | | | | | | | | | | | | | | | |\ \ \ \ \ \ \ \ \  
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | a0c500b4 context : prepare for abstraction
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 99422dfa context : introduce llama_batch_manager
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | cb8f2095 wip
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 133ad6a7 context : initial need_reserve logic
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | c75ba685 context : move adapter code in the implementation [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | f0713498 context : add get_ctx_padding()
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | b4ec1d44 cont : move kv_self update to llama_context
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | f2524c0e llama : remove references to llama_kv_cache (wip)
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | ae274f97 llama : fix names [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | a19f671f context : minor
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 17b363af llama : update llama_kv_self API
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | fd05ab87 kv_cache : move state read/write to llama_kv_cache
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 4cd1b6fa context : prepare kv_cache_read/write to be moved to kv_cache
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 73a14ecc kv_cache : minor
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | fef90cb3 kv_cache : fix
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | 4d7bd03e kv_cache : functions -> members
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | e4550fba llama : cont
| | | | | | | | | | | | | | | | | | | | | | | | | * | | | | | | | | | f78b396e llama : add struct llama_kv_cache (wip) [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f343850b cont : fix archive name to use matrix
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 3f683b40 ci : fix arm upload artifacts
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5fa07c2f CUDA: optimize FA for GQA + large batches (#12014)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 335eb04a ci : Build on Github-hosted arm64 runners (#12009)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | cf756d6e server : disable Nagle's algorithm (#12020)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d7090842 cuda: Add Q5_1, Q5_0, Q4_1 and Q4_0 to F32 conversion support. (#12000)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | de8b5a36 llama.swiftui : add "Done" dismiss button to help view (#11998)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 51f311e0 llama : skip loading unused tensors (#12004)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 586d5fe6 doc: update contributing guidelines [no ci] (#11969)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ecc8e3ae CUDA: correct the lowest Maxwell supported by CUDA 12 (#11984)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0b3863ff MUSA: support ARM64 and enable dp4a .etc (#11843)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | ee02ad02 clip : fix visual encoders with no CLS (#11982)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c392e509 server (webui): Fix Premature Submission During IME Conversion (#11971)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c5d91a74 ggml-cpu: Add CPU backend support for KleidiAI library (#11390)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4806498b ggml: aarch64: implement SVE kernels for q3_K_q8_K vector dot (#11917)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0d559580 run : add --chat-template-file (#11961)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d04e7163 doc: add links to ggml examples [no ci] (#11958)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d07c6213 common : add llama.vim preset for Qwen2.5 Coder (#11945)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | abd4d0bc speculative : update default params (#11954)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 0f2bf555 speculative : do not discard the last drafted token
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 965ad1c0 speculative : update default params
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9626d935 llama : fix indentation in llama-grammar [no ci] (#11943)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b58934c1 server : (webui) Enable communication with parent html (if webui is in iframe) (#11940)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 63e489c0 tool-call: refactor common chat / tool-call api (+ tests / fixes) (#11900)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 63ac1285 server : add TEI API format for /rerank endpoint (#11942)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5137da7b scripts: corrected encoding when getting chat template (#11866) (#11907)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 09aaf4f1 docs : Fix duplicated file extension in test command (#11935)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 73e2ed3c CUDA: use async data loading for FlashAttention (#11894)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | f7b1116a update release requirements (#11897)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c4d29baf server : fix divide-by-zero in metrics reporting (#11915)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2eea03d8 vulkan: implement several ops relevant for ggml_opt (#11769)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0f2bbe65 server : bump httplib to 0.19.0 (#11908)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fe163d5b common : Fix a typo in help (#11899)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 818a340e ci : fix (again) arm64 build fails (#11895)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bf42a23d vulkan: support multi/vision rope, and noncontiguous rope (#11902)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c2ea16f2 metal : fix the crash caused by the lack of residency set support on Intel Macs. (#11904)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 6dde1782 scripts: fix compare-llama-bench commit hash logic (#11891)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fc10c38d examples: fix typo in imatrix/README.md (#11884)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 22885105 metal : optimize dequant q6_K kernel (#11892)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c2cd24fb readme : add notice about new package registry (#11890)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 68ff663a repo : update links to new url (#11886)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | f3552296 server: fix type promotion typo causing crashes w/ --jinja w/o tools  (#11880)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fc1b0d09 vulkan: initial support for IQ1_S and IQ1_M quantizations (#11528)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 89daa256 llguidance build fixes for Windows (#11664)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 300907b2 opencl: Fix rope and softmax (#11833)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 94b87f87 cuda : add ampere to the list of default architectures (#11870)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | dbc2ec59 docker : drop to CUDA 12.4 (#11869)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3d68f034 llama : add completion for --chat-template-file (#11860)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 38e32eb6 ggml: optimize some vec dot functions for LoongArch ASX (#11842)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | a4f011e8 vulkan: linux builds + small subgroup size fixes (#11767)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | a7b8ce22 llama-bench : fix unexpected global variable initialize sequence issue (#11832)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 04045bb8 readme : minor
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8a8c4ceb llamafile: use member variable instead of constant for iq4nlt (#11780)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c1f958c0 server : (docs) Update wrong tool calling example (#11809)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c48f630d llama : add --completion-bash option (#11846)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bd6e55bf musa: bump MUSA SDK version to rc3.1.1  (#11822)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c7f460ab `server`: fix tool-call of DeepSeek R1 Qwen, return reasoning_content (Command 7RB & DeepSeek R1) unless `--reasoning-format none` (#11607)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 27e8a233 sampling: add Top-nσ sampler (#11223)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e4376270 llama.cpp: fix warning message (#11839)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3e693197 llama : update llama_decode_internal ref [no ci] (#11840)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | a394039d ggml-cpu : add chunking support to mul_mat_id (#11666)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | be3bbd62 ggml : x2 speed for WASM by optimizing SIMD (#11453)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 86548050 docker : publish to both ggerganov and ggml-org
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 31afcbee server : (webui) Give copy button back to all message bubbles (#11814)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 5c4284d5 HIP: Remove GCN from list of devices that avoid MMQ (#11831)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | bfd11a23 Fix: Compile failure due to Microsoft STL breaking change (#11836)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0fb77f82 sync : ggml
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | * f30aca84 Revert "HIP: Switch to std::vector in rocblas version check (#11820)"
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | e598697d HIP: Switch to std::vector in rocblas version check (#11820)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | fef0cbea cleanup: fix compile warnings associated with gnu_printf (#11811)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 748ee9fe ggml : fix multi-threaded clamp_f32 (#11824)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 198b1ec6 ggml-cpu: Fix duplicate MATMUL_INT8 (#11817)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c3d6af7c CUDA: fix CUDART_VERSION checks (#11821)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 369be559 llama : fix typo in llama-grammar.h [no ci] (#11816)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4078c77f docs: add OpenCL (#11697)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 90e4dba4 Fix #11802: Compile bug - RegQueryValueExA changed to RegQueryValueEx (#11803)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | a18f481f server : use common_token_to_piece instead of common_detokenize (#11740)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b9ab0a4d CUDA: use arch list for compatibility check (#11775)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7b891bdc fix: typos in documentation files (#11791)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 81732619 docs: utilize the forward slash (/) as the path separator for Unix-like systems (#11770)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 507f9174 server : (webui) introduce conversation branching + idb storage (#11792)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 19b392d5 llama-mmap: fix missing include (#11796)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0893e011 server : correct signal handler (#11795)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | d7b31a9d sync: minja (https://github.com/google/minja/commit/a72057e5190de2c612d4598bb10b4bfd0f53011f) (#11774)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9ac3457b Update README.md [no ci] (#11781)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | c2a67efe vulkan: Make Vulkan optional at runtime (#11493). (#11494)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | | b044a0fe vulkan: add environment variable GGML_VK_PREFER_HOST_MEMORY to avoid VRAM allocation (#11592)
| |_|_|/ / / / / / / / / / / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 19d3c829 There's a better way of clearing lines (#11756)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 98f6b0fd vulkan: account for lookup tables when checking shared memory size (#11502)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 55ac8c77 server : (webui) revamp Settings dialog, add Pyodide interpreter (#11759)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | e6e65831 server : (webui) increase edit textarea size (#11763)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | aaa55053 server : minor log updates (#11760)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | bdcf8b6a cont : fix mmap flag print (#11699)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 4d3465c5 ggml: Fix data race in ggml threadpool (#11736)
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * d86e2310 server : minor log updates
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d80be897 CUDA: fix min. version for movmatrix (#11751)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 3ab410f5 readme : update front-end framework (#11753)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 0cf86716 server : (webui) fix numeric settings being saved as string (#11739)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | d2fe216f Make logging more verbose (#11714)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ed926d88 llama : fix defrag logic (#11707)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2d219b38 vocab : ignore invalid UTF-8 input in the BPE tokenizer (#11729)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 333820d7 llama : fix progress dots (#11730)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c026ba3c vulkan: print shared memory size (#11719)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 7ee953a6 llama : add llama_sampler_init for safe usage of llama_sampler_free (#11727)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | ec3bc827 SYCL: remove XMX info from print devices (#11712)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | b7552cfc common : add default embeddings presets (#11677)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 225bbbfa ggml : optimize and build warning fix for LoongArch (#11709)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 855cd073 llama : fix old glm4 models (#11670)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8a59053f sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1d20e53c rpc: fix known RCE in rpc-server (ggml/1103)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2fb3c32a server : (webui) migrate project to ReactJS with typescript (#11688)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9ab42dc7 docs: update fedora cuda guide for 12.8 release (#11393)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 194b2e69 SYCL: Adjust support condition for norm operators (#11674)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 9dd7a039 llama : add log about loading model tensors (#11699)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c0d48432 build : fix llama.pc (#11658)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8d4d2be1 ggml : fix LoongArch compile error with 128-bit SIMD (#11701)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * e00c9d1c Update examples/server/server.cpp
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * a4e9e4d4 Update examples/server/server.cpp
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * d23abdc3 When llama_chat_apply_template doesn't work
| | | | | | | | | | | | | | | | | | | | | | | | | | | | | * 3b6a0a81 llama : add log about loading model tensors
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 2c6c8df5 vulkan: optimize coopmat2 iq2/iq3 callbacks (#11521)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 8a7e3bf1 vulkan: initial support for IQ4_XS quantization (#11501)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 1b598b30 vulkan: use smaller combined allocations to avoid fragmentation (#11551)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | 902368a0 metal : avoid breaking build when metal API predates TARGET_OS_VISION (#11690)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | | c3db0480 readme : add link to Autopen under UIs (#11684)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | | | | * 947158ee Specify podman works in Container documentation
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d774ab3a metal : adjust support conditions for norm operators (#11671)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | fa62da9b CUDA: support for mat. mul. with ne03 != ne13 (#11656)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1ec20808 llava: add quantization for the visual projector LLAVA, Qwen2VL (#11644)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 9f4cc8f8 `sync`: minja (#11641)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | fd08255d CUDA: non-contiguous (RMS) norm support (#11659)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3ec9fd4b HIP: force max threads per block to be 1024 (#11621)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 3962fc1a server : add try..catch to places not covered by set_exception_handler (#11620)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1bef571f arg : list RPC devices first when using --list-devices (#11655)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | db288b60 `tool-call`: command r7b fix for normal responses (#11608)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 106045e7 readme : add llm_client Rust crate to readme bindings (#11628)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | f117d84b swift : fix llama-vocab api usage (#11645)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 534c46b5 metal : use residency set for other platforms (#11648)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 387a1598 authors : update
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 7c9e0ca5 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 8f8290ad cmake: Add ability to pass in GGML_BUILD_NUMBER (ggml/1096)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b34aedd5 ci : do not stale-close roadmap issues
* | | | | | | | | | | | | | | | | | | | | | | | | | | | cde38332 `tool-call`: allow `--chat-template chatml` w/ `--jinja`, default to chatml upon parsing issue, avoid double bos (#11616)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | b3451785 server : (webui) revert hacky solution from #11626 (#11634)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 1d1e6a90 server : (webui) allow typing and submitting during llm response (#11626)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 5598f475 server : remove CPPHTTPLIB_NO_EXCEPTIONS define (#11622)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 8ec05832 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 21c84b5d CUDA: fix Volta FlashAttention logic (#11615)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | d92cb67e server : (webui) Fix Shift+Enter handling (#11609)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 6eecde3c HIP: fix flash_attn_stream_k_fixup warning (#11604)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 396856b4 CUDA/HIP: add support for selectable warp size to mmv (#11519)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 4d0598e1 HIP: add GGML_CUDA_CC_IS_* for amd familys as increasing cc archtectures for amd gpus are not supersets of eatch other (#11601)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 90f9b88a nit: more informative crash when grammar sampler fails (#11593)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 864a0b67 CUDA: use mma PTX instructions for FlashAttention (#11583)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 84ec8a58 Name colors (#11573)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | bfcce4d6 `tool-call`: support Command R7B (+ return tool_plan "thoughts" in API) (#11585)
* | | | | | | | | | | | | | | | | | | | | | | | | | | | 69804487 Fix exotic ci env that lacks ostringstream::str (#11581)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | | ff227703 sampling : support for llguidance grammars (#10224)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 0cec062a llama : add support for GLM-Edge and GLM-Edge-V series models (#10573)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 53debe6f ci: use sccache on windows HIP jobs (#11553)
* | | | | | | | | | | | | | | | | | | | | | | | | | | cfd74c86 `sync`: minja (https://github.com/google/minja/commit/418a2364b56dc9be4ed9a1a2b0fb16fb53a7a22e) (#11574)
* | | | | | | | | | | | | | | | | | | | | | | | | | | ecef206c Implement s3:// protocol (#11511)
* | | | | | | | | | | | | | | | | | | | | | | | | | | 5bbc7362 ci: simplify cmake build commands (#11548)
* | | | | | | | | | | | | | | | | | | | | | | | | | | aa6fb132 `ci`: use sccache on windows instead of ccache (#11545)
* | | | | | | | | | | | | | | | | | | | | | | | | | | a83f5286 `tool-call`: fix llama 3.x and functionary 3.2, play nice w/ pydantic_ai package, update readme (#11539)
* | | | | | | | | | | | | | | | | | | | | | | | | | | b1bcd309 fix stop regression (#11543)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | 5783575c Fix chatml fallback for unsupported builtin templates (when --jinja not enabled) (#11533)
* | | | | | | | | | | | | | | | | | | | | | | | | | 4a2b196d server : fix --jinja when there's no tools or schema (typo was forcing JSON) (#11531)
* | | | | | | | | | | | | | | | | | | | | | | | | | 1bd3047a common: Add missing va_end (#11529)
* | | | | | | | | | | | | | | | | | | | | | | | | | a2df2787 server : update help metrics processing/deferred (#11512)
| |_|/ / / / / / / / / / / / / / / / / / / / / / /  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | 553f1e46 `ci`: ccache for all github worfklows (#11516)
* | | | | | | | | | | | | | | | | | | | | | | | | 8b576b6c Tool call support (generic + native for Llama, Functionary, Hermes, Mistral, Firefunction, DeepSeek) w/ lazy grammars (#9639)
* | | | | | | | | | | | | | | | | | | | | | | | | 27d135c9 HIP: require at least HIP 5.5
* | | | | | | | | | | | | | | | | | | | | | | | | 6af1ca48 HIP: Prepare reduction operators for wave 64
* | | | | | | | | | | | | | | | | | | | | | | | | c300e68e CUDA/HIP: add warp_size to cuda_device_info
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | 3d804dec sync: minja (#11499)
* | | | | | | | | | | | | | | | | | | | | | | | ffd0821c vocab : correctly identify LF token for GPT-2 style BPE tokenizer (#11496)
* | | | | | | | | | | | | | | | | | | | | | | | 4314e56c server : use lambda instead of std::bind (#11507)
* | | | | | | | | | | | | | | | | | | | | | | | 496e5bf4 server : (docs) added response format for /apply-template [no ci] (#11503)
* | | | | | | | | | | | | | | | | | | | | | | | 7919256c readme : reference examples relative links (#11505)
* | | | | | | | | | | | | | | | | | | | | | | | e0449763 server : update json snippets in README.md [no ci] (#11492)
* | | | | | | | | | | | | | | | | | | | | | | | eb7cf15a server : add /apply-template endpoint for additional use cases of Minja functionality (#11489)
* | | | | | | | | | | | | | | | | | | | | | | | 66ee4f29 vulkan: implement initial support for IQ2 and IQ3 quantizations (#11360)
* | | | | | | | | | | | | | | | | | | | | | | | e51c47b4 server : update auto gen files comments [no ci] (#11484)
* | | | | | | | | | | | | | | | | | | | | | | | 2711d021 vulkan: Catch pipeline creation failure and print an error message (#11436)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | f0d4b29e Parse https://ollama.com/library/ syntax (#11480)
* | | | | | | | | | | | | | | | | | | | | | | 81585779 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | 1a0e87d2 ggml : add option to not print stack on abort (ggml/1081)
* | | | | | | | | | | | | | | | | | | | | | | d2e518e9 ggml-cpu : fix ggml_graph_compute_thread did not terminate on abort. (ggml/1065)
* | | | | | | | | | | | | | | | | | | | | | | b636228c embedding : enable --no-warmup option (#11475)
* | | | | | | | | | | | | | | | | | | | | | | 325afb37 llama: fix missing k_cache store for rwkv6qwen2 (#11445)
* | | | | | | | | | | | | | | | | | | | | | | 794fe23f cmake: add hints for locating ggml on Windows using Llama find-package (#11466)
* | | | | | | | | | | | | | | | | | | | | | | cf8cc856 server : Fixed wrong function name in llamacpp server unit test (#11473)
* | | | | | | | | | | | | | | | | | | | | | | d0c08040 ci : fix build CPU arm64 (#11472)
* | | | | | | | | | | | | | | | | | | | | | | be5ef796 HIP: Supress transformation warning in softmax.cu
* | | | | | | | | | | | | | | | | | | | | | | cae9fb43 HIP: Only call rocblas_initialize on rocblas versions with the multiple instantation bug (#11080)
* | | | | | | | | | | | | | | | | | | | | | | 7fee2889 Add github protocol pulling and http:// (#11465)
* | | | | | | | | | | | | | | | | | | | | | | d7d1ecca docker: allow installing pip packages system-wide (#11437)
* | | | | | | | | | | | | | | | | | | | | | | 4bf3119d cmake : don't fail on `GGML_CPU=OFF` (#11457)
* | | | | | | | | | | | | | | | | | | | | | | f643120b docker: add perplexity and bench commands to full image (#11438)
* | | | | | | | | | | | | | | | | | | | | | | 6e84b0ab SYCL : SOFTMAX F16 mask support and other fixes (#11261)
* | | | | | | | | | | | | | | | | | | | | | | 2b8525d5 Handle missing model in CLI parameters for llama-run (#11399)
* | | | | | | | | | | | | | | | | | | | | | | a4417ddd Add new hf protocol for ollama (#11449)
* | | | | | | | | | | | | | | | | | | | | | | d6d24cd9 AMD: parse the architecture as supplied by gcnArchName (#11244)
* | | | | | | | | | | | | | | | | | | | | | | a5203b44 llama : minor fixes for up llama load model speed (#11448)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | df984e01 llama: refactor llama_decode_impl (#11381)
* | | | | | | | | | | | | | | | | | | | | | acd38efe metal: Handle null returned from MTLCreateSystemDefaultDevice() (#11441)
* | | | | | | | | | | | | | | | | | | | | | caf773f2 docker : fix ARM build and Vulkan build (#11434)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 178a7eb9 metal : use residency sets (#11427)
* | | | | | | | | | | | | | | | | | | | | 6f53d8a6 docker: add missing vulkan library to base layer and update to 24.04 (#11422)
* | | | | | | | | | | | | | | | | | | | | 19f65187 cmake: add ggml find package (#11369)
* | | | | | | | | | | | | | | | | | | | | 1d8ee060 rpc: fix register position (#11424)
* | | | | | | | | | | | | | | | | | | | | 2cc9b8c3 readme : update hot topics
* | | | | | | | | | | | | | | | | | | | | f35726c2 build: apply MSVC /bigobj option to c/cpp files only (#11423)
* | | | | | | | | | | | | | | | | | | | | 4a75d193 vulkan: compile shaders on-demand (#11406)
* | | | | | | | | | | | | | | | | | | | | 26771a14 Hip: disable VMM on hip as it seams that it dosent work in some configurations (#11420)
* | | | | | | | | | | | | | | | | | | | | ca6baf76 build: add /bigobj to MSVC build (#11407)
* | | | | | | | | | | | | | | | | | | | | 6e264a90 docker : add GGML_CPU_ARM_ARCH arg to select ARM architecture to build for (#11419)
* | | | | | | | | | | | | | | | | | | | | 49b0e3ce server : fix cleaning up stream task (#11418)
* | | | | | | | | | | | | | | | | | | | | 20a75815 docker : fix CPU ARM build (#11403)
* | | | | | | | | | | | | | | | | | | | | 00c24acb ci : fix line breaks on windows builds (#11409)
* | | | | | | | | | | | | | | | | | | | | 466ea66f CANN: Add Ascend CANN build ci (#10217)
* | | | | | | | | | | | | | | | | | | | | 5f0db952 hip : Add hipGraph and VMM support to ROCM (#11362)
| | | | | | | | | | | | | | | | | | | | | * de9d2c6f test [pack]
| | | | | | | | | | | | | | | | | | | | | * df0edbb0 test
| | | | | | | | | | | | | | | | | | | | | * 202b1e71 ci : allow creating artifacts on PRs on demand
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | c5d9effb CUDA: fix FP16 cuBLAS GEMM (#11396)
* | | | | | | | | | | | | | | | | | | | | 9fbadaef rocBLAS: Avoid fp32->fp16->fp32 conversion on cdna (#11356)
* | | | | | | | | | | | | | | | | | | | | 9755129c release : pack /lib in the packages (#11392)
* | | | | | | | | | | | | | | | | | | | | a07c2c8a docs : Update readme to build targets for local docker build (#11368)
* | | | | | | | | | | | | | | | | | | | | 8137b4bb CPU/CUDA: fix (GQA) mul mat back, add CUDA support (#11380)
* | | | | | | | | | | | | | | | | | | | | 1af6945e cmake : avoid -march=native when reproducible build is wanted (#11366)
| | | | | | | | | | | | | | | | | | | | | * 969b2646 Revert "TMP : push artifacts"
| | | | | | | | | | | | | | | | | | | | | * 5740ec7a ci : change ubuntu package to 22.04
| | | | | | | | | | | | | | | | | | | | | * 872fd184 ci : fix typo
| | | | | | | | | | | | | | | | | | | | | * 39d06218 ci : macos set build rpath to "@loader_path"
| | | | | | | | | | | | | | | | | | | | | * dae44bf2 ci : change back to ubuntu latest
| | | | | | | | | | | | | | | | | | | | | * 537b09e7 TMP : push artifacts
| | | | | | | | | | | | | | | | | | | | | * 8b2ed1e4 ci : remove obsolete MacOS build
| | | | | | | | | | | | | | | | | | | | | * f9f65f01 ci : try to fix macos build rpaths
| | | | | | | | | | | | | | | | | | | | | * 56e26a7f ci : change ubuntu build from latest to 20.04
| | | | | | | | | | | | | | | | | | | | | * 194358e3 ci : restore the original HIP commands
| | | | | | | | | | | | | | | | | | | | | * 50455ded ci : fix HIP cmake compiler options to be on first line
| | | | | | | | | | | | | | | | | | | | | * 564353c9 Revert "TMP : push artifacts"
| | | | | | | | | | | | | | | | | | | | | * 4decf2c4 TMP : push artifacts
| | | | | | | | | | | | | | | | | | | | | * 3a35bfe1 cmake : put libs in /bin
| | | | | | | | | | | | | | | | | | | | | * ff4cb6ef release : pack /lib and /include in the packages
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 01f37edf Update llama-run README.md (#11386)
* | | | | | | | | | | | | | | | | | | | | c07e87f3 server : (webui) put DeepSeek R1 CoT in a collapsible <details> element (#11364)
* | | | | | | | | | | | | | | | | | | | | 564804b7 tests: fix some mul_mat test gaps (#11375)
* | | | | | | | | | | | | | | | | | | | | 05f63cc9 Update documentation (#11373)
* | | | | | | | | | | | | | | | | | | | | f7fb43cd Add -ngl (#11372)
* | | | | | | | | | | | | | | | | | | | | 58456616 server : add more clean up when cancel_tasks is called (#11340)
* | | | | | | | | | | | | | | | | | | | | f211d1dc Treat hf.co/ prefix the same as hf:// (#11350)
* | | | | | | | | | | | | | | | | | | | | 955a6c2d Vulkan-run-test: fix mmq_wg_denoms (#11343)
* | | | | | | | | | | | | | | | | | | | | 1971adf5 vulkan: sort shaders for more deterministic binary (#11315)
* | | | | | | | | | | | | | | | | | | | | 5245729e vulkan: fix diag_mask_inf (#11323)
* | | | | | | | | | | | | | | | | | | | | 6152129d main : update README documentation for batch size (#11353)
* | | | | | | | | | | | | | | | | | | | | 16d3df7a readme : add plugin links (#11355)
* | | | | | | | | | | | | | | | | | | | | 12c2bdf2 server : fix draft context not being released (#11354)
* | | | | | | | | | | | | | | | | | | | | c64d2bec `minja`: sync at https://github.com/google/minja/commit/0f5f7f2b3770eb682fbc11763266d45204173686 (#11352)
* | | | | | | | | | | | | | | | | | | | | 96f40539 Adding logprobs to /v1/completions (#11344)
* | | | | | | | | | | | | | | | | | | | | a94f3b27 `common`: utils to split / join / repeat strings (from json converter) (#11342)
* | | | | | | | | | | | | | | | | | | | | 3e3357fd llava : support Minicpm-omni (#11289)
* | | | | | | | | | | | | | | | | | | | | 6171c9d2 Add Jinja template support (#11016)
* | | | | | | | | | | | | | | | | | | | | e28245f3 export-lora : fix tok_embd tensor (#11330)
* | | | | | | | | | | | | | | | | | | | | 6da5bec8 rpc : better caching of the base buffer pointer (#11331)
* | | | | | | | | | | | | | | | | | | | | 2e2f8f09 linenoise.cpp refactoring (#11301)
* | | | | | | | | | | | | | | | | | | | | 2139667e metal : fix out-of-bounds write (#11314)
* | | | | | | | | | | | | | | | | | | | | 80d0d6b4 common : add -hfd option for the draft model (#11318)
* | | | | | | | | | | | | | | | | | | | | aea8ddd5 vulkan: fix coopmat2 validation failures (#11284)
* | | | | | | | | | | | | | | | | | | | | 9f7add1c examples : fix add_special conditions (#11311)
* | | | | | | | | | | | | | | | | | | | | 90d987b1 mmap: add include for cerrno (#11296)
* | | | | | | | | | | | | | | | | | | | | a4251edd cmake: fix shell command quoting in build-info script (#11309)
* | | | | | | | | | | | | | | | | | | | | ec7f3ac9 llama : add support for Deepseek-R1-Qwen distill model (#11310)
* | | | | | | | | | | | | | | | | | | | | ef6dada6 cont : fix whitespaces (#11305)
* | | | | | | | | | | | | | | | | | | | | ae3c1db2 llama : re-add LLM_ARCH_PHIMOE (#11305)
* | | | | | | | | | | | | | | | | | | | | 92bc4939 tests : increase timeout when sanitizers are enabled (#11300)
* | | | | | | | | | | | | | | | | | | | | b9daaffe simple-chat : fix BOS being added to each message (#11278)
* | | | | | | | | | | | | | | | | | | | | 99487b57 SYCL: Introducing memory host pool (#11251)
* | | | | | | | | | | | | | | | | | | | | a1649cc1 Adding linenoise.cpp to llama-run (#11252)
* | | | | | | | | | | | | | | | | | | | | 4dd34ff8 cmake : add sanitizer flags for llama.cpp (#11279)
* | | | | | | | | | | | | | | | | | | | | f30f0992 server : implement cancellable request (#11285)
* | | | | | | | | | | | | | | | | | | | | f26c8741 scripts : restore hf.sh (#11288)
| | | | | | | | | | | | | | | | | | | | | * c9e7cbb0 safer jinja `llama_chat_templates` struct
| | | | | | | | | | | | | | | | | | | | | * cc503564 minja: fix vigogne (https://github.com/google/minja/pull/22)
| | | | | | | | | | | | | | | | | | | | | * e3c475cd Disable jinja test that has a cryptic windows failure
| | | | | | | | | | | | | | | | | | | | | * 0e74c9da Add missing optional include to server.cpp
| | | | | | | | | | | | | | | | | | | | | * fc60802b Rm unused optional include
| | | | | | | | | | | | | | | | | | | | | * 5074e6fe Fix copy elision warning
| | | | | | | | | | | | | | | | | | | | | * 33322e82 Flush stdout in chat template before potential crash
| | | | | | | | | | | | | | | | | | | | | * e63520f3 Forward decl minja::chat_template to avoid eager json dep
| | | | | | | | | | | | | | | | | | | | | * ee1e10e2 Normalize newlines in test-chat-templates for windows tests
| | | | | | | | | | | | | | | | | | | | | * d5fa351a Revert LLAMA_CHATML_TEMPLATE refactor
| | | | | | | | | | | | | | | | | | | | | * 81c0d437 Attempt to fix linkage of LLAMA_CHATML_TEMPLATE
| | | | | | | | | | | | | | | | | | | | | *   40db7896 Merge remote-tracking branch 'origin/master' into jinja
| | | | | | | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | | | | | | * | b75d0622 Refactor common_chat_* functions to accept minja template + use_jinja option
| | | | | | | | | | | | | | | | | | | | | * |   3ed670b6 Merge remote-tracking branch 'origin/master' into jinja
| | | | | | | | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | | | | | | | * | | 1b3bb7ee Update arg.cpp
| | | | | | | | | | | | | | | | | | | | | * | | 4daae0bf Update run.cpp
| | | | | | | | | | | | | | | | | | | | | * | | a57bb94e Update test_chat_completion.py
| | | | | | | | | | | | | | | | | | | | | * | | b7e21710 Update utils.py
| | | | | | | | | | | | | | | | | | | | | * | | b4083e41 Test chat_template in e2e test
| | | | | | | | | | | | | | | | | | | | | * | | a6afb273 Update common_chat_format_example to use minja template wrapper
| | | | | | | | | | | | | | | | | | | | | * | |   c04c50e4 Merge remote-tracking branch 'origin/master' into jinja
| | | | | | | | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | | | | | | * | | | 8dd4f334 Add --jinja to llama-run
| | | | | | | | | | | | | | | | | | | | | * | | | 18f257bf Fix deprecation
| | | | | | | | | | | | | | | | | | | | | * | | | 7c84ebc2 Test templates w/ minja
| | | | | | | | | | | | | | | | | | | | | * | | | 1aac99ad Refactor test-chat-template
| | | | | | | | | | | | | | | | | | | | | * | | | 78861a3e Wire LLM_KV_TOKENIZER_CHAT_TEMPLATE_N in llama_model_chat_template
| | | | | | | | | | | | | | | | | | | | | * | | |   cb72cf1f Merge remote-tracking branch 'origin/master' into jinja
| | | | | | | | | | | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | | | | | | | | | | * | | | | 238b9689 Update test_chat_completion.py
| | | | | | | | | | | | | | | | | | | | | * | | | | 389d79b6 Try and work around msvc++ non-macro max resolution quirk
| | | | | | | | | | | | | | | | | | | | | * | | | | ce48584f No designated initializers yet
| | | | | | | | | | | | | | | | | | | | | * | | | | 06b51595 Avoid print in get_hf_chat_template.py
| | | | | | | | | | | | | | | | | | | | | * | | | | 80138d90 Add missing <optional> include
| | | | | | | | | | | | | | | | | | | | | * | | | | e5113e8d Add --jinja and --chat-template-file flags
| | | | | | | | | | | | | | | | | | | | | * | | | | abd274a4 Copy minja from https://github.com/google/minja/commit/58f0ca6dd74bcbfbd4e71229736640322b31c7f9
| | | | | | | | | | | | | | | | | | | | | | | | | | * 90a03493 recommended way to check if the version is 0.3, as requested by ngxson
| | | | | | | | | | | | | | | | | | | | | | | | | | * b5486956 added rudimentary support for outetts v0.3 500m and 1b models
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | | 6390a998 tts : add guide tokens support (#11186)
* | | | | | | | | | | | | | | | | | | | | | | | | | 44e18ef9 vulkan: fix coopmat2 flash attention for non-contiguous inputs (#11281)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | | | | | * ba421dd0 gguf-test: tensor data comparison
| | | | | | | | | | | | | | | | | | | | | | | | | * 7000623c tests : fix gguf context use in same_tensor_data
| | | | | | | | | | | | | | | | | | | | | | | | | * e872097c cmake : apply only sanitizer flags at top level
| | | | | | | | | | | | | | | | | | | | | | | | | * 9d1b20ad cmake : move llama.cpp compile flags to top level lists
| | | | | | | | | | | | | | | | | | | | | | | | | * 9a03bc81 cmake : move sanitizer flags to llama_add_compile_flags
| | | | | | | | | | | | | | | | | | | | | | | | | * ce293d83 tests : fix compile warnings
| | | | | | | | | | | | | | | | | | | | | | | | | * 72dc7bff cmake : add sanitizer flags for llama.cpp
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | 3edfa7d3 llama.android: add field formatChat to control whether to parse special tokens when send message (#11270)
* | | | | | | | | | | | | | | | | | | | | | | | | 667d7284 rpc : early register backend devices (#11262)
* | | | | | | | | | | | | | | | | | | | | | | | | a133566d vocab : fix double-eos check (#11273)
* | | | | | | | | | | | | | | | | | | | | | | | | 960ec652 llama : fix deprecation message: vocabable -> vocab (#11269)
* | | | | | | | | | | | | | | | | | | | | | | | | 7a689c41 README : added kalavai to infrastructure list (#11216)
* | | | | | | | | | | | | | | | | | | | | | | | | bd38ddea vulkan: support copy from f32 to q4_0/q4_1/q5_0/q5_1/q8_0/iq4_nl (#11166)
* | | | | | | | | | | | | | | | | | | | | | | | | 466300fe vulkan: optimize coopmat2 q4_k/q5_k dequant functions. (#11206)
* | | | | | | | | | | | | | | | | | | | | | | | | 206bc534 vulkan: optimize coopmat2 q2_k dequant function (#11130)
* | | | | | | | | | | | | | | | | | | | | | | | | 4dbc8b9c llama : add internlm3 support (#11233)
* | | | | | | | | | | | | | | | | | | | | | | | | 9c8dcefe CUDA: backwards pass for misc. ops, add tests (#11257)
* | | | | | | | | | | | | | | | | | | | | | | | | 681149ce llama : add `llama_model_load_from_splits` (#11255)
* | | | | | | | | | | | | | | | | | | | | | | | | c67cc983 ggml: aarch64: implement SVE kernels for q4_K_q8_K vector dot (#11227)
* | | | | | | | | | | | | | | | | | | | | | | | | adc5dd92 vulkan: scale caching for k quants + misc fixes (#11081)
* | | | | | | | | | | | | | | | | | | | | | | | | f11cfdfd ci : use -no-cnv in gguf-split tests (#11254)
| | | | | | | | | | | | | | | | | | | | | | | | | * 492eaad5 ci : change python3 -> python
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | | 1d850433 fix: ggml: fix vulkan-shaders-gen build (#10448)
* | | | | | | | | | | | | | | | | | | | | | | | | 432df2d5 RoPE: fix back, CUDA support for back + noncont. (#11240)
* | | | | | | | | | | | | | | | | | | | | | | | | 0ccd7f3e examples : add embd_to_audio to tts-outetts.py [no ci] (#11235)
* | | | | | | | | | | | | | | | | | | | | | | | | f446c2cf SYCL: Add gated linear attention kernel (#11175)
* | | | | | | | | | | | | | | | | | | | | | | | | b4d92a59 ci : add -no-cnv for tests (#11238)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | bbf3e55e vocab : add dummy tokens for "no_vocab" type (#11231)
* | | | | | | | | | | | | | | | | | | | | | | | c5bf0d1b server : Improve code snippets direction between RTL text (#11221)
* | | | | | | | | | | | | | | | | | | | | | | | 091592d7 Refactor test-chat-template.cpp (#11224)
* | | | | | | | | | | | | | | | | | | | | | | | 44d1e796 sync : ggml
| | | | | | | | | | | | | | | | | | | | | | | | * 0cf9a067 vocab : minor [no ci]
| | | | | | | | | | | | | | | | | | | | | | | | * 69fc940d vocab : add dummy tokens for "no_vocab" type
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | | a4f3f5d8 scripts : sync gguf (cont)
* | | | | | | | | | | | | | | | | | | | | | | | 48e1ae0e scripts : sync gguf
* | | | | | | | | | | | | | | | | | | | | | | | d00a80e8 scripts : sync opencl
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | 504af20e server : (UI) Improve messages bubble shape in RTL (#11220)
* | | | | | | | | | | | | | | | | | | | | | | 84a44815 cli : auto activate conversation mode if chat template is available (#11214)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | 39509fb0 cuda : CUDA Graph Compute Function Refactor (precursor for performance improvements) (#11042)
* | | | | | | | | | | | | | | | | | | | | | a29f0870 contrib : add naming guidelines (cont) (#11177)
* | | | | | | | | | | | | | | | | | | | | | 437e05f7 server : (UI) Support for RTL text as models input or output (#11208)
* | | | | | | | | | | | | | | | | | | | | | ca001f66 contrib : add naming guidelines (cont) (#11177)
* | | | | | | | | | | | | | | | | | | | | | 00b4c3da common : support tag-based --hf-repo like on ollama (#11195)
* | | | | | | | | | | | | | | | | | | | | | 7426a26b contrib : add naming guidelines (#11177)
* | | | | | | | | | | | | | | | | | | | | | 8f70fc3d llama : remove 'd' from bad special token log (#11212)
* | | | | | | | | | | | | | | | | | | | | | 1244cdcf ggml : do not define GGML_USE_CUDA when building with GGML_BACKEND_DL (#11211)
* | | | | | | | | | | | | | | | | | | | | | 924518e2 Reset color before we exit (#11205)
* | | | | | | | | | | | | | | | | | | | | | 9a483999 llama : fix chat template gguf key (#11201)
| | | | | | | | | | | | | | | | | | | | | | * a97b3621 ggml : ggml_backend_graph_copy -> ggml_backend_graph_copy_state
| | | | | | | | | | | | | | | | | | | | | | * afd40ea2 minor : better names
| | | | | | | | | | | | | | | | | | | | | | * 36803b19 common : cont
| | | | | | | | | | | | | | | | | | | | | | * a59ee7c4 common : cont
| | | | | | | | | | | | | | | | | | | | | | * 10eb8740 shadow : cont gcc
| | | | | | | | | | | | | | | | | | | | | | * f65e3d32 ggml : ggml_backend_graph_copy -> ggml_backend_graph_copy_init
| | | | | | | | | | | | | | | | | | | | | | * 439e68c1 cmake : re-enable GCC -Wshadow
| | | | | | | | | | | | | | | | | | | | | | * 34889bf8 cmake : cont
| | | | | | | | | | | | | | | | | | | | | | * e159e775 cmake : disable -Wshadow for GCC
| | | | | | | | | | | | | | | | | | | | | | * 9a735ae6 examplse : de-shadow
| | | | | | | | | | | | | | | | | | | | | | * 82caffa7 llama : de-shadow libllama [no ci]
| | | | | | | | | | | | | | | | | | | | | | * 32e7b9dc llama : de-shadow (cont) [no ci]
| | | | | | | | | | | | | | | | | | | | | | * 0127774a llama : remove unused mutable n_tokens [no ci]
| | | | | | | | | | | | | | | | | | | | | | * 0bebe45a llama : de-shadow (wip) [no ci]
| | | | | | | | | | | | | | | | | | | | | | * 168324a3 cmake : enable -Wshadow for C++ code [no ci]
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | 08f10f69 llama : remove notion of CLS token (#11064)
* | | | | | | | | | | | | | | | | | | | | | afa8a9ec llama : add `llama_vocab`, functions -> methods, naming (#11110)
| | | | | | | | | | | | | | | | | | | | | | * 9af90481 Vulkan: Add renderdoc tracing support
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | c05e8c99 gguf-py: fixed local detection of gguf package (#11180)
* | | | | | | | | | | | | | | | | | | | | | 2739a71e convert : sort print supported models [no ci] (#11179)
* | | | | | | | | | | | | | | | | | | | | | ba8a1f9c examples : add README.md to tts example [no ci] (#11155)
* | | | | | | | | | | | | | | | | | | | | | ff3fcabc convert : add --print-supported-models option (#11172)
* | | | | | | | | | | | | | | | | | | | | | c3f9d257 Vulkan: Fix float16 use on devices without float16 support + fix subgroup_size_control validation error (#11161)
* | | | | | | | | | | | | | | | | | | | | | ee7136c6 llama: add support for QRWKV6 model architecture (#11001)
* | | | | | | | | | | | | | | | | | | | | | c6860cc7 SYCL: Refactor ggml_sycl_compute_forward (#11121)
| | | | | | | | | | | | | | | | | | | | | | * fbddb262 ggml-cuda : use i and j instead of i0 and i in vec_dot_tq2_0_q8_1
| | | | | | | | | | | | | | | | | | | | | | * b6fc9f03 ggml-metal : supports_op returns false for ternary types
| | | | | | | | | | | | | | | | | | | | | | * 946796fc ggml-cuda : slight optimizations for TQ2_0
| | | | | | | | | | | | | | | | | | | | | | * f5fddb6d ggml-cuda : remove some superfluous comments for TQ2_0 tile loading
| | | | | | | | | | | | | | | | | | | | | | *   983aa09b Merge branch 'master' into compilade/cuda-tq2_0
| | | | | | | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | 1204f972 doc: add cuda guide for fedora (#11135)
* | | | | | | | | | | | | | | | | | | | | | | 8eceb888 server : add tooltips to settings and themes btn (#11154)
* | | | | | | | | | | | | | | | | | | | | | | f8feb4b0 model: Add support for PhiMoE arch (#11003)
* | | | | | | | | | | | | | | | | | | | | | | be0e950c media : remove old img [no ci]
* | | | | | | | | | | | | | | | | | | | | | | d9feae1c llama-chat : add phi 4 template (#11148)
* | | | | | | | | | | | | | | | | | | | | | | 8d59d911 fix: add missing msg in static_assert (#11143)
* | | | | | | | | | | | | | | | | | | | | | | 8a1d9c25 gguf-py : move scripts directory (#11116)
* | | | | | | | | | | | | | | | | | | | | | | 1bf839b1 Enhance user input handling for llama-run (#11138)
* | | | | | | | | | | | | | | | | | | | | | | f7cd1330 ci : use actions from ggml-org (#11140)
* | | | | | | | | | | | | | | | | | | | | | | 4d2b3d88 lora : improve compat with `mergekit-extract-lora` (#11131)
* | | | | | | | | | | | | | | | | | | | | | | c07d437b llama : avoid hardcoded QK_K (#11061)
* | | | | | | | | | | | | | | | | | | | | | | 99a3755a sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | c792dcf4 ggml : allow loading backend with env variable (ggml/1059)
* | | | | | | | | | | | | | | | | | | | | | | 80ccf5d7 ci : pin dependency to specific version (#11137)
* | | | | | | | | | | | | | | | | | | | | | | a3c1232c arg : option to exclude arguments from specific examples (#11136)
* | | | | | | | | | | | | | | | | | | | | | | 8cef75c7 llamafile : ppc64le MMA INT8 implementation (#10912)
* | | | | | | | | | | | | | | | | | | | | | | 0d52a69e ci : fix cmake option (#11125)
* | | | | | | | | | | | | | | | | | | | | | | 02f04301 Disable GL_KHR_cooperative_matrix Vulkan extension if not available. (#11117)
* | | | | | | | | | | | | | | | | | | | | | | bec2183f fix: Vulkan shader gen binary path when Cross-compiling (#11096)
* | | | | | | | | | | | | | | | | | | | | | | 53ff6b9b GGUF: C++ refactor, backend support, misc fixes (#11030)
* | | | | | | | | | | | | | | | | | | | | | | 017cc5f4 ggml-backend : only offload from host buffers (fix) (#11124)
* | | | | | | | | | | | | | | | | | | | | | | a3d50bc0 ggml-backend : only offload from host buffers (#11120)
* | | | | | | | | | | | | | | | | | | | | | | a4dd4900 rpc : code cleanup (#11107)
* | | | | | | | | | | | | | | | | | | | | | | c0d6f790 SYCL: Use get_multi_ptr instead of deprecated get_pointer in wkv6 (#11087)
* | | | | | | | | | | | | | | | | | | | | | | dc7cef9f llama-run : fix context size (#11094)
* | | | | | | | | | | | | | | | | | | | | | | ecebbd29 llama : remove unused headers (#11109)
* | | | | | | | | | | | | | | | | | | | | | | 96be8c32 github : add cmd line field to bug report (#11090)
* | | | | | | | | | | | | | | | | | | | | | | e6e7c75d server : fix extra BOS in infill endpoint (#11106)
* | | | | | | | | | | | | | | | | | | | | | | 09186fab llama : remove check flash_attn with lora (#11104)
* | | | | | | | | | | | | | | | | | | | | | | 96a1dc27 llama : prevent system info string accumulation across calls (#11101)
* | | | | | | | | | | | | | | | | | | | | | | 6369f867 llama : rename missed batch params/vars to ubatch (#10059)
* | | | | | | | | | | | | | | | | | | | | | | 47182dd0 llama : update llama_model API names (#11063)
* | | | | | | | | | | | | | | | | | | | | | | 3e6e7a6b tokenize : escape the prompt (#11058)
* | | | | | | | | | | | | | | | | | | | | | | ae2f606b mmap : fix fileno macro clash (#11076)
* | | | | | | | | | | | | | | | | | | | | | | 727368c6 llama : use LLAMA_TOKEN_NULL (#11062)
* | | | | | | | | | | | | | | | | | | | | | | 5047dd35 llama : use _impl suffix instead of _internal (#11060)
* | | | | | | | | | | | | | | | | | | | | | | 46e3556e CUDA: add BF16 support (#11093)
* | | | | | | | | | | | | | | | | | | | | | | b56f079e Vulkan: Add device-specific blacklist for coopmat for the AMD proprietary driver (#11074)
* | | | | | | | | | | | | | | | | | | | | | | 9394bbd4 llama : Add support for DeepSeek V3 (#11049)
* | | | | | | | | | | | | | | | | | | | | | | f922a9c5 [GGML][RPC] Support for models with non-512-aligned tensors over RPC. (#11047)
* | | | | | | | | | | | | | | | | | | | | | | 46be9422 llama : add support for the cohere2 model architecture (#10900)
* | | | | | | | | | | | | | | | | | | | | | | 78c67851 sync : ggml
* | | | | | | | | | | | | | | | | | | | | | | 5e3b08d6 ggml : do not install metal source when embed library (ggml/1054)
* | | | | | | | | | | | | | | | | | | | | | | db68c93b ggml : improve inputs log sched_print_assignments (ggml/1053)
* | | | | | | | | | | | | | | | | | | | | | | c31fc8b9 fix: Vulkan shader gen binary path (#11037)
| | | | | | | | | | | | | | | | | | | | | | * fb43d5e8 ggml-cuda : cleanup TQ2_0
| | | | | | | | | | | | | | | | | | | | | | * 970b5ab7 ggml-cuda : add TQ2_0 support
| | | | | | | | | | | | | | | | | | | | | | | * 9605c5fb cmake : remove explicit _XOPEN_SOURCE
| | | | | | | | | | | | | | | | | | | | | | | * eb76b842 feat(ci): add visionOS build workflow
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | 4b0c638b common : disable KV cache shifting automatically for unsupported models (#11053)
* | | | | | | | | | | | | | | | | | | | | | | e7da954e metal : avoid uint (#11019)
* | | | | | | | | | | | | | | | | | | | | | | f66f5829 llama : refactor `src/llama.cpp` (#10902)
* | | | | | | | | | | | | | | | | | | | | | | 2f0ee84b server: bench: minor fixes (#10765)
* | | | | | | | | | | | | | | | | | | | | | | 0da5d860 server : allow using LoRA adapters per-request (#10994)
* | | | | | | | | | | | | | | | | | | | | | | a45433ba readme : add llama-swap to infrastructure section (#11032)
* | | | | | | | | | | | | | | | | | | | | | | 0827b2c1 ggml : fixes for AVXVNNI instruction set with MSVC and Clang (#11027)
* | | | | | | | | | | | | | | | | | | | | | | 45095a61 server : clean up built-in template detection (#11026)
* | | | | | | | | | | | | | | | | | | | | | | 5896c652 server : add OAI compat for /v1/completions (#10974)
* | | | | | | | | | | | | | | | | | | | | | | bc7b1f86 convert : fix Llama-3_1-Nemotron-51B rope settings (#11008)
* | | | | | | | | | | | | | | | | | | | | | | 6e1531ac common, examples, ggml : fix MSYS2 GCC compiler errors and warnings when building with LLAMA_CURL=ON and GGML_OPENCL=ON (#11013)
* | | | | | | | | | | | | | | | | | | | | | | 716bd6de vulkan: optimize mul_mat for small values of N (#10991)
* | | | | | | | | | | | | | | | | | | | | | | c250ecb3 android : fix llama_batch free (#11014)
| | | | | | | | | | | | | | | | | | | | | | | * aa014d7e Use mutex instead of atomics for vk_instance counters
| | | | | | | | | | | | | | | | | | | | | | | * d9b0958f Vulkan: Refactor to make sure Vulkan instance is destroyed properly on program exit
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | | a813badb vulkan: im2col and matmul optimizations for stable diffusion (#10942)
* | | | | | | | | | | | | | | | | | | | | | | fdd21889 vulkan: Use push constant offset to handle misaligned descriptors (#10987)
* | | | | | | | | | | | | | | | | | | | | | | f865ea14 server: added more docs for response_fields field (#10995)
* | | | | | | | | | | | | | | | | | | | | | | 16cdce7b server : fix token duplication when streaming with stop strings (#10997)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | | d79d8f39 vulkan: multi-row k quants (#10846)
* | | | | | | | | | | | | | | | | | | | | | d283d02b examples, ggml : fix GCC compiler warnings (#10983)
* | | | | | | | | | | | | | | | | | | | | | 9ba399df server : add support for "encoding_format": "base64" to the */embeddings endpoints (#10967)
* | | | | | | | | | | | | | | | | | | | | | 2cd43f49 ggml : more perfo with llamafile tinyblas on x86_64 (#10714)
* | | | | | | | | | | | | | | | | | | | | | 09fe2e76 server:  allow filtering llama server response fields (#10940)
* | | | | | | | | | | | | | | | | | | | | | 30caac3a llama : the WPM vocabs use the CLS token as BOS (#10930)
* | | | | | | | | | | | | | | | | | | | | | 60cfa728 ggml : use wstring for backend search paths (#10960)
* | | | | | | | | | | | | | | | | | | | | | 3327bb0f ggml : fix arm enabled features check (#10961)
* | | | | | | | | | | | | | | | | | | | | | 32d6ee63 ggml : fix const usage in SSE path (#10962)
* | | | | | | | | | | | | | | | | | | | | | 14b699ec server : fix missing model id in /model endpoint (#10957)
* | | | | | | | | | | | | | | | | | | | | | 485dc012 server : add system_fingerprint to chat/completion (#10917)
* | | | | | | | | | | | | | | | | | | | | | 86bf31cf rpc-server : add support for the SYCL backend (#10934)
* | | | | | | | | | | | | | | | | | | | | | b92a14a8 llama : support InfiniAI Megrez 3b (#10893)
* | | | | | | | | | | | | | | | | | | | | | 6f0c9e03 llama : support for Llama-3_1-Nemotron-51B (#10669)
* | | | | | | | | | | | | | | | | | | | | | dab76c92 llama-run : include temperature option (#10899)
* | | | | | | | | | | | | | | | | | | | | | 7024d59e ggml : fix run-time on FreeBSD in get_executable_path() (#10948)
* | | | | | | | | | | | | | | | | | | | | | 7c0e2858 devops : add docker-multi-stage builds (#10832)
* | | | | | | | | | | | | | | | | | | | | | 7ae33a61 llama : add Falcon3 support (#10883)
* | | | | | | | | | | | | | | | | | | | | | ebdee947 vulkan: build fixes for 32b (#10927)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 5cd85b5e convert : add BertForMaskedLM (#10919)
* | | | | | | | | | | | | | | | | | | | | a91a4136 vulkan: optimize coopmat2 dequant functions (#10855)
* | | | | | | | | | | | | | | | | | | | | e34c5af4 ggml-cpu: replace NEON asm with intrinsics in ggml_gemv_q4_0_4x8_q8_0() (#10874)
* | | | | | | | | | | | | | | | | | | | | eb5c3dc6 SYCL: Migrate away from deprecated ggml_tensor->backend (#10840)
* | | | | | | | | | | | | | | | | | | | | 0ca416c9 server : (UI) fix copy to clipboard function (#10916)
* | | | | | | | | | | | | | | | | | | | | 21ae3b9b ggml : add test for SVE and disable when it fails (#10906)
* | | | | | | | | | | | | | | | | | | | | 0a11f8b7 convert : fix RWKV v6 model conversion (#10913)
* | | | | | | | | | | | | | | | | | | | | d408bb92 clip : disable GPU support (#10896)
* | | | | | | | | | | | | | | | | | | | | 5cab3e4a llama : minor grammar refactor (#10897)
* | | | | | | | | | | | | | | | | | | | | 36319dec tts : small QoL for easy model fetch (#10903)
* | | | | | | | | | | | | | | | | | | | | 57bb2c40 server : fix logprobs, make it OAI-compatible (#10783)
* | | | | | | | | | | | | | | | | | | | | a3c33b1d ggml: fix arm build with gcc (#10895)
* | | | | | | | | | | | | | | | | | | | | 2fffc52b llama : fix Roberta embeddings (#10856)
* | | | | | | | | | | | | | | | | | | | | 7585edbd convert : Add support for Microsoft Phi-4 model  (#10817)
* | | | | | | | | | | | | | | | | | | | | cd920d0a tests: disable GGUF test for bad value size (#10886)
* | | | | | | | | | | | | | | | | | | | | 7909e858 llama-run : improve progress bar (#10821)
* | | | | | | | | | | | | | | | | | | | | 9177484f ggml : fix arm build (#10890)
* | | | | | | | | | | | | | | | | | | | | 0bf2d10c tts : add OuteTTS support (#10784)
* | | | | | | | | | | | | | | | | | | | | 7bbb5acf server: avoid overwriting Authorization header (#10878)
* | | | | | | | | | | | | | | | | | | | | 152610ed server : output embeddings for all tokens when pooling = none (#10861)
* | | | | | | | | | | | | | | | | | | | | 0e70ba68 server : add "tokens" output (#10853)
* | | | | | | | | | | | | | | | | | | | | 46828872 server : (embeddings) using same format for "input" and "content" (#10872)
* | | | | | | | | | | | | | | | | | | | | 6b064c92 docs: Fix HIP (née hipBLAS) in README (#10880)
| | | | | | | | | | | | | | | | | | | | | * fe9235d7 Force max subgroup size for coopmat shaders
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 4da69d1a Revert "llama : add Falcon3 support (#10864)" (#10876)
* | | | | | | | | | | | | | | | | | | | | d62b532c Use model->gguf_kv for loading the template instead of using the C API. (#10868)
* | | | | | | | | | | | | | | | | | | | | 081b29bd tests: add tests for GGUF (#10830)
* | | | | | | | | | | | | | | | | | | | | 5437d4aa sync : ggml
* | | | | | | | | | | | | | | | | | | | | 78f76676 cmake : fix "amd64" processor string (whisper/2638)
* | | | | | | | | | | | | | | | | | | | | 8dd19a48 vulkan : fix soft_max.comp division by zero (whisper/2633)
* | | | | | | | | | | | | | | | | | | | | 130d0c90 ggml : remove return from ggml_gallocr_allocate_node (ggml/1048)
* | | | | | | | | | | | | | | | | | | | | 3919da8e ggml : add check for grad_accs (ggml/1046)
* | | | | | | | | | | | | | | | | | | | | 0006f5a7 ggml : update ggml_backend_cpu_device_supports_op (#10867)
* | | | | | | | | | | | | | | | | | | | | 05c3a444 server : fill usage info in embeddings and rerank responses (#10852)
| | | | | | | | | | | | | | | | | | | | | * 4fbb801a ggml : update ggml_backend_cpu_device_supports_op
| | | | | | | | | | | | | | | | | | | | | * 8cc7145c ggml : disable tests involving i-matrix quantization
| | | | | | | | | | | | | | | | | | | | | * b0597b14 ggml : fix cpy op for IQ-quants to use reference impl
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 382bc7f2 llama : add Falcon3 support (#10864)
* | | | | | | | | | | | | | | | | | | | | 4f51968a readme : update typos (#10863)
* | | | | | | | | | | | | | | | | | | | | 227d7c5a server : (UI) fix missing async generator on safari (#10857)
* | | | | | | | | | | | | | | | | | | | | 7b1ec53f vulkan: bugfixes for small subgroup size systems + llvmpipe test (#10809)
* | | | | | | | | | | | | | | | | | | | | 160bc039 rwkv6: add wkv6 support for Vulkan backend (#10829)
* | | | | | | | | | | | | | | | | | | | | 08ea539d unicode : improve naming style (#10838)
* | | | | | | | | | | | | | | | | | | | | 644fd71b sampling : refactor + optimize penalties sampler (#10803)
* | | | | | | | | | | | | | | | | | | | | 4ddd199f llava : Allow locally downloaded models for QwenVL (#10833)
* | | | | | | | | | | | | | | | | | | | | a0974156 llama : add Deepseek MoE v1 & GigaChat models (#10827)
* | | | | | | | | | | | | | | | | | | | | 87cf323c scripts : change build path to "build-bench" for compare-commits.sh (#10836)
* | | | | | | | | | | | | | | | | | | | | 5478bbcd server: (UI) add syntax highlighting and latex math rendering (#10808)
* | | | | | | | | | | | | | | | | | | | | b5ae1ddf gguf-py : bump to v0.13.0
| | | | | | | | | | | | | | | | | | | | | * 3e92f4ec cont [no ci]
| | | | | | | | | | | | | | | | | | | | | * 7a20c287 unicode : improve naming style
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | * 7e9208e4 scripts : change build path to "build-bench" for compare-commits.sh
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 89d604f2 server: Fix `has_next_line` in JSON response (#10818)
* | | | | | | | | | | | | | | | | | | | | e52aba53 nix: allow to override rocm gpu targets (#10794)
* | | | | | | | | | | | | | | | | | | | | ba1cb19c llama : add Qwen2VL support + multimodal RoPE (#10361)
* | | | | | | | | | | | | | | | | | | | | 56eea078 Removes spurious \r in output that causes logging in journalctl to treat lines as binary and therefore hidden by default (#10771)
* | | | | | | | | | | | | | | | | | | | | a76c56fa Introducing experimental OpenCL backend with support for Qualcomm Adreno GPUs (#10693)
* | | | | | | | | | | | | | | | | | | | | c27ac678 Opt class for positional argument handling (#10508)
* | | | | | | | | | | | | | | | | | | | | 11e07fd6 fix: graceful shutdown for Docker images (#10815)
* | | | | | | | | | | | | | | | | | | | | 4601a8bb gguf-py : numpy 2 newbyteorder fix (#9772)
* | | | | | | | | | | | | | | | | | | | | 9f35e445 Fix crash caused by ggml_backend_load_all when launching on Android Activity (#10812)
* | | | | | | | | | | | | | | | | | | | | 64ae0655 vulkan: small mul_mat_vec optimizations (#10665)
* | | | | | | | | | | | | | | | | | | | | 83ed24a9 SYCL: Reduce most of the compiler warnings (#10748)
* | | | | | | | | | | | | | | | | | | | | d583cd03 ggml : Fix compilation issues on ARM platform when building without fp16 (#10811)
* | | | | | | | | | | | | | | | | | | | | adffa6ff common : improve -ctv -ctk CLI arguments (#10806)
* | | | | | | | | | | | | | | | | | | | | 274ec65a contrib : add ngxson as codeowner (#10804)
* | | | | | | | | | | | | | | | | | | | | 8faa1d4d CUDA: faster non-contiguous concat (#10760)
* | | | | | | | | | | | | | | | | | | | | cb13ef85 remove CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS (#10797)
* | | | | | | | | | | | | | | | | | | | | 4064c0e3 Vulkan: Use improved q4_k and q5_k dequant code in dequant shaders (#10798)
* | | | | | | | | | | | | | | | | | | | | dc5301d5 Vulkan: Add VK_EXT_subgroup_size_control support to ensure full subgroups for coopmats (#10721)
* | | | | | | | | | | | | | | | | | | | | 9fdb1243 common : add missing env var for speculative (#10801)
* | | | | | | | | | | | | | | | | | | | | 5555c0c1 docs: update server streaming mode documentation (#9519)
* | | | | | | | | | | | | | | | | | | | |   973f328b Merge pull request #10788 from ggerganov/gg/gguf-py-0.11.0
|\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  
| * | | | | | | | | | | | | | | | | | | | | fb18934a gguf-py : bump version to 0.11.0
|/ / / / / / / / / / / / / / / / / / / / /  
* | | | | | | | | | | | | | | | | | | | | 235f6e14 server : (UI) add tok/s, get rid of completion.js (#10786)
* | | | | | | | | | | | | | | | | | | | | 1a31d0dc Update README.md (#10772)
* | | | | | | | | | | | | | | | | | | | | 92f77a64 ci : pin nodejs to 22.11.0 (#10779)
* | | | | | | | | | | | | | | | | | | | | 484d2f31 bug-fix: snprintf prints NULL in place of the last character (#10419)
* | | | | | | | | | | | | | | | | | | | | 4b4d92b0 docs: fix server documentation formatting (#10776)
* | | | | | | | | | | | | | | | | | | | | 43041d2e ggml: load all backends from a user-provided search path (#10699)
* | | | | | | | | | | | | | | | | | | | | b685daf3 vulkan: request round-to-even for fp16 in im2col/rope_head (#10767)
| | | | | | | | | | | | | | | | | | | | | * 4f3a7e27 Force max subgroup size for coopmat shaders
| | | | | | | | | | | | | | | | | | | | | * 2dc175fb Vulkan: Add VK_EXT_subgroup_size_control support to ensure full subgroups for coopmats
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | dafae66c vulkan: dynamic subgroup size for the remaining k quants (#10745)
* | | | | | | | | | | | | | | | | | | | | ae4b9226 imatrix : Add imatrix to --no-context-shift (#10766)
* | | | | | | | | | | | | | | | | | | | | 750cb3e2 CUDA: rename macros to avoid conflicts with WinAPI (#10736)
* | | | | | | | | | | | | | | | | | | | | a86ad841 server : add flag to disable the web-ui (#10762) (#10751)
* | | | | | | | | | | | | | | | | | | | | a05e2afc vulkan: disable spirv-opt for coopmat shaders (#10763)
* | | | | | | | | | | | | | | | | | | | | 26a8406b CUDA: fix shared memory access condition for mmv (#10740)
* | | | | | | | | | | | | | | | | | | | | c37fb4cf Changes to CMakePresets.json to add ninja clang target on windows (#10668)
* | | | | | | | | | | | | | | | | | | | | 3d98b4cb vulkan: fix compile warnings (#10731)
* | | | | | | | | | | | | | | | | | | | | 1a050047 cmake : simplify msvc charsets (#10672)
* | | | | | | | | | | | | | | | | | | | | ce8784bd server : fix format_infill (#10724)
| | | | | | | | | | | | | | | | | | | | | * b8d1b1a5 server : fix infill prompt format
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | e52522b8 server : bring back info of final chunk in stream mode (#10722)
* | | | | | | | | | | | | | | | | | | | | 06d70147 Vulkan: fix NaN in tanh.comp with AMD proprietary driver on Windows (#10723)
* | | | | | | | | | | | | | | | | | | | | 43ed389a llama : use cmake for swift build (#10525)
* | | | | | | | | | | | | | | | | | | | | ecc93d05 vulkan: compile a test shader in cmake to check for coopmat2 support (#10713)
| | | | | | | | | | | | | | | | | | | | | * a6648b9d server : chunked prefill support
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 62e84d98 llama : add 128k yarn context for Qwen (#10698)
* | | | | | | | | | | | | | | | | | | | | 3573fa8e server : (refactor) no more json in server_task input (#10691)
* | | | | | | | | | | | | | | | | | | | | d9c3ba2b ggml : disable iq4_nl interleave size 8 (#10709)
* | | | | | | | | | | | | | | | | | | | | ce4a7b84 server : various fixes (#10704)
* | | | | | | | | | | | | | | | | | | | | 19d8762a ggml : refactor online repacking (#10446)
* | | | | | | | | | | | | | | | | | | | | c2a16c0b server : fix free of spec context and batch (#10651)
* | | | | | | | | | | | | | | | | | | | | 3df784b3 Vulkan: VK_KHR_cooperative_matrix support to speed up prompt processing (#10597)
* | | | | | | | | | | | | | | | | | | | | 86a19349 metal : Extend how Llama.cpp locates metal resources (#10676)
* | | | | | | | | | | | | | | | | | | | | 784a14aa convert : add support for Roberta embeddings (#10695)
* | | | | | | | | | | | | | | | | | | | | c5ede384 convert : add custom attention mapping
* | | | | | | | | | | | | | | | | | | | | f162d45a common : bring back --no-warmup to server (#10686)
* | | | | | | | | | | | | | | | | | | | | 6c5bc062 server : (refactoring) do not rely on JSON internally (#10643)
* | | | | | | | | | | | | | | | | | | | | 7736837d fix(server) : not show alert when DONE is received (#10674)
* | | | | | | | | | | | | | | | | | | | | c9c6e01d vulkan: Add VK_NV_cooperative_matrix2 support for mul_mat and flash attention (#10206)
* | | | | | | | | | | | | | | | | | | | | 6fe62478 llama : add Minerva 7B model support (#10673)
* | | | | | | | | | | | | | | | | | | | | 0cd182eb sync : ggml
* | | | | | | | | | | | | | | | | | | | | a8cbab20 ggml: add `GGML_SET` Metal kernel + i32 CPU kernel (ggml/1037)
* | | | | | | | | | | | | | | | | | | | | c2082d93 ggml : add `GGML_PAD_REFLECT_1D` operation (ggml/1034)
* | | | | | | | | | | | | | | | | | | | | d405804b py : update outdated copy-paste instructions [no ci] (#10667)
* | | | | | | | | | | | | | | | | | | | | f112d198 Update deprecation-warning.cpp (#10619)
* | | | | | | | | | | | | | | | | | | | | 1da7b765 server : fix speculative decoding with context shift (#10641)
* | | | | | | | | | | | | | | | | | | | | 59f4db10 ggml : add predefined list of CPU backend variants to build (#10626)
* | | | | | | | | | | | | | | | | | | | | 28035408 ggml-cpu : fix HWCAP2_I8MM value (#10646)
* | | | | | | | | | | | | | | | | | | | | 253b7fde Fix HF repo commit to clone lora test models (#10649)
* | | | | | | | | | | | | | | | | | | | | 8d0cfd55 llama: Support MiniCPM-1B (with & w/o longrope) (#10559)
* | | | | | | | | | | | | | | | | | | | | 2759916d vulkan: Implement "fast divide" (mul+shift) for unary ops like copy (#10642)
* | | | | | | | | | | | | | | | | | | | | 40c6d79f SYCL : Move to compile time oneMKL interface backend selection for NVIDIA backend (#10584)
* | | | | | | | | | | | | | | | | | | | | 98036d56 fix typo of README.md (#10605)
* | | | | | | | | | | | | | | | | | | | | cd2f37b3 Avoid using __fp16 on ARM with old nvcc (#10616)
* | | | | | | | | | | | | | | | | | | | | da6aac91 Add docs for creating a static build (#10268) (#10630)
* | | | | | | | | | | | | | | | | | | | | 01e6d9bb clip : add sycl support (#10574)
| | | | | | | | | | | | | | | | | | | | | * a8046c88 use calloc instead of malloc
| | | | | | | | | | | | | | | | | | | | | * 096b847a fix wrong type in print
| | | | | | | | | | | | | | | | | | | | | * b8872700 GGUF: backend support, fixed-width I/O, misc fixes
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | | | * 81611bef server : add tests
| | | | | | | | | | | | | | | | | | | | | * b436edaa server : take into account speculative limits
| | | | | | | | | | | | | | | | | | | | | * a5a915b5 server : fix speculative decoding with context shift
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | cc98896d vulkan: optimize and reenable split_k (#10637)
* | | | | | | | | | | | | | | | | | | | | 91c36c26 server : (web ui) Various improvements, now use vite as bundler (#10599)
* | | | | | | | | | | | | | | | | | | | | 1cd3df46 scripts : remove amx sync
* | | | | | | | | | | | | | | | | | | | | c5054718 sync : ggml
* | | | | | | | | | | | | | | | | | | | | e9e661bd CUDA: remove unnecessary warp reduce in FA (ggml/1032)
* | | | | | | | | | | | | | | | | | | | | efb6ae96 feat: add `GGML_UNARY_OP_ARGMAX` Metal kernel (ggml/1019)
* | | | | | | | | | | | | | | | | | | | | 667d70d1 metal : add `GGML_OP_CONV_TRANSPOSE_1D` kernels (ggml/1026)
* | | | | | | | | | | | | | | | | | | | | 3b4f2e33 llama : add missing LLAMA_API for llama_chat_builtin_templates (#10636)
* | | | | | | | | | | | | | | | | | | | | 82bca225 readme : add option, update default value, fix formatting (#10271)
* | | | | | | | | | | | | | | | | | | | | 0115df2f metal : small-batch mat-mul kernels (#10581)
* | | | | | | | | | | | | | | | | | | | | 515d4e53 github : minify link [no ci] (revert)
* | | | | | | | | | | | | | | | | | | | | 844e2e1f github : minify link [no ci]
* | | | | | | | | | | | | | | | | | | | | 70b98fad server : fix default draft model parameters (#10586)
| | | | | | | | | | | | | | | | | | | | | * 33d7b70c server : do not speculate during prompt processing
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 642330ac llama : add enum for built-in chat templates (#10623)
* | | | | | | | | | | | | | | | | | | | | 8648c521 make : deprecate (#10514)
* | | | | | | | | | | | | | | | | | | | | 64ed2091 server: Add "tokens per second" information in the backend (#10548)
* | | | | | | | | | | | | | | | | | | | | 991f8aab SYCL: Fix and switch to GGML_LOG system instead of fprintf (#10579)
* | | | | | | | | | | | | | | | | | | | | 4cb003dd contrib : refresh (#10593)
* | | | | | | | | | | | | | | | | | | | | 917786f4 Add `mistral-v1`, `mistral-v3`, `mistral-v3-tekken` and `mistral-v7` chat template types (#10572)
* | | | | | | | | | | | | | | | | | | | | 5e1ed955 grammars : add English-only grammar (#10612)
* | | | | | | | | | | | | | | | | | | | | 5c7a5aa0 ci: add error handling for Python venv creation in run.sh (#10608)
* | | | | | | | | | | | | | | | | | | | | 3420909d ggml : automatic selection of best CPU backend (#10606)
* | | | | | | | | | | | | | | | | | | | | 86dc11c5 server : bind to any port when specified (#10590)
* | | | | | | | | | | | | | | | | | | | | 6acce397 readme : update the usage section with examples (#10596)
* | | | | | | | | | | | | | | | | | | | | 43957ef2 build: update Makefile comments for C++ version change (#10598)
* | | | | | | | | | | | | | | | | | | | | 0c39f44d ggml-cpu: replace AArch64 NEON assembly with intrinsics in ggml_gemv_q4_0_4x4_q8_0() (#10567)
* | | | | | | | | | | | | | | | | | | | | 3e0ba0e6 readme : remove old badge
* | | | | | | | | | | | | | | | | | | | | abadba05 readme : refresh (#10587)
* | | | | | | | | | | | | | | | | | | | | 0533e7fb vulkan: Dynamic subgroup size support for Q6_K mat_vec (#10536)
* | | | | | | | | | | | | | | | | | | | | 7cc2d2c8 ggml : move AMX to the CPU backend (#10570)
* | | | | | | | | | | | | | | | | | | | | b782e5c7 server : add more test cases (#10569)
* | | | | | | | | | | | | | | | | | | | | 3a8e9af4 imatrix : support combine-only (#10492)
* | | | | | | | | | | | | | | | | | | | | a3a3048e cleanup UI link list (#10577)
* | | | | | | | | | | | | | | | | | | | | f0678c5f ggml : fix I8MM Q4_1 scaling factor conversion (#10562)
* | | | | | | | | | | | | | | | | | | | | 4b3242bb ggml-cpu: fix typo in gemv/gemm iq4_nl_4_4 (#10580)
* | | | | | | | | | | | | | | | | | | | | 0f77aae5 sycl : offload of get_rows set to 0 (#10432)
* | | | | | | | | | | | | | | | | | | | | 266b8519 sycl : Reroute permuted mul_mats through oneMKL (#10408)
* | | | | | | | | | | | | | | | | | | | | 938f6087 CANN: RoPE operator optimization (#10563)
* | | | | | | | | | | | | | | | | | | | | f095a649 vulkan: get the first command buffer submitted sooner (#10499)
* | | | | | | | | | | | | | | | | | | | | 678d7994 llava: return false instead of exit (#10546)
* | | | | | | | | | | | | | | | | | | | | dc223440 ggml : remove redundant copyright notice + update authors
* | | | | | | | | | | | | | | | | | | | | 4c0a95b1 llama : add missing model types
* | | | | | | | | | | | | | | | | | | | | 6c595676 server : (tests) don't use thread for capturing stdout/stderr, bump openai client library (#10568)
* | | | | | | | | | | | | | | | | | | | | 89071931 common: fix warning message when no GPU found (#10564)
* | | | | | | | | | | | | | | | | | | | | 7281cf13 docs: fix outdated usage of llama-simple (#10565)
* | | | | | | | | | | | | | | | | | | | | e90688ed ci : fix tag name in cuda and hip releases (#10566)
* | | | | | | | | | | | | | | | | | | | | 76b27d29 ggml : fix row condition for i8mm kernels (#10561)
* | | | | | | | | | | | | | | | | | | | | eea986f2 cmake : fix ARM feature detection (#10543)
* | | | | | | | | | | | | | | | | | | | | c202cef1 ggml-cpu: support IQ4_NL_4_4 by runtime repack (#10541)
* | | | | | | | | | | | | | | | | | | | | 2025fa67 kompute : improve backend to pass test_backend_ops (#10542)
* | | | | | | | | | | | | | | | | | | | | c6bc7395 CANN: Update cann.md to display correctly in CLion (#10538)
* | | | | | | | | | | | | | | | | | | | | 605fa66c CANN: Fix SOC_TYPE compile bug (#10519)
* | | | | | | | | | | | | | | | | | | | | b7420131 CANN: ROPE operator optimization (#10540)
* | | | | | | | | | | | | | | | | | | | | 9f912511 common : fix duplicated file name with hf_repo and hf_file (#10550)
* | | | | | | | | | | | | | | | | | | | | 3ad5451f Add some minimal optimizations for CDNA (#10498)
* | | | | | | | | | | | | | | | | | | | | 46c69e0e ci : faster CUDA toolkit installation method and use ccache (#10537)
* | | | | | | | | | | | | | | | | | | | | 9e2301f4 metal : fix group_norm support condition (#0)
* | | | | | | | | | | | | | | | | | | | | fee824a1 sync : ggml
* | | | | | | | | | | | | | | | | | | | | 9150f8fe Do not include arm_neon.h when compiling CUDA code (ggml/1028)
* | | | | | | | | | | | | | | | | | | | | c31ed2ab vulkan: define all quant data structures in types.comp (#10440)
* | | | | | | | | | | | | | | | | | | | | 5b3466be vulkan: Handle GPUs with less shared memory (#10468)
* | | | | | | | | | | | | | | | | | | | | 249a7902 vulkan: further optimize q5_k mul_mat_vec (#10479)
* | | | | | | | | | | | | | | | | | | | | 71a64989 vulkan: skip integer div/mod in get_offsets for batch_idx==0 (#10506)
* | | | | | | | | | | | | | | | | | | | | 4a57d362 vulkan: optimize Q2_K and Q3_K mul_mat_vec (#10459)
* | | | | | | | | | | | | | | | | | | | | c9b00a70 ci : fix cuda releases (#10532)
* | | | | | | | | | | | | | | | | | | | | de509735 Add OLMo 2 model in docs (#10530)
* | | | | | | | | | | | | | | | | | | | | 5a349f28 ci : remove nix workflows (#10526)
* | | | | | | | | | | | | | | | | | | | | 30ec3983 llama : disable warnings for 3rd party sha1 dependency (#10527)
* | | | | | | | | | | | | | | | | | | | | be0e350c Fix HIP flag inconsistency & build docs (#10524)
* | | | | | | | | | | | | | | | | | | | | 249cd93d mtgpu: Add MUSA_DOCKER_ARCH in Dockerfiles && update cmake and make (#10516)
* | | | | | | | | | | | | | | | | | | | | 904109ed vulkan: fix group_norm (#10496)
* | | | | | | | | | | | | | | | | | | | | 45abe0f7 server : replace behave with pytest (#10416)
* | | | | | | | | | | | | | | | | | | | | 0bbd2262 restore the condistion to build & update pacakge when merge (#10507)
* | | | | | | | | | | | | | | | | | | | | ab96610b cmake : enable warnings in llama (#10474)
* | | | | | | | | | | | | | | | | | | | | 7db3846a ci : publish the docker images created during scheduled runs (#10515)
* | | | | | | | | | | | | | | | | | | | | c6807b3f ci : add ubuntu cuda build, build with one arch on windows (#10456)
| | | | | | | | | | | | | | | | | | | | | * 3c8a2a83 shmem experiments
| | | | | | | | | | | | | | | | | | | | | * dafedd33 4x4 -> 4x
| | | | | | | | | | | | | | | | | | | | | * bf349434 metal : some mul_mv experiments
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 25669aa9 ggml-cpu: cmake add arm64 cpu feature check for macos (#10487)
* | | | | | | | | | | | | | | | | | | | | 84e1c33c server : fix parallel speculative decoding (#10513)
* | | | | | | | | | | | | | | | | | | | | 811872a5 speculative : simplify the implementation (#10504)
* | | | | | | | | | | | | | | | | | | | | 9a4b79bc CANN: Improve the Inferencing Performance for Ascend NPU Device (#10454)
* | | | | | | | | | | | | | | | | | | | | 7066b4cc CANN: RoPE and CANCAT operator optimization (#10488)
| | | | | | | | | | | | | | | | | | | | | * b83cae08 speculative : add infill mode
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | | 0eb4e12b vulkan: Fix a vulkan-shaders-gen arugment parsing error (#10484)
* | | | | | | | | | | | | | | | | | | | | 0cc63754 Introduce llama-run (#10291)
* | | | | | | | | | | | | | | | | | | | | 50d5cecb ci : build docker images only once daily (#10503)
* | | | | | | | | | | | | | | | | | | | | 9fd8c268 server : add more information about error (#10455)
* | | | | | | | | | | | | | | | | | | | | 47f931c8 server : enable cache_prompt by default (#10501)
* | | | | | | | | | | | | | | | | | | | | 106964e3 metal : enable mat-vec kernels for bs <= 4 (#10491)
* | | | | | | | | | | | | | | | | | | | | 80acb7b4 Rename Olmo1124 to Olmo2 (#10500)
* | | | | | | | | | | | | | | | | | | | | 10bce045 llama : accept a list of devices to use to offload a model (#10497)
* | | | | | | | | | | | | | | | | | | | | 1f922254 Github: update issue templates [no ci] (#10489)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | a9a678a6 Add download chat feature to server chat (#10481)
* | | | | | | | | | | | | | | | | | | | 9ca2e677 server : add speculative decoding support (#10455)
* | | | | | | | | | | | | | | | | | | | 5931c1f2 ggml : add support for dynamic loading of backends (#10469)
* | | | | | | | | | | | | | | | | | | | f6d12e7d tests : fix compile warning
| | | | | | | | | | | | | | | | | | | | * 4ff0831c metal : use F16 math in mul_mat kernels
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | | b7564411 metal : minor code formatting
* | | | | | | | | | | | | | | | | | | | 5a898779 [SYCL] Fix building Win package for oneAPI 2025.0 update (#10483)
* | | | | | | | | | | | | | | | | | | | d9d54e49 speculative : refactor and add a simpler example (#10362)
| |_|_|_|_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | cce5a900 flake.lock: Update (#10470)
* | | | | | | | | | | | | | | | | | | dc39012c llama : fix op mul check with command-r-plus (#10476)
* | | | | | | | | | | | | | | | | | | 9336db46 convert : XLMRoberta Type Vocab Size (#10458)
* | | | | | | | | | | | | | | | | | | 96fa2c5e fix gguf-py:  Conversion error when multiple licenses are configured (#9807)
* | | | | | | | | | | | | | | | | | | 55ed008b ggml : do not use ARM features not included in the build (#10457)
* | | | | | | | | | | | | | | | | | | 6dfcfef0 ci: Update oneAPI runtime dll packaging (#10428)
* | | | | | | | | | | | | | | | | | | 599b3e0c GitHub: ask for more info in issue templates (#10426)
* | | | | | | | | | | | | | | | | | | c18610b4 CANN: Support Ascend310P to accelerate F32 and F16 Model (#10216)
* | | | | | | | | | | | | | | | | | | a5e47592 cuda : optimize argmax (#10441)
* | | | | | | | | | | | | | | | | | | 1bb30bf2 llama : handle KV shift for recurrent models (#10402)
* | | | | | | | | | | | | | | | | | | 87a533be sync : ggml
* | | | | | | | | | | | | | | | | | | 59b91728 ggml/sched : do not skip views in pre-assignments
* | | | | | | | | | | | | | | | | | | 02e4eaf2 ggml-opt: fix data corruption (ggml/1022)
* | | | | | | | | | | | | | | | | | | 9abe9eea vulkan: predicate max operation in soft_max shaders/soft_max (#10437)
* | | | | | | | | | | | | | | | | | | f95caa79 cmake: add link dependencies to cmake find pkg (#10433)
* | | | | | | | | | | | | | | | | | | fab5d30f llama : add .clang-format file (#10415)
* | | | | | | | | | | | | | | | | | | 8fd4b7fa vulkan: copy iq4_nl LUT into shared memory (#10409)
* | | | | | | | | | | | | | | | | | | 1bacb9f6 vulkan: further optimize mul_mat_vec using larger loads (#10387)
* | | | | | | | | | | | | | | | | | | ad21c9e1 update rel to 4040 (#10395)
* | | | | | | | | | | | | | | | | | | 3952a221 Fix missing file renames in Makefile due to changes in commit ae8de6d50a (#10413)
* | | | | | | | | | | | | | | | | | | 42ae10bb add cmake rvv support (#10411)
* | | | | | | | | | | | | | | | | | | 9fe0fb06 sync : ggml
* | | | | | | | | | | | | | | | | | | 611fabd7 metal : fox offset integer overflows in im2col (ggml/1015)
* | | | | | | | | | | | | | | | | | | 12b0ad95 metal : add `GGML_UNARY_OP_ELU` kernel (ggml/1018)
* | | | | | | | | | | | | | | | | | | 342397dc cmake: force MSVC compiler charset to utf-8 (#9989)
* | | | | | | | | | | | | | | | | | | 2a11b6b0 Add required ggml-base and backend libs to cmake pkg (#10407)
* | | | | | | | | | | | | | | | | | | 3ee6382d cuda : fix CUDA_FLAGS not being applied (#10403)
* | | | | | | | | | | | | | | | | | | 8e752a77 llama : add check for KV cache shifts (#10401)
* | | | | | | | | | | | | | | | | | | a88ad007 llama : add OLMo November 2024 support (#10394)
* | | | | | | | | | | | | | | | | | | 2a1507c1 sycl : Add option to set the SYCL architecture for all targets (#10266)
* | | | | | | | | | | | | | | | | | | b3e58598 vulkan: Optimize soft_max (#10301)
* | | | | | | | | | | | | | | | | | | 557924f2 sycl: Revert MUL_MAT_OP support changes (#10385)
* | | | | | | | | | | | | | | | | | | d3481e63 cuda : only use native when supported by cmake (#10389)
* | | | | | | | | | | | | | | | | | | 531cb1c2 Skip searching root path for cross-compile builds (#10383)
* | | | | | | | | | | | | | | | | | | f139d2ea vulkan: remove use of null initializer (#10372)
* | | | | | | | | | | | | | | | | | | 2eb76b2a flake.lock: Update (#10346)
* | | | | | | | | | | | | | | | | | | 9b75f03c Vulkan: Fix device info output format specifiers (#10366)
* | | | | | | | | | | | | | | | | | | 75207b3a docker: use GGML_NATIVE=OFF (#10368)
* | | | | | | | | | | | | | | | | | | 76e9e58b CUDA: fix MMV kernel being used for FP16 src1 (#10357)
* | | | | | | | | | | | | | | | | | | ce2e59ba CMake: fix typo in comment [no ci] (#10360)
* | | | | | | | | | | | | | | | | | | be5cacce llama : only use default buffer types for the KV cache (#10358)
* | | | | | | | | | | | | | | | | | | 20a780c7 gitignore : ignore local run scripts [no ci]
* | | | | | | | | | | | | | | | | | | cf32a9b9 metal : refactor kernel args into structs (#10238)
* | | | | | | | | | | | | | | | | | | a4317829 ggml : fix undefined reference to 'getcpu' (#10354)
* | | | | | | | | | | | | | | | | | | c3ea58ac CUDA: remove DMMV, consolidate F16 mult mat vec (#10318)
* | | | | | | | | | | | | | | | | | | 467576b6 CMake: default to -arch=native for CUDA build (#10320)
* | | | | | | | | | | | | | | | | | | eda7e1d4 ggml : fix possible buffer use after free in sched reserve (#9930)
* | | | | | | | | | | | | | | | | | | 24203e9d ggml : inttypes.h -> cinttypes (#0)
* | | | | | | | | | | | | | | | | | | 5d9e5997 ggml : adapt AMX to tensor->grad removal (#0)
* | | | | | | | | | | | | | | | | | | a4200caf make : add ggml-opt (#0)
* | | | | | | | | | | | | | | | | | | 84274a10 tests : remove test-grad0
* | | | | | | | | | | | | | | | | | | 68fcb475 ggml : fix compile warnings (#0)
* | | | | | | | | | | | | | | | | | | 8a43e940 ggml: new optimization interface (ggml/988)
* | | | | | | | | | | | | | | | | | | 5c9a8b22 scripts : update sync
* | | | | | | | | | | | | | | | | | | 0fff7fd7 docs : vulkan build instructions to use git bash mingw64 (#10303)
* | | | | | | | | | | | | | | | | | | 4e54be0e llama/ex: remove --logdir argument (#10339)
* | | | | | | | | | | | | | | | | | | db4cfd5d llamafile : fix include path (#0)
* | | | | | | | | | | | | | | | | | | 8ee0d09a make : auto-determine dependencies (#0)
* | | | | | | | | | | | | | | | | | | bcdb7a23 server: (web UI) Add samplers sequence customization (#10255)
| | | | | | | | | | | | | | | | | | | * f7b0233e wip
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | f245cc28 scripts : fix missing key in compare-llama-bench.py (#10332)
* | | | | | | | | | | | | | | | | | | 772703c8 vulkan: Optimize some mat-vec mul quant shaders (#10296)
* | | | | | | | | | | | | | | | | | | dd3a6ce9 vulkan : add cmake preset debug/release (#10306)
* | | | | | | | | | | | | | | | | | | 1e58ee13 ggml : optimize Q4_0 into Q4_0_X_Y repack (#10324)
* | | | | | | | | | | | | | | | | | | 89e4caaa llama : save number of parameters and the size in llama_model (#10286)
* | | | | | | | | | | | | | | | | | | 74d73dc8 Make updates to fix issues with clang-cl builds while using AVX512 flags (#10314)
* | | | | | | | | | | | | | | | | | | 4047be74 scripts: update compare-llama-bench.py (#10319)
* | | | | | | | | | | | | | | | | | | 883d206f ggml : fix some build issues
* | | | | | | | | | | | | | | | | | | 09ecbcb5 cmake : fix ppc64 check (whisper/0)
* | | | | | | | | | | | | | | | | | | 32250089 ggml : vulkan logs (whisper/2547)
* | | | | | | | | | | | | | | | | | | cbf5541a sync : ggml
* | | | | | | | | | | | | | | | | | | 18429220 AVX BF16 and single scale quant optimizations (#10212)
* | | | | | | | | | | | | | | | | | | f0204a0e ci: build test musa with cmake (#10298)
* | | | | | | | | | | | | | | | | | | 57f8355b sycl: Update Intel docker images to use DPC++ 2025.0 (#10305)
* | | | | | | | | | | | | | | | | | | 9901068a server : (web UI) add copy button for code block, fix api key (#10242)
* | | | | | | | | | | | | | | | | | | 231f9360 cann: dockerfile and doc adjustment (#10302)
* | | | | | | | | | | | | | | | | | | 4802ad35 scripts : fix regex in sync [no ci]
* | | | | | | | | | | | | | | | | | | 5a54af4d sycl: Use syclcompat::dp4a (#10267)
* | | | | | | | | | | | | | | | | | | 1607a5e5 backend cpu: add online flow for aarch64 Q4_0 GEMV/GEMM kernels (#9921)
* | | | | | | | | | | | | | | | | | | ae8de6d5 ggml : build backends as libraries (#10256)
* | | | | | | | | | | | | | | | | | | 4a8ccb37 CUDA: no -sm row for very small matrices (#10185)
* | | | | | | | | | | | | | | | | | | 2a82891a speculative : fix out-of-bounds access (#10289)
| | | | | | | | | | | | | | | | | | | * 5e6dad93 speculative : experimenting with Qwen2.5
| | | | | | | | | | | | | | | | | | | * 33bdee66 speculative : fix out-of-bounds access
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | af148c93 vulkan: Optimize binary ops (#10270)
* | | | | | | | | | | | | | | | | | | 66798e42 vulkan: Use macros to make the mat mul pipeline creation more concise (#10259)
* | | | | | | | | | | | | | | | | | | fb4a0ec0 llama : propagate the results of `graph_compute` (#9525)
* | | | | | | | | | | | | | | | | | | 5ea926da sync : ggml
* | | | | | | | | | | | | | | | | | | 1ee9eea0 docs : update bindings list (#10261)
* | | | | | | | | | | | | | | | | | | ff7fb670 server : add missing docs (#10269)
* | | | | | | | | | | | | | | | | | | 0e712a5a server : fix incorrect res in validate_model_chat_template (#10272)
* | | | | | | | | | | | | | | | | | | a0ec17b3 metadata: Detailed Dataset Authorship Metadata (#8875)
* | | | | | | | | | | | | | | | | | | 2e82ffa4 sycl : Fixes to broken builds and test-backend-ops (#10257)
* | | | | | | | | | | | | | | | | | | 80dd7ff2 vulkan: Optimize contiguous copies (#10254)
| | | | | | | | | | | | | | | | | | | * 8c1b186c metal : minor Q4_0 optimization
| | | | | | | | | | | | | | | | | | | * 86ed72d2 ggml : add ggml-metal-impl.h
| | | | | | | | | | | | | | | | | | | * 63bab93c metal : add TODOs for rest of ops
| | | | | | | | | | | | | | | | | | | * 964206a7 metal : GGML_OP_NORM
| | | | | | | | | | | | | | | | | | | * e9ecd5d4 metal : GGML_OP_RMS_NORM
| | | | | | | | | | | | | | | | | | | * 647a7044 metal : GGML_OP_CPY
| | | | | | | | | | | | | | | | | | | * f46f710c metal : GGML_OP_REPEAT
| | | | | | | | | | | | | | | | | | | * 3250c98b metal : GGML_OP_ADD, GGML_OP_SUB, GGML_OP_MUL, GGML_OP_DIV
| | | | | | | | | | | | | | | | | | | * 9058c51d metal : GGML_OP_CONCAT
| | | | | | | | | | | | | | | | | | | * bb821e48 cont : int safety + register optimizations
| | | | | | | | | | | | | | | | | | | * c5cf1d74 cont : mul mm id
| | | | | | | | | | | | | | | | | | | * 15a71059 cont : thread counters style
| | | | | | | | | | | | | | | | | | | * cacc4c22 cont : shmem style
| | | | | | | | | | | | | | | | | | | * a1a201c1 cont : use char ptr
| | | | | | | | | | | | | | | | | | | * c81640a5 cont : args is first argument
| | | | | | | | | | | | | | | | | | | * b65e4c1e cont : pass by reference
| | | | | | | | | | | | | | | | | | | * c59a13d9 cont : mul mat vec
| | | | | | | | | | | | | | | | | | | * 7670809a metal : mul mat struct (wip)
| | | | | | | | | | | | | | | | | | | * 4fd6fc5a metal : cont + avoid potential int overflow [no ci]
| | | | | | | | | | | | | | | | | | | * 9e07bcc0 metal : fattn args
| | | | | | | | | | | | | | | | | | | * 1198ae77 metal : add kernel arg structs (wip)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | 54ef9cfc vulkan: Throttle the number of shader compiles during the build step. (#10222)
* | | | | | | | | | | | | | | | | | | b0cefea5 metal : more precise Q*K in FA vec kernel (#10247)
* | | | | | | | | | | | | | | | | | | b141e5f6 server : enable KV cache defrag by default (#10233)
* | | | | | | | | | | | | | | | | | | 4b3a9212 flake.lock: Update (#10243)
* | | | | | | | | | | | | | | | | | | 505f3327 server : (web UI) Add back sampler settings (#10239)
* | | | | | | | | | | | | | | | | | | 160687b3 vulkan: Fix newly added tests for permuted mul_mat and 1D im2col (#10226)
* | | | | | | | | | | | | | | | | | | 6423c65a metal : reorder write loop in mul mat kernel + style (#10231)
* | | | | | | | | | | | | | | | | | | 39a334a9 metal : fix build and some more comments (#10229)
* | | | | | | | | | | | | | | | | | | bb38cdd8 metal : fix F32 accumulation in FA vec kernel (#10232)
* | | | | | | | | | | | | | | | | | | f018acba llama : fix Qwen model type strings
* | | | | | | | | | | | | | | | | | | 46323fa9 metal : hide debug messages from normal log
| | | | | | | | | | | | | | | | | | | * 3d1fe1bb metal : int -> short, style
| | | | | | | | | | | | | | | | | | | * 53505057 metal : reorder write loop
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | | | * bd1198a6 metal : fix build and some more comments
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | 5b359bb1 ggml: fix zero division in ‘dne’ calculation in CUDA COUNT_EQUAL operator when ‘ne’ is small (#10213)
* | | | | | | | | | | | | | | | | | | e8921349 ggml : optimize llamafile cpu matrix multiplication for ppc64le (#10156)
* | | | | | | | | | | | | | | | | | | 8fc393f2 scripts : fix pattern and get n_tokens in one go (#10221)
* | | | | | | | | | | | | | | | | | | ec450d3b metal : opt-in compile flag for BF16 (#10218)
* | | | | | | | | | | | | | | | | | | 695ad752 metal : improve clarity (minor) (#10171)
* | | | | | | | | | | | | | | | | | | 841f27ab metal : optimize FA kernels (#10171)
* | | | | | | | | | | | | | | | | | | d05b3127 swift : exclude ggml-metal-embed.metal (#10211)
| | | | | | | | | | | | | | | | | | | * a2385da5 make : clean-up [no ci]
| | | | | | | | | | | | | | | | | | | * b89e71b1 metal : fix BF16 requirement for FA kernels
| | | | | | | | | | | | | | | | | | | * bc143ecf cuda : disable BF16 FA
| | | | | | | | | | | | | | | | | | | * 5d1a10d2 metal : prevent int overflows [no ci]
| | | | | | | | | | | | | | | | | | | * 486a5eb8 build : remove obsolete compile flag [no ci]
| | | | | | | | | | | | | | | | | | | * 120d5128 metal : compile-guard bf16 FA kernels
| | | | | | | | | | | | | | | | | | | * 2fccc8ac metal : minor clean-up
| | | | | | | | | | | | | | | | | | | * 7facc29d metal : use F16 precision in FA kernels
| | | | | | | | | | | | | | | | | | | * 25e87730 ggml : add ggml_flash_attn_ext_get_prec
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | 76c6e7f1 server : minor UI fix (#10207)
* | | | | | | | | | | | | | | | | | | a71d81cf server : revamp chat UI with vuejs and daisyui (#10175)
* | | | | | | | | | | | | | | | | | | eec4d717 scripts : add amx to sync-ggml.sh [no ci]
* | | | | | | | | | | | | | | | | | | 3b088286 sync : ggml
* | | | | | | | | | | | | | | | | | | a2c6fd74 scripts : sync update
* | | | | | | | | | | | | | | | | | | 97404c4a ggml : add ggml-cpu.h to the public headers (#10204)
* | | | | | | | | | | | | | | | | | | 60e17ce2 Remove identical wte/etw logic for jais (#10203)
* | | | | | | | | | | | | | | | | | | 5107e8ce DRY: Fixes clone functionality (#10192)
* | | | | | | | | | | | | | | | | | | 2319126a fix q4_0_8_8 format for corrupted tokens issue (#10198)
* | | | | | | | | | | | | | | | | | | 3bcd40b3 Optimize RWKV6 Operator Naming and Implement Multi-core CPU/ SYCL Acceleration (#10133)
| | | | | | | | | | | | | | | | | | | * 94accca4 vec move mask to shmem
| | | | | | | | | | | | | | | | | | | * 3b962503 f16 vec
| | | | | | | | | | | | | | | | | | | * 8f0ef152 clean-up
| | | | | | | | | | | | | | | | | | | * 022e5e90 remove compile flag
| | | | | | | | | | | | | | | | | | | * a6c8dbfa wip
| | | | | | | | | | | | | | | | | | | * 4abeb60a int64 dst
| | | | | | | | | | | | | | | | | | | * 3ab47eb7 float -> half regs
| | | | | | | | | | | | | | | | | | | * e121d82f 64-bit -> 32-bit
| | | | | | | | | | | | | | | | | | | * a75cdcca remove inner if mask
| | | | | | | | | | | | | | | | | | | * 61d05b57 remove ms array
| | | | | | | | | | | | | | | | | | | * 98492810 move mask to shared mem
| | | | | | | | | | | | | | | | | | | * 2aedbb35 wip 5
| | | | | | | | | | | | | | | | | | | * dc2a27f2 wip 4
| | | | | | | | | | | | | | | | | | | * 9bd5ae09 wip 3
| | | | | | | | | | | | | | | | | | | * 2335086f wip2
| | | | | | | | | | | | | | | | | | | * 01c7f112 wip
| | | | | | | | | | | | | | | | | | | * 0f7e8f38 metal : add GGML_METAL_FORCE_FATTN_PREC_F16
| | | | | | | | | | | | | | | | | | | * eefc132b metal : use F16 precision in FA kernel
| | | | | | | | | | | | | | | | | | | * 22a9311a ggml : add ggml_flash_attn_ext_get_prec
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | 5c333e01 metal : add BF16 support (#8439)
* | | | | | | | | | | | | | | | | | | b11f9ba9 server : remove hack for extra parallel slot (#10187)
* | | | | | | | | | | | | | | | | | | 94d8cb8b metal : fix from ptr buffer name (#10189)
* | | | | | | | | | | | | | | | | | | 1dc04b2d ggml : adjust is_first_call init value (#10193)
* | | | | | | | | | | | | | | | | | | a1eaf6a9 metal : add quantized FA support (#10149)
* | | | | | | | | | | | | | | | | | | b8deef0e llama : add <|tool_call|> formatting to Granite template (#10177)
* | | | | | | | | | | | | | | | | | | a9e8a9a0 ggml : fix arch check in bf16_to_fp32 (#10164)
* | | | | | | | | | | | | | | | | | | 34073647 Q6_K AVX improvements (#10118)
* | | | | | | | | | | | | | | | | | | d5a409e5 ggml : fix gelu tables initialization (#10172)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 401558b7 ggml : fix q4xx mat mul, increase ggml_aligned_malloc alignment (#10167)
* | | | | | | | | | | | | | | | | | 9e0ecfb6 server : clarify /slots endpoint, add is_processing (#10162)
* | | | | | | | | | | | | | | | | | 6a066b99 fix build break on arm64 linux (#10166)
* | | | | | | | | | | | | | | | | | ea02c753 cuda : clear error after changing peer access (#10153)
* | | | | | | | | | | | | | | | | | 05697f67 metal : simplify f16 and f32 dequant kernels (#0)
* | | | | | | | | | | | | | | | | | f8e58135 metal : move dequantize templates to beginning of MSL source (#0)
* | | | | | | | | | | | | | | | | | 329ed914 CANN: adjust backend registry refactor. (#10158)
* | | | | | | | | | | | | | | | | | ce027adf sync : ggml
* | | | | | | | | | | | | | | | | | 284e5b02 cmake : make it possible linking ggml as external lib (ggml/1003)
* | | | | | | | | | | | | | | | | | e2292aaa metal : fix minor string leaks (ggml/1004)
* | | | | | | | | | | | | | | | | | 9f409893 ggml : move CPU backend to a separate file (#10144)
* | | | | | | | | | | | | | | | | | 08828a6d metal : minor fixup in FA kernel (#10143)
* | | | | | | | | | | | | | | | | | 1839f691 flake.lock: Update (#10146)
* | | | | | | | | | | | | | | | | | 9830b692 Add apple arm to presets (#10134)
* | | | | | | | | | | | | | | | | | 42cadc74 server : fix slot selection by lru (#10126)
* | | | | | | | | | | | | | | | | | 45950415 server : fix endpoint checks (#10135)
| | | | | | | | | | | | | | | | | | * c5d8bb5a leave only basic functions for SYCL CI
| | | | | | | | | | | | | | | | | | * c263ca76 remove wrong assert in norm WA for permute(0,1,3,2) mul_mat ggml-ci
| | | | | | | | | | | | | | | | | | | * 4fc8673d llama-bench : skip repeated values in consecutive lines
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | 1926d6e3 llama : adjust default context size + print warnings (#10136)
* | | | | | | | | | | | | | | | | | | b634f8a2 simple-chat : only add bos on first prompt (#10129)
* | | | | | | | | | | | | | | | | | | 7554aa46 convert-lora : make `--base` optional (#10110)
| | | | | | | | | | | | | | | | | | | * 20e12112 llama : suggest reduce ctx size when kv init fails
| | | | | | | | | | | | | | | | | | | * bf60f27c ggml : do not abort when ggml_aligned_malloc fails
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | a6744e43 llama : add simple-chat example (#10124)
* | | | | | | | | | | | | | | | | | | e991e312 llama : use smart pointers for ggml resources (#10117)
* | | | | | | | | | | | | | | | | | | 418f5eef vulkan : improve ggml_vk_create_buffer error handling (#9898)
* | | | | | | | | | | | | | | | | | | ba6f62eb readme : update hot topics
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | d865d147 server : fix smart selection of available slot (#10120)
* | | | | | | | | | | | | | | | | | 1804adb0 ggml : remove ggml_scratch (#10121)
* | | | | | | | | | | | | | | | | | 815fe72a sync : ggml
* | | | | | | | | | | | | | | | | | f221d562 ggml : alloc ggml_contexts on the heap (whisper/2525)
* | | | | | | | | | | | | | | | | | e597e507 build: fix build error in Windows env with OneAPI setup (#10107)
* | | | | | | | | | | | | | | | | | 85679d37 llama : improve output buffer type selection (#10098)
* | | | | | | | | | | | | | | | | | 1e9f9499 quantize : fix --keep-split (#10114)
* | | | | | | | | | | | | | | | | | c02e5ab2 llama : fix buffer checks for mamba and rwk (#10111)
* | | | | | | | | | | | | | | | | | ab3d71f9 loader:  refactor tensor weights storage (#9935)
* | | | | | | | | | | | | | | | | | 0a683e80 server : include scheme when printing URL (#10106)
* | | | | | | | | | | | | | | | | | dea5e860 ggml : check tensor name lengths in gguf files (#10100)
* | | | | | | | | | | | | | | | | | 1329c0a7 kompute: add mul_mat_q4_k shader (#10097)
* | | | | | | | | | | | | | | | | | 61408e7f kompute: add backend registry / device interfaces (#10045)
| | | | | | | | | | | | | | | | | | * afc4a7de llama : enable flash attn automatically when supported
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | b9e02e81 ggml : fix memory leaks when loading invalid gguf files (#10094)
* | | | | | | | | | | | | | | | | | 6763f713 readme : more lora detail in main example readme (#10064)
* | | | | | | | | | | | | | | | | | 79a2bc04 convert : more detailed convert lora usage docs (#10065)
* | | | | | | | | | | | | | | | | | fc83a9e5 ggml : add Q4_0_8_8 RISC-V GEMV and GEMM kernels (#10029)
* | | | | | | | | | | | | | | | | | c5b0f4b5 llama : refactor model loader with backend registry (#10026)
* | | | | | | | | | | | | | | | | | 8f275a7c ggml: Add POOL2D OP for GPU acceleration to the Vulkan backend in the MobileVLM model. (#9763)
* | | | | | | | | | | | | | | | | | 8d8ff715 llama : remove Tail-Free sampling (#10071)
* | | | | | | | | | | | | | | | | | 61715d5c llama : Add IBM granite template (#10013)
* | | | | | | | | | | | | | | | | | 07028f9d flake.lock: Update (#10063)
* | | | | | | | | | | | | | | | | | 524afeec musa: workaround for Guilty Lockup in cleaning src0 (#10042)
* | | | | | | | | | | | | | | | | | 8125e6cb server : don't overfill the batch during infill (#10018)
* | | | | | | | | | | | | | | | | | 8841ce3f llama : switch KQ multiplication to F32 precision by default (#10015)
* | | | | | | | | | | | | | | | | | cc2983d3 sync : ggml
* | | | | | | | | | | | | | | | | | 8c60a8a4 increase cuda_cpy block size (ggml/996)
* | | | | | | | | | | | | | | | | | 9e4a2563 scripts : fix amx sync [no ci]
* | | | | | | | | | | | | | | | | | 66875035 metal : support permuted matrix multiplicaions (#10033)
* | | | | | | | | | | | | | | | | | ff252ea4 llama : add DRY sampler (#9702)
* | | | | | | | | | | | | | | | | | d80fb71f llama: string_split fix (#10022)
* | | | | | | | | | | | | | | | | | 2f8bd2b9 llamafile : extend sgemm.cpp support for Q5_0 models (#10010)
* | | | | | | | | | | | | | | | | | bc5ba007 server : check that the prompt fits in the slot's context (#10030)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 958367bf server : refactor slot input data, move tokenizer to HTTP thread (#10023)
* | | | | | | | | | | | | | | | | 40f25557 ci : fix cmake flags for SYCL
* | | | | | | | | | | | | | | | | 167a5156 CUDA: fix insufficient buffer clearing for MMQ (#10032)
* | | | | | | | | | | | | | | | | c39665f5 CUDA: fix MMQ for non-contiguous src0, add tests (#10021)
* | | | | | | | | | | | | | | | | 0a1c750c server : samplers accept the prompt correctly (#10019)
* | | | | | | | | | | | | | | | | 190a37d7 sync : ggml
* | | | | | | | | | | | | | | | | 2d3aba9e llama.vim : bump generation time limit to 3s [no ci]
* | | | | | | | | | | | | | | | | 80273a30 CUDA: fix 1D im2col, add tests (ggml/993)
* | | | | | | | | | | | | | | | | c19af0ac ggml : remove redundant set of contexts used field (ggml/978)
* | | | | | | | | | | | | | | | | ac113a0f llama.vim : add classic vim support (#9995)
* | | | | | | | | | | | | | | | | 4c9388fb metal : add POOL2D and fix IM2COL (#9943)
* | | | | | | | | | | | | | | | | 873279b1 flake.lock: Update
* | | | | | | | | | | | | | | | | c8c07d65 llama : fix empty batch causing llama_batch_allocr to crash (#9966)
* | | | | | | | | | | | | | | | | 19d900a7 llama : rename batch to ubatch (#9950)
* | | | | | | | | | | | | | | | | 11d47057 Rwkv chat template fix (#10001)
* | | | | | | | | | | | | | | | | c421ac07 lora : warn user if new token is added in the adapter (#9948)
* | | | | | | | | | | | | | | | | 4ff7fe1f llama : add chat template for RWKV-World + fix EOT (#9968)
* | | | | | | | | | | | | | | | | 6b844735 [CANN] Adapt to dynamically loadable backends mechanism (#9970)
* | | | | | | | | | | | | | | | | 674804a9 arg : fix typo in embeddings argument help [no ci] (#9994)
* | | | | | | | | | | | | | | | | e94a138d llama.vim : fix info text display [no ci] (#9787)
* | | | | | | | | | | | | | | | | e01c67af llama.vim : move info to the right of screen [no ci] (#9787)
* | | | | | | | | | | | | | | | | 994cfb1a readme : update UI list (#9972)
* | | | | | | | | | | | | | | | | 94008cc7 arg : fix attention non-causal arg value hint (#9985)
* | | | | | | | | | | | | | | | | dbd5f2f5 llama.vim : plugin for Neovim (#9787)
* | | | | | | | | | | | | | | | | f594bc80 ggml : add asserts for type conversion in fattn kernels (#9971)
* | | | | | | | | | | | | | | | | d5ebd79c rpc : pack only RPC structs (#9959)
* | | | | | | | | | | | | | | | | 55e47786 llama : default sampling changes + greedy update (#9897)
* | | | | | | | | | | | | | | | | bc219750 speculative : fix handling of some input params (#9963)
* | | | | | | | | | | | | | | | | 1db8c84f fix mul_mat_vec_q and *_vec_q error (#9939)
* | | | | | | | | | | | | | | | | 45f09764 readme : update bindings list (#9951)
* | | | | | | | | | | | | | | | | 7cab2083 readme : update infra list (#9942)
* | | | | | | | | | | | | | | | | cda0e4b6 llama : remove all_pos_0, all_pos_1, all_seq_id from llama_batch (#9745)
| | | | | | | | | | | | | | | | | * 8233009d Support SYCL device register
| | | | | | | | | | | | | | | | | | * bc82fc2e llama-bench : add time-to-first-byte stat
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | afd9909a rpc : backend refactoring (#9912)
* | | | | | | | | | | | | | | | | | 87421a23 [SYCL] Add SYCL Backend registry, device and Event Interfaces (#9705)
* | | | | | | | | | | | | | | | | | 60ce97c9 add amx kernel for gemm (#8998)
* | | | | | | | | | | | | | | | | | 8901755b server : add n_indent parameter for line indentation requirement (#9929)
* | | | | | | | | | | | | | | | | | 6f55bccb llama : rename batch_all to batch (#8881)
* | | | | | | | | | | | | | | | | | 17bb9280 readme : remove --memory-f32 references (#9925)
* | | | | | | | | | | | | | | | | | 9f45fc1e llama : change warning to debug log
* | | | | | | | | | | | | | | | | | 99bd4ac2 llama : infill sampling handle very long tokens (#9924)
| | | | | | | | | | | | | | | | | | * 2d3fc54a add amx kernel for gemm
| | | | | | | | | | | | | | | | | | | * 630bce5a ggml : fix possible buffer use after free in sched reserve
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | 3752217e readme : update bindings list (#9918)
* | | | | | | | | | | | | | | | | | | f010b77a vulkan : add backend registry / device interfaces (#9721)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 21942002 fix: allocating CPU buffer with size `0` (#9917)
* | | | | | | | | | | | | | | | | | 73afe681 fix: use `vm_allocate` to allocate CPU backend buffer on macOS (#9875)
* | | | | | | | | | | | | | | | | | 9e041024 llama : suppress conversion from 'size_t' to 'int' (#9046)
* | | | | | | | | | | | | | | | | | dbf18e4d llava : fix typo in error message [no ci] (#9884)
* | | | | | | | | | | | | | | | | | 66c2c930 grammar : fix JSON Schema for string regex with top-level alt. (#9903)
* | | | | | | | | | | | | | | | | | 10433e8b llama : add tensor name for "result_norm" (#9907)
* | | | | | | | | | | | | | | | | | 1f66b699 server : fix the disappearance of the end of the text (#9867)
* | | | | | | | | | | | | | | | | | 0e41b300 sync : ggml
* | | | | | | | | | | | | | | | | | cd60b88b ggml-alloc : remove buffer_id from leaf_alloc (ggml/987)
* | | | | | | | | | | | | | | | | | becfd387 [CANN] Fix cann compilation error (#9891)
* | | | | | | | | | | | | | | | | | 755a9b2b llama : add infill sampler (#9896)
* | | | | | | | | | | | | | | | | | 223c25a7 server : improve infill context reuse (#9894)
* | | | | | | | | | | | | | | | | | fbc98b74 sampling : add XTC sampler (#9742)
* | | | | | | | | | | | | | | | | | dcdd5353 server : update preact (#9895)
* | | | | | | | | | | | | | | | | | 4c42f93b readme : update bindings list (#9889)
* | | | | | | | | | | | | | | | | | a89f75e1 server : handle "logprobs" field with false value (#9871)
* | | | | | | | | | | | | | | | | | 13dca2a5 Vectorize load instructions in dmmv f16 CUDA kernel (#9816)
* | | | | | | | | | | | | | | | | | d4c19c0f server : accept extra_context for the infill endpoint (#9874)
* | | | | | | | | | | | | | | | | | c7181bd2 server : reuse cached context chunks (#9866)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 92be9f12 flake.lock: Update (#9870)
| |_|_|_|_|_|_|_|_|_|_|_|/ / / /  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | edc26566 server : add option to time limit the generation phase (#9865)
* | | | | | | | | | | | | | | | 1bde94dd server : remove self-extend features (#9860)
* | | | | | | | | | | | | | | | 95c76e8e server : remove legacy system_prompt feature (#9857)
* | | | | | | | | | | | | | | | 11ac9800 llama : improve infill support and special token detection (#9798)
* | | | | | | | | | | | | | | | 943d20b4 musa : update doc (#9856)
* | | | | | | | | | | | | | | | 96776405 ggml : move more prints to the ggml log system (#9839)
| | | | | | | | | | | | | | | | * 17b3a3e8 llama : minor llama_grammar refactoring
| | | | | | | | | | | | | | | | * 2aa6dd27 add stacks cache into llama_grammar
| | | | | | | | | | | | | | | | * 901a3479 move cache stack to advance stack
| | | | | | | | | | | | | | | | * cb1632b5 llama : adds llama-grammar memorization stacks (#4218)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 7eee341b common : use common_ prefix for common library functions (#9805)
* | | | | | | | | | | | | | | | 0e9f760e rpc : add backend registry / device interfaces (#9812)
* | | | | | | | | | | | | | | | cf8e0a3b musa: add docker image support (#9685)
* | | | | | | | | | | | | | | | c7499c55 examples : do not use common library in simple example (#9803)
* | | | | | | | | | | | | | | | c81f3bbb cmake : do not build common library by default when standalone (#9804)
* | | | | | | | | | | | | | | | e7022064 perplexity : fix integer overflow (#9783)
* | | | | | | | | | | | | | | | 3dc48fe7 examples : remove llama.vim
* | | | | | | | | | | | | | | | dca1d4b5 ggml : fix BLAS with unsupported types (#9775)
* | | | | | | | | | | | | | | | 458367a9 server : better security control for public deployments (#9776)
* | | | | | | | | | | | | | | | fa42aa6d scripts : fix spelling typo in messages and comments (#9782)
* | | | | | | | | | | | | | | | 63747437 ggml : add backend registry / device interfaces to BLAS backend (#9752)
* | | | | | | | | | | | | | | | f1af42fa Update building for Android (#9672)
* | | | | | | | | | | | | | | | 6279dac0 flake.lock: Update (#9753)
* | | | | | | | | | | | | | | | d5ac8cf2 ggml : add metal backend registry / device (#9713)
* | | | | | | | | | | | | | | | 96b69121 metal : single allocation of encode_async block (#9747)
* | | | | | | | | | | | | | | | d5cb8684 contrib : simplify + minor edits [no ci]
* | | | | | | | | | | | | | | | f4b2dcdf readme : fix typo [no ci]
* | | | | | | | | | | | | | | | b6d6c528 sync : llama.cpp
* | | | | | | | | | | | | | | | b0915d5b vulkan : retry allocation with fallback flags (whisper/2451)
* | | | | | | | | | | | | | | | 8c475b97 rerank : use [SEP] token instead of [BOS] (#9737)
* | | | | | | | | | | | | | | | 58b16695 sync : ggml
* | | | | | | | | | | | | | | | 905f5485 metal : zero-init buffer contexts (whisper/0)
* | | | | | | | | | | | | | | | 71967c2a Add Llama Assistant (#9744)
* | | | | | | | | | | | | | | | 17880771 sync : ggml
* | | | | | | | | | | | | | | | 55951c01 ggml : fix typo in example usage ggml_gallocr_new (ggml/984)
* | | | | | | | | | | | | | | | ff565769 ggml : fixes after sync (ggml/983)
* | | | | | | | | | | | | | | | f3fdcfaa ci : fine-grant permission (#9710)
* | | | | | | | | | | | | | | | 133c7b46 Fixed RNG seed docs (#9723)
* | | | | | | | | | | | | | | | d5ed2b92 metal : remove abort (skip) (ggml/0)
* | | | | | | | | | | | | | | | 1bb8a64e sync : ggml
* | | | | | | | | | | | | | | | fabdc3bd ggml/ex: calculate accuracy in graph, adapt MNIST (ggml/980)
* | | | | | | | | | | | | | | | eee39bdc ggml: refactor cross entropy loss CPU impl. (ggml/976)
* | | | | | | | | | | | | | | | 5d5ab1e5 metal : fix compute pass descriptor autorelease crash (#9718)
* | | | | | | | | | | | | | | | a7ad5535 ggml-backend : add device description to CPU backend (#9720)
* | | | | | | | | | | | | | | | d6fe7abf ggml: unify backend logging mechanism (#9709)
* | | | | | | | | | | | | | | | e3c355ba convert : handle tokenizer merges format from transformers 4.45 (#9696)
* | | | | | | | | | | | | | | | 841713e1 rpc : enable vulkan (#9714)
* | | | | | | | | | | | | | | | 56399714 Fixed dequant precision issues in Q4_1 and Q5_1 (#9711)
* | | | | | | | | | | | | | | | c83ad6d0 ggml-backend : add device and backend reg interfaces (#9707)
* | | | | | | | | | | | | | | | a39ab216 llama : reduce compile time and binary size (#9712)
* | | | | | | | | | | | | | | | f536f4c4 [SYCL] Initial cmake support of SYCL for AMD GPUs (#9658)
* | | | | | | | | | | | | | | | 00b7317e vulkan : do not use tensor->extra (#9407)
* | | | | | | | | | | | | | | | 76b37d15 gguf-split : improve --split and --merge logic (#9619)
* | | | | | | | | | | | | | | | 148844fe examples : remove benchmark (#9704)
* | | | | | | | | | | | | | | | 3f1ae2e3 Update README.md (#9591)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | f1b8c427 sync : ggml
* | | | | | | | | | | | | | | e98c1c18 test: fix OPT_STEP_ADAMW for test-backend-ops (ggml/974)
* | | | | | | | | | | | | | | cb000205 vulkan : mul_mat: fix UB with small warps (ggml/952)
* | | | | | | | | | | | | | | 6c532248 ggml : fix ggml_cast (ggml/973)
* | | | | | | | | | | | | | | 7254cdf7 ggml: fix gradient allocation logic (ggml/966)
* | | | | | | | | | | | | | | cad341d8 metal : reduce command encoding overhead (#9698)
* | | | | | | | | | | | | | | a90484c6 llama : print correct model type for Llama 3.2 1B and 3B
* | | | | | | | | | | | | | | 1927378b convert : refactor rope_freqs generation (#9396)
* | | | | | | | | | | | | | | 6f1d9d71 Fix Docker ROCM builds, use AMDGPU_TARGETS instead of GPU_TARGETS (#9641)
* | | | | | | | | | | | | | | 511636df ci : reduce severity of unused Pyright ignore comments (#9697)
| | | | | | | | | | | | | | | * a34fc0dd ci : reduce severity of unused Pyright ignore comments
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 08a43d05 py : update transfomers version (#9694)
* | | | | | | | | | | | | | | ace4f4be flake.lock: Update (#9680)
* | | | | | | | | | | | | | | 8277a817 console : utf-8 fix for windows stdin (#9690)
* | | | | | | | | | | | | | | c919d5db ggml : define missing HWCAP flags (#9684)
* | | | | | | | | | | | | | | d0b1d663 sync : ggml
* | | | | | | | | | | | | | | aaa40999 CUDA: remove bad assert (ggml/972)
* | | | | | | | | | | | | | | 641002fb vulkan : multithread pipeline creation (ggml/963)
* | | | | | | | | | | | | | | 0de8b203 vulkan : fix build for GGML_VULKAN_RUN_TESTS, add TFLOPS to log (ggml/961)
* | | | | | | | | | | | | | | 544f409b vulkan : argsort barriers must be under uniform control flow (ggml/951)
* | | | | | | | | | | | | | | 6084bfb2 ggml : fix GGML_MAX_N_THREADS + improve formatting (ggml/969)
* | | | | | | | | | | | | | | faac0bae common : ensure llama_batch size does not exceed max size (#9668)
* | | | | | | | | | | | | | | f99d3f83 py : add model class for Chameleon conversion (#9683)
* | | | | | | | | | | | | | | 589b48d4 contrib : add Resources section (#9675)
* | | | | | | | | | | | | | | f4d2b884 llama : add reranking support (#9510)
* | | | | | | | | | | | | | | 1b2f992c test-backend-ops : use flops for some performance tests (#9657)
* | | | | | | | | | | | | | | 73984270 llama : add comment about thread-safety [no ci] (#9449)
* | | | | | | | | | | | | | | 6102037b vocab : refactor tokenizer to reduce init overhead (#9449)
* | | | | | | | | | | | | | | 9a913110 llama : add support for Chameleon (#8543)
* | | | | | | | | | | | | | | 43bcdd97 readme : add tool (#9655)
* | | | | | | | | | | | | | | 6a0f7794 ggml : add run-time detection of neon, i8mm and sve (#9331)
* | | | | | | | | | | | | | | 89f99449 Enable use to the rebar feature to upload buffers to the device. (#9251)
* | | | | | | | | | | | | | | b5de3b74 readme : update hot topics
* | | | | | | | | | | | | | | 44f59b43 cmake : add option for common library (#9661)
* | | | | | | | | | | | | | | 95bc82fb [SYCL] add missed dll file in package (#9577)
* | | | | | | | | | | | | | | 7691654c mtgpu: enable VMM (#9597)
* | | | | | | | | | | | | | | ea9c32be ci : fix docker build number and tag name (#9638)
* | | | | | | | | | | | | | | 1e436302 ggml : remove assert for AArch64 GEMV and GEMM Q4 kernels (#9217)
* | | | | | | | | | | | | | | afbbfaa5 server : add more env vars, improve gen-docs (#9635)
* | | | | | | | | | | | | | | 3d6bf691 llama : add IBM Granite MoE architecture (#9438)
* | | | | | | | | | | | | | | 904837e0 cann: fix crash when llama-bench is running on multiple cann devices (#9627)
* | | | | | | | | | | | | | | 70392f1f ggml : add AVX512DQ requirement for AVX512 builds (#9622)
* | | | | | | | | | | | | | | bb5f8199 sync : ggml
* | | | | | | | | | | | | | | c0389316 examples : adapt to ggml.h changes (ggml/0)
* | | | | | | | | | | | | | | 31ac5834 llama : keep track of all EOG tokens in the vocab (#9609)
* | | | | | | | | | | | | | | cea1486e log : add CONT level for continuing previous log entry (#9610)
* | | | | | | | | | | | | | | 0aa15011 server : add newline after chat example (#9616)
* | | | | | | | | | | | | | | b0f27361 sampling : avoid expensive softmax during greedy sampling (#9605)
* | | | | | | | | | | | | | | c087b6f1 threads: fix msvc build without openmp (#9615)
* | | | | | | | | | | | | | | 116efee0 cuda: add q8_0->f32 cpy operation (#9571)
* | | | | | | | | | | | | | | 0b3bf966 server : add --no-context-shift option (#9607)
* | | | | | | | | | | | | | | f0c7b5ed threads: improve ggml_barrier scaling with large number of threads (#9598)
* | | | | | | | | | | | | | | 1d48e98e readme : add programmable prompt engine language CLI (#9599)
* | | | | | | | | | | | | | | f3979df7 flake.lock: Update (#9586)
* | | | | | | | | | | | | | | 1e7b9299 ggml : AVX512 gemm for Q4_0_8_8 (#9532)
| | | | | | | | | | | | | | | * 114ab634 sampling : fix off-by-one in tail-free sampling
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 37f8c7b4 perplexity : remove extra new lines after chunks (#9596)
* | | | | | | | | | | | | | | bf9c1013 metal : use F32 prec for K*Q in vec FA (#9595)
* | | | | | | | | | | | | | | e62e9789 Revert "[SYCL] fallback mmvq (#9088)" (#9579)
* | | | | | | | | | | | | | | c35e586e musa: enable building fat binaries, enable unified memory, and disable Flash Attention on QY1 (MTT S80) (#9526)
* | | | | | | | | | | | | | | 912c331d Fix merge error in #9454 (#9589)
* | | | | | | | | | | | | | | a5b57b08 CUDA: enable Gemma FA for HIP/Pascal (#9581)
* | | | | | | | | | | | | | | ecd5d6b6 llama: remove redundant loop when constructing ubatch (#9574)
* | | | | | | | | | | | | | | 2a63caaa RWKV v6: RWKV_WKV op CUDA implementation (#9454)
* | | | | | | | | | | | | | | d09770ca ggml-alloc : fix list of allocated tensors with GGML_ALLOCATOR_DEBUG (#9573)
* | | | | | | | | | | | | | | 41f47787 Update CUDA graph on scale change plus clear nodes/params  (#9550)
* | | | | | | | | | | | | | | e948a7da CI: Provide prebuilt windows binary for hip (#9467)
* | | | | | | | | | | | | | | 63351143 quantize : improve type name parsing (#9570)
* | | | | | | | | | | | | | | d13edb17 ggml : fix builds (#0)
* | | | | | | | | | | | | | | 27609c49 ggml : fix trailing whitespace (#0)
* | | | | | | | | | | | | | | 43015353 sync : ggml
* | | | | | | | | | | | | | | 424c5d00 ggml/examples: add backend support for numerical optimization (ggml/949)
* | | | | | | | | | | | | | | a6809c6a examples : add null threadpool args where needed (ggml/0)
* | | | | | | | | | | | | | | 5cb12f68 CUDA: fix sum.cu compilation for CUDA < 11.7 (#9562)
* | | | | | | | | | | | | | | d39e2674 examples : flush log upon ctrl+c (#9559)
| | | | | | | | | | | | | | | * 6e873e56 llama : make llm_tokenizer more private
| | | | | | | | | | | | | | | * d949c584 refactor tokenizer
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 722ec1eb perplexity : do not escape input data by default (#9548)
* | | | | | | | | | | | | | | 6026da52 server : clean-up completed tasks from waiting list (#9531)
* | | | | | | | | | | | | | | eca0fab4 imatrix : disable prompt escape by default (#9543)
* | | | | | | | | | | | | | | 64c6af31 ggml : fix n_threads_cur initialization with one thread (#9538)
* | | | | | | | | | | | | | | 0d2f22e4 scripts : verify py deps at the start of compare (#9520)
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
| | | | | | | | | | | | | | * 6b0248c2 Update ggml/src/ggml.c
| | | | | | | | | | | | | | * f9196c91 ggml : fix n_threads_cur initialization with one thread
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 6443ddd9 llama : use reserve/emplace_back in sampler_sample (#9534)
* | | | | | | | | | | | | | 8a308354 server : match OAI structured output response (#9527)
* | | | | | | | | | | | | | f799155a server : fix OpenSSL build (remove obsolete `LOG_INFO`) (#9529)
* | | | | | | | | | | | | | faf67b3d [SYCL]set context default value to avoid memory issue, update guide (#9476)
* | | | | | | | | | | | | | 7be099fa llama-bench: correct argument parsing error message (#9524)
* | | | | | | | | | | | | | 8b836ae7 arg : add env variable for parallel (#9513)
* | | | | | | | | | | | | | 8344ef58 llama : fix n_vocab init for 'no_vocab' case (#9511)
* | | | | | | | | | | | | | 02266138 threadpool : skip polling for unused threads (#9461)
* | | | | | | | | | | | | | 503147a9 unicode : add <algorithm> (#9508)
* | | | | | | | | | | | | | 0d2ec438 llama : support IBM Granite architecture (#9412)
* | | | | | | | | | | | | | 37f3a381 llama : add llama_n_head() (#9512)
* | | | | | | | | | | | | | 23e0d70b ggml : move common CPU backend impl to new header (#9509)
* | | | | | | | | | | | | | acb2c32c llama : rename n_embed to n_embd in rwkv6_time_mix (#9504)
* | | | | | | | | | | | | | a6a3a5c5 ggml : link MATH_LIBRARY not by its full path (#9339)
* | | | | | | | | | | | | | d54c21df convert : identify missing model files (#9397)
* | | | | | | | | | | | | | 19514d63 cmake : do not hide GGML options + rename option (#9465)
* | | | | | | | | | | | | | 5c3d0f18 ggml : IQ4_NL sgemm + Q4_0 AVX optimization (#9422)
* | | | | | | | | | | | | | 0aadac10 llama : support OLMoE (#9462)
* | | | | | | | | | | | | | 95ca8516 llama : support MiniCPM3 (#9322)
* | | | | | | | | | | | | | 441b72b9 main : option to disable context shift (#9484)
| | | | | | | | | | | | | | * a6a8f8d0 Update docs/backend/SYCL.md
| | | | | | | | | | | | | | * 8241151f set context default to avoid memory issue, update guide
| | | | | | | | | | | | | | | * cc1c0171 naming : normalize the name of callback-related identifiers
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | c4965a64 metal : handle zero-sized allocs (#9466)
* | | | | | | | | | | | | | | 90a2fff0 flake.lock: Update (#9488)
* | | | | | | | | | | | | | | 6262d13e common : reimplement logging (#9418)
* | | | | | | | | | | | | | | e6deac31 gguf-split : add basic checks (#9499)
* | | | | | | | | | | | | | | 6988da94 cmake : correct order of sycl flags (#9497)
| | | | | | | | | | | | | | | * 73ef3f76 Update llama-server-intel.Dockerfile
| | | | | | | | | | | | | | | * 3956cf92 Update llama-cli-intel.Dockerfile
| | | | | | | | | | | | | | | * af95b142 [SYCL] fix cmake broken
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 3c7989fd py : add "LLaMAForCausalLM" conversion support (#9485)
* | | | | | | | | | | | | | | d6b37c88 readme : update tools list (#9475)
* | | | | | | | | | | | | | | 7596487b cmake : try to fix sycl+intel build (#9487)
| |_|_|_|_|_|_|_|_|_|_|/ / /  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 822b6322 ggml : ggml_type_name return "NONE" for invalid values (#9458)
* | | | | | | | | | | | | | dcdcee3a server: add data: [DONE] to /chat/completions stream response (#9459)
* | | | | | | | | | | | | | 1f4111e5 cmake : use list(APPEND ...) instead of set() + dedup linker (#9463)
* | | | | | | | | | | | | | befaf119 llama : make cell_id const in inp_s_mask block (#9470)
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | feff4aa8 server : add loading html page while model is loading (#9468)
* | | | | | | | | | | | | 0abc6a2c llama : llama_perf + option to disable timings during decode (#9355)
| | | | | | | | | | | | | * fb8f1425 one more CMAKE_CXX_FLAGS fix (#9471)
| | | | | | | | | | | | | * 228df2bc cmake : fix sycl build (#9469)
| | | | | | | | | | | | | * b653b1e9 cmake : try to fix sycl 2
| | | | | | | | | | | | | * ae9475de cmake : try fix sycl
| | | | | | | | | | | | | * 19ecca19 cmake : use list(APPEND ...) instead of set() + dedup linker
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | bd35cb0a feat: remove a sampler from a chain (#9445)
* | | | | | | | | | | | | 78203641 server : Add option to return token pieces in /tokenize endpoint (#9108)
* | | | | | | | | | | | | e6b7801b cann: Add host buffer type for Ascend NPU (#9406)
* | | | | | | | | | | | | e6657443 llava : fix the script error in MobileVLM README (#9054)
* | | | | | | | | | | | | d4c3c10f lora : raise error if lm_head is ignored (#9103)
* | | | | | | | | | | | | 2a825116 cmake : fix for builds without `GGML_CDEF_PUBLIC` (#9338)
* | | | | | | | | | | | | 4dc4f5f1 ci : update HIP SDK to 24.Q3 (ROCm 6.1) (#9329)
* | | | | | | | | | | | | c837981b py : add Phi-1.5/Phi-2 tokenizer (#9361)
* | | | | | | | | | | | | 3c26a164 ci : bump actions/checkout to v4 (#9377)
* | | | | | | | | | | | | ff76e185 cmake : fixed the order of linking libraries for llama-quantize (#9450)
* | | | | | | | | | | | | 39f852f4 py : add special tokens in hf_converter for RWKV v6 (#9428)
* | | | | | | | | | | | | 2b00fa79 riscv : modify Makefile and add a RISCV_VECT to print log info (#9442)
* | | | | | | | | | | | | d6a04f87 ggml : hide ggml_object, ggml_cgraph, ggml_hash_set (#9408)
* | | | | | | | | | | | | c9c8575a enhance run script to be easy to change the parameters (#9448)
* | | | | | | | | | | | | df4b7945 cann: Fix error when running a non-exist op (#9424)
* | | | | | | | | | | | | 449ccfb6 Add Jais to list of supported models (#9439)
| | | | | | | | | | | | | * d7c042d1 ggml : make n_threads_cur atomic_int
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 1b280614 llama : skip token bounds check when evaluating embeddings (#9437)
* | | | | | | | | | | | | 8db003a1 py : support converting local models (#7547)
* | | | | | | | | | | | | 0996c559 llava : correct args for minicpmv-cli (#9429)
* | | | | | | | | | | | | 5bb2c5db files : remove accidentally added `lora_test` submodule (#9430)
* | | | | | | | | | | | | 67155ab7 feat: Implements retrying logic for downloading models using --model-url flag (#9255)
* | | | | | | | | | | | | 5af118ef CUDA: fix --split-mode row race condition (#9413)
* | | | | | | | | | | | | d2b496bf batched-bench : remove unused code (#9305)
* | | | | | | | | | | | | b34e0234 musa: remove Clang builtins mapping (#9421)
* | | | | | | | | | | | | 51b60386 sycl : update support conditions  (#9394)
* | | | | | | | | | | | | cb9c933e flake.lock: Update (#9360)
* | | | | | | | | | | | | 6cd4e034 arg : bring back missing ifdef (#9411)
* | | | | | | | | | | | | 8d300bd3 enable --special arg for llama-server (#9419)
* | | | | | | | | | | | | 49006c67 llama : move random seed generation to the samplers (#9398)
| | | | | | | | | | | | | * f9968f66 ggml : update comments [no ci]
| | | | | | | | | | | | | * 119e0bc9 ggml : remove ggml_cplan + rework ggml_cgraph
| | | | | | | | | | | | | * ee154457 ggml : fix compiler warnings
| | | | | | | | | | | | | * 92a96865 ggml : add ggml-impl.h to backends
| | | | | | | | | | | | | * c8a3f291 ggml : hide ggml_object, ggml_cgraph, ggml_hash_set
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 00ba2ff7 metal : fix compile warning with GGML_METAL_NDEBUG (#0)
* | | | | | | | | | | | | 83008b7c llama : update llm_build_copy_mask_state comment [no ci] (#9385)
* | | | | | | | | | | | | 0b4ac757 RWKV v6: Add time_mix_decay_w1/w2 in quant exclusion list (#9387)
* | | | | | | | | | | | | fb3f2498 make : do not run llama-gen-docs when building (#9399)
* | | | | | | | | | | | | bfe76d4a common : move arg parser code to `arg.cpp` (#9388)
* | | | | | | | | | | | | 293bebe0 rpc : fix segfault with nkvo (#9389)
* | | | | | | | | | | | | 5fac4d57 ggml : vector length agnostic SVE support (#9290)
* | | | | | | | | | | | | 5fb5e248 llama : minor sampling refactor (2) (#9386)
* | | | | | | | | | | | | 38ca6f64 readme : update hot topics
* | | | | | | | | | | | | 8e6e2fbe CUDA: fix variable name conflict for Windows build (#9382)
* | | | | | | | | | | | | 5ed08757 readme : add LLMUnity to UI projects (#9381)
* | | | | | | | | | | | | 54f376d0 rpc : update README [no ci] (#9320)
* | | | | | | | | | | | | b2e89a32 Arm AArch64: Documentation updates (#9321)
* | | | | | | | | | | | | daa9623a Overlap cmdbuffer creation and cmdbuffer execution in Vulkan backend by submitting smaller cmdbuffers early. (#9118)
* | | | | | | | | | | | | e079bffb cuda : fix FA Q src index (1 -> 0) (#9374)
* | | | | | | | | | | | | 3f7ccfd6 common : bring back missing args, add env var duplication check (#9375)
* | | | | | | | | | | | | a249843d common : restore --n-gpu-layers (#9371)
| |/ / / / / / / / / / /  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 19f4a7b2 llama : refactor samplers internal implementation (#9370)
* | | | | | | | | | | | 2a358fb0 [SYCL] add check malloc result on device (#9346)
* | | | | | | | | | | | eae59718 llama : sanitize tokens in the upper bound (#9359)
* | | | | | | | | | | | 00b02bb2 imatrix : fix arg parser for imatrix (#9366)
* | | | | | | | | | | | a8768614 metal : update support condition for im2col + fix warning (#0)
* | | | | | | | | | | | 385decbd sync : ggml
* | | | | | | | | | | | 60a3107c scripts : option to increase git patch context
* | | | | | | | | | | | 406c1a32 vulkan: add dryrun support to sin and cos ops (ggml/947)
* | | | | | | | | | | | 9cb92608 vulkan: correctly report support for OP_CONT (ggml/946)
* | | | | | | | | | | | 202084d3 tests: add gradient tests for all backends (ggml/932)
* | | | | | | | | | | | dbbebcab ggml: fix ggml_graph_cpy undefined behavior (ggml/943)
* | | | | | | | | | | | ba1cf846 cann : fix doxy (ggml/0)
* | | | | | | | | | | | d2d3200b cann : add Ascend NPU support (whisper/2336)
* | | | | | | | | | | | 51d964a4 cuda : mark BF16 CONT as unsupported
* | | | | | | | | | | | efe6a83e ggml : fix cont with transposed tensors when one dimension is 1 (ggml/934)
* | | | | | | | | | | | fbb7fcff llama : set attrs of mislabelled EOT/EOM tokens (#9348)
* | | | | | | | | | | | a5b5d9a1 llama.android : fix build (#9350)
* | | | | | | | | | | | f12295b8 llama : fix empty ring buffer push (#9358)
* | | | | | | | | | | | faf69d42 llama : sanitize invalid tokens (#9357)
* | | | | | | | | | | | e536426d llamafile : disable sgemm for batch-size 1 (#9330)
* | | | | | | | | | | | 1b9ae518 common : refactor arg parser (#9308)
* | | | | | | | | | | | e32d0816 ggml : always check bounds on get_rows operations (#9354)
* | | | | | | | | | | | df270ef7 llama : refactor sampling v2 (#9294)
* | | | | | | | | | | | 947538ac ggml : fix missing `cpu_set_t` on emscripten (#9336)
* | | | | | | | | | | | 6c89eb0b ci : disable rocm image creation (#9340)
| | | | | | | | | | | | * cfbf33a7 ggml : style changes + fix 512-bit nb loop check
| | | | | | | | | | | | * a9a9f666 Removed WhiteSpaces
| | | | | | | | | | | | * 6a6cfd6c Implemented vector length agnostic SVE using switch case for 512-bit, 256-bit, 128-bit vector lengths
| | | | | | | | | | | | * 4dbdb6c8 Implemented vector length agnostic SVE using switch case for 512-bit, 256-bit, 128-bit vector lengths
| | | | | | | | | | | | | * c3e2bb6d rpc : fix nkvo
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 9b2c24c0 server : simplify state machine for slot (#9283)
* | | | | | | | | | | | | 134bc38e llama-bench : log benchmark progress (#9287)
* | | | | | | | | | | | | 815b1fb2 batched-bench : add `--output-format jsonl` option (#9293)
* | | | | | | | | | | | | 409dc4f8 ggml : fix build break for the vulkan-debug (#9265)
* | | | | | | | | | | | | 4a1411b4 server : fix missing lock (#9334)
* | | | | | | | | | | | | 8ebe8dde Improve Vulkan shader build system (#9239)
* | | | | | | | | | | | | 9bc6db28 ggml-quants : ternary packing for TriLMs and BitNet b1.58 (#8151)
* | | | | | | | | | | | | 32b2ec88 Update build.yml (#9184)
* | | | | | | | | | | | | 1031771f CMake fix: host for msvc compiler can only be x86 or x64 (#8624)
| | | | | | | | | | | | | * b979fc97 cmake : use ggml-metal.metal from source dir to build default.metallib
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 4db04784 cuda : fix defrag with quantized KV (#9319)
* | | | | | | | | | | | | bdf314f3 llama-bench : fix NUL terminators in CPU name (#9313)
| | | | | | | | | | | | | * 75b3a096 test-backend-ops : add TQ1_0 and TQ2_0 comments for later
| | | | | | | | | | | | | * 8d616076 ggml ; remove unused ggml_mul special case
| | | | | | | | | | | | | *   7f3a619c Merge branch 'master' into compilade/bitnet-ternary
| | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 581c3051 ggml : AVX2 support for Q4_0_8_8 (#8713)
* | | | | | | | | | | | | | 5910ea94 [SYCL] Fix DMMV dequantization (#9279)
* | | | | | | | | | | | | | c8671ae2 Fix broken links in docker.md (#9306)
* | | | | | | | | | | | | | 82e3b03c rpc : make RPC servers come first in the device list (#9296)
* | | | | | | | | | | | | | 9379d3cc readme : rename result_format to response_format (#9300)
* | | | | | | | | | | | | | 7605ae7d flake.lock: Update (#9261)
* | | | | | | | | | | | | | 8962422b llama-bench : add JSONL (NDJSON) output mode (#9288)
| | | | | | | | | | | | | *   cb6d9962 Merge branch 'master' into compilade/bitnet-ternary
| | | | | | | | | | | | | |\  
| | | | | | | | | | | | | * | 35cc5567 ggml-quants : deduplicate TQ1_0 and TQ2_0 __ARM_FEATURE_DOTPROD support
| | | | | | | | | | | | | * |   82b24040 Merge branch 'master' into compilade/bitnet-ternary
| | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | * | | 69f77268 ggml-quants : allow using ARM dot product instructions for TQ1_0
| | | | | | | | | | | | | * | | 895004f3 convert : allow direct conversion to TQ1_0 and TQ2_0
| | | | | | | | | | | | | * | | 3a0bf17d gguf-py : Numpy (de)quantization for TQ1_0 and TQ2_0
| | | | | | | | | | | | | * | |   d911cd1f Merge branch 'master' into compilade/bitnet-ternary
| | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | * | | | 96b3d411 ggml-quants : allow using vdotq_s32 in TQ2_0 vec_dot
| | | | | | | | | | | | | * | | | f034aa1b ggml-quants : rename fields of TQ1_0 and TQ2_0 structs for consistency
| | | | | | | | | | | | | * | | | 04eec581 ggml : remove q1_3 and q2_2
| | | | | | | | | | | | | * | | | 45719a24 ggml : avoid directly using vmlal_high_s8, for 32-bit ARM compat
| | | | | | | | | | | | | * | | | 5417089a ggml : add NEON vec_dot implementation for TQ1_0 and TQ2_0
| | | | | | | | | | | | | * | | | a6dd6994 ggml : fix build issues in certain environments
| | | | | | | | | | | | | * | | | e9719576 ggml : also faster TQ1_0
| | | | | | | | | | | | | * | | | 560873f3 ggml : even faster TQ2_0
| | | | | | | | | | | | | * | | | 77b8f84a ggml : add TQ1_0 and TQ2_0 ternary quantization types
| | | | | | | | | | | | | * | | |   79a278e9 Merge branch 'master' into compilade/bitnet-ternary
| | | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | | * | | | | dd3e62a7 ggml : add some informative comments in q1_3 vec_dot
| | | | | | | | | | | | | * | | | | 8fbd5930 ggml-quants : attempt to fix Arm 32-bit support
| | | | | | | | | | | | | * | | | | ec50944b ggml-quants : fix build failure on Windows
| | | | | | | | | | | | | * | | | | bfd2f21f bitnet : replace 1.58b with b1.58, as in the paper
| | | | | | | | | | | | | * | | | | 09961499 convert-hf : allow converting the weird BitNet 1.3B
| | | | | | | | | | | | | * | | | | 961e2938 convert-hf : simplify BitNet pre-quantization
| | | | | | | | | | | | | * | | | | 89dc3b25 ggml-quants : use ceiling division when quantizing q1_3
| | | | | | | | | | | | | * | | | | 9465ec6e ggml-quants : ARM NEON vec_dot for q2_2 and q1_3
| | | | | | | | | | | | | * | | | | 638ad52f ggml-quants : cleanup Q1_3 code formatting
| | | | | | | | | | | | | * | | | | ef1e345c ggml-quants : Q2_2 now faster than Q4_K on with AVX2
| | | | | | | | | | | | | * | | | | 48b73b84 ggml-quants : substract 1 when back in epi8
| | | | | | | | | | | | | * | | | | 7ef4254a ggml-quants : faster 1.625 bpw AVX2 vec_dot
| | | | | | | | | | | | | * | | | | bd807499 ggml-quants : 1.625 bpw ternary packing for BitNet 1.58b
| | | | | | | | | | | | | | | | | | * f648ca2c llama : add llama_sampling API + move grammar in libllama
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | b69a480a readme : refactor API section + remove old hot topics
| |_|_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 48baa61e server : test script : add timeout for all requests (#9282)
* | | | | | | | | | | | | | | | | f1485161 src: make tail invalid when kv cell is intersection for mamba (#9249)
* | | | | | | | | | | | | | | | | 048de848 docker : fix missing binaries in full-cuda image (#9278)
* | | | | | | | | | | | | | | | | f771d064 ggml : add pthread includes on FreeBSD (#9258)
* | | | | | | | | | | | | | | | | 6e7d133a server : refactor multitask handling (#9274)
* | | | | | | | | | | | | | | | | b60074f1 llama-cli : remove duplicated log message (#9275)
* | | | | | | | | | | | | | | | | 9c1ba557 build(nix): Package gguf-py (#5664)
* | | | | | | | | | | | | | | | | c6d4cb46 llama : minor style
| |_|_|_|_|_|_|_|_|/ / / / / / /  
|/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | * 40fa68cb readme : add API change notice
| | | | | | | | | | | | | | | | * 4e379017 llama : fix comment
| | | | | | | | | | | | | | | | * 086e7f6e llama : disambiguate API
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 8f1d81a0 llama : support RWKV v6 models (#8980)
| |_|_|_|_|_|_|_|/ / / / / / /  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | a47667cf nix: fix CUDA build - replace deprecated autoAddOpenGLRunpathHook
* | | | | | | | | | | | | | | ea5d7478 sgemm : improved Q4_0 and Q8_0 performance via 4xN and Mx4 gemm (#8908)
* | | | | | | | | | | | | | | 49271efb llama : fix typo in xcda_array_view comment [no ci] (#9132)
* | | | | | | | | | | | | | | 0ab30f8d llama : fix llama_split_mode enum values in main_gpu document (#9057)
* | | | | | | | | | | | | | | cddae488 Correct typo run_llama2.sh > run-llama2.sh (#9149)
* | | | | | | | | | | | | | | 7ea8d80d llava : the function "clip" should be int (#9237)
* | | | | | | | | | | | | | | 42c76d13 Threadpool: take 2 (#8672)
* | | | | | | | | | | | | | | 9f7d4bcf server : fix crash when error handler dumps invalid utf-8 json (#9195)
* | | | | | | | | | | | | | | 1d1ccce6 flake.lock: Update (#9162)
* | | | | | | | | | | | | | | 9fe94cca docker : build images only once (#9225)
* | | | | | | | | | | | | | | 66b039a5 docker : update CUDA images (#9213)
* | | | | | | | | | | | | | | 20f1789d vulkan : fix build (#0)
* | | | | | | | | | | | | | | 231cff5f sync : ggml
* | | | | | | | | | | | | | | 3246fe84 Fix minicpm example directory (#9111)
* | | | | | | | | | | | | | | 78eb487b llama : fix qs.n_attention_wv for DeepSeek-V2 (#9156)
* | | | | | | | | | | | | | | a77feb5d server : add some missing env variables (#9116)
* | | | | | | | | | | | | | | 2e59d61c llama : fix ChatGLM4 wrong shape (#9194)
* | | | | | | | | | | | | | | 75e1dbba llama : fix llama3.1 rope_freqs not respecting custom head_dim (#9141)
* | | | | | | | | | | | | | | ad76569f common : Update stb_image.h to latest version (#9161)
* | | | | | | | | | | | | | | 7d787ed9 ggml : do not crash when quantizing q4_x_x with an imatrix (#9192)
* | | | | | | | | | | | | | | 06658ad7 metal : separate scale and mask from QKT in FA kernel (#9189)
* | | | | | | | | | | | | | | fc18425b ggml : add SSM Metal kernels (#8546)
* | | | | | | | | | | | | | | 879275ac tests : fix compile warnings for unreachable code (#9185)
| | | | | | | | | | | | | | | * a95225cd metal : another fix for the fa kernel
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | * aa931d03 metal : fix fa kernel
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 7a3df798 ci : add VULKAN support to ggml-ci (#9055)
* | | | | | | | | | | | | | | e5edb210 server : update deps (#9183)
* | | | | | | | | | | | | | | 0c41e03c metal : gemma2 flash attention support (#9159)
* | | | | | | | | | | | | | | f12ceaca ggml-ci : try to improve build time (#9160)
| | | | | | | | | | | | | | | * 64945098 backup
| | | | | | | | | | | | | | | * b180cb35 backup
| | | | | | | | | | | | | | | | * ccb45186 docs : remove references
| | | | | | | | | | | | | | | | * e48fd74b ggml : remove k_quants_per_iteration macro
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 436787f1 llama : fix time complexity of string replacement (#9163)
* | | | | | | | | | | | | | | | 93bc3839 common: fixed not working find argument --n-gpu-layers-draft (#9175)
* | | | | | | | | | | | | | | | f91fc563 CUDA: fix Gemma 2 numerical issues for FA (#9166)
* | | | | | | | | | | | | | | | e11bd856 CPU/CUDA: Gemma 2 FlashAttention support (#8542)
* | | | | | | | | | | | | | | | 8f824ffe quantize : fix typo in usage help of `quantize.cpp` (#9145)
* | | | | | | | | | | | | | | | 3ba780e2 lora : fix llama conversion script with ROPE_FREQS (#9117)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | a07c32ea llama : use F32 precision in GLM4 attention and no FA (#9130)
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 11b84eb4 [SYCL] Add a space to supress a cmake warning (#9133)
* | | | | | | | | | | | | | 1731d423 [SYCL] Add oneDNN primitive support (#9091)
| |_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | a1631e53 llama : simplify Mamba with advanced batch splits (#8526)
* | | | | | | | | | | | | fc54ef0d server : support reading arguments from environment variables (#9105)
| | | | | | | | | | | | | * 80626503 llama : fix simple splits when the batch contains embeddings
| | | | | | | | | | | | | *   80d9d2a5 Merge branch 'master' into compilade/batch-splits
| | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | b40eb848 llama : support for `falcon-mamba` architecture (#9074)
* | | | | | | | | | | | | | f63f603c llava : zero-initialize clip_ctx structure fields with aggregate initialization 908)
* | | | | | | | | | | | | | 8455340b llama : std::move llm_bigram_bpe from work_queue (#9062)
* | | | | | | | | | | | | | 2f3c1466 llava: Add ACC OP for GPU acceleration to the Vulkan backend in the LLAVA CLIP model. (#8984)
* | | | | | | | | | | | | | 50addec9 [SYCL] fallback mmvq (#9088)
* | | | | | | | | | | | | | 4f8d19ff [SYCL] Fix SYCL `im2col` and `convert` Overflow with Large Dims (#9052)
|/ / / / / / / / / / / / /  
* | | | | | | | | | | | | 90db8146 tests : add missing comma in grammar integration tests (#9099)
* | | | | | | | | | | | | cfac111e cann: add doc for cann backend (#8867)
* | | | | | | | | | | | | 1b6ff90f rpc : print error message when failed to connect endpoint (#9042)
* | | | | | | | | | | | | 18eaf29f rpc : prevent crashes on invalid input (#9040)
* | | | | | | | | | | | | 554b0490 flake.lock: Update (#9068)
* | | | | | | | | | | | | 2339a0be tests : add integration test for lora adapters (#8957)
* | | | | | | | | | | | | 2fb92678 Fix incorrect use of ctx_split for bias tensors (#9063)
* | | | | | | | | | | | | 8b3befc0 server : refactor middleware and /health endpoint (#9056)
* | | | | | | | | | | | | d565bb2f llava : support MiniCPM-V-2.6 (#8967)
* | | | | | | | | | | | | ee2984bd py : fix wrong input type for raw_dtype in ggml to gguf scripts (#8928)
* | | | | | | | | | | | | c8ddce85 Fix inference example lacks required parameters (#9035)
* | | | | | | | | | | | | 23fd4535 gguf-py : bump version from 0.9.1 to 0.10.0 (#9051)
* | | | | | | | | | | | | c679e0cb llama : add EXAONE model support (#9025)
* | | | | | | | | | | | | fb487bb5 common : add support for cpu_get_num_physical_cores() on Windows (#8771)
* | | | | | | | | | | | | 2a24c8ca Add Nemotron/Minitron GGUF Conversion & Inference Support (#8922)
* | | | | | | | | | | | | e3f6fd56 ggml : dynamic ggml_sched_max_splits based on graph_size (#9047)
* | | | | | | | | | | | | 4b9afbbe retrieval : fix memory leak in retrieval query handling (#8955)
* | | | | | | | | | | | | 37501d9c server : fix duplicated n_predict key in the generation_settings (#8994)
* | | | | | | | | | | | | 4af8420a common : remove duplicate function llama_should_add_bos_token (#8778)
* | | | | | | | | | | | | 6bda7ce6 llama : add pre-tokenizer regexes for BLOOM and gpt3-finnish (#8850)
* | | | | | | | | | | | | d5492f05 ci : disable bench workflow (#9010)
* | | | | | | | | | | | | 234b3067 server : init stop and error fields of the result struct (#9026)
| | | | | | | | | | | | * 1be5ea7d llama : add llama_model_is_recurrent to simplify figuring that out
| | | | | | | | | | | | * b264eddb llama : fix Mamba pooled embeddings with multiple sequences
| | | | | | | | | | | | * 652e9b0d llama : fix T5 segfault again
| | | | | | | | | | | | *   702e1995 Merge branch 'master' into compilade/batch-splits
| | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 5fd89a70 Vulkan Optimizations and Fixes (#8959)
| | | | | | | | | | | | * 0596a99f minor : add struct members for clarity
| | | | | | | | | | | | * cfd5a113 llama : rename llama_reorder_outputs to llama_output_reorder
| | | | | | | | | | | | *   5679a3bd Merge branch 'master' into compilade/batch-splits
| | | | | | | | | | | | |\  
| | | | | | | | | | | | * | 952ed35b llama : minor cosmetic changes
| | | | | | | | | | | | * | 704a3033 llama : fix Mamba session save and restore
| | | | | | | | | | | | * |   0dea4263 Merge branch 'master' into compilade/batch-splits
| | | | | | | | | | | | |\ \  
| | | | | | | | | | | | * \ \   9c0a61f8 Merge branch 'master' into compilade/batch-splits
| | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | * | | | 1725de76 llama : fix t5 segfault
| | | | | | | | | | | | * | | | 1fb5d4fd llama : apply suggestions
| | | | | | | | | | | | * | | | 7b7db0bb llama : logits_all has priority over batch->logits
| | | | | | | | | | | | * | | | 2e4adb47 llama : fix integer signedness mixing
| | | | | | | | | | | | * | | |   22504ec6 Merge branch 'master' into compilade/batch-splits
| | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | * | | | | c51daefc llama : advanced batch splits
| | | | | | | | | | | | | | | | | * 9127800d wip
| | | | | | | | | | | | | | | | | * 33a5c8e3 llama : prepare next graph while the current one is being evaluated
| | | | | | | | | | | | | | | | | | * 62d7b6c8 cuda : re-add q4_0
| | | | | | | | | | | | | | | | | | * 503983a6 cuda : build only necessary templates
| | | | | | | | | | | | | | | | | | * ae41fd2e make : force CPU extensions [no ci]
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 98a532d4 server : fix segfault on long system prompt (#8987)
* | | | | | | | | | | | | | | | | | 43bdd3ce cmake : remove unused option GGML_CURL (#9011)
| |_|_|_|_|_|_|_|_|_|/ / / / / / /  
|/| | | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | | * 93ec58b9 server : fix typo in comment
| | | | | | | | | | | | | | | | | *   af2f84c9 Merge branch 'master' into compilade/fix-server-long-system-prompt
| | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 06943a69 ggml : move rope type enum to ggml.h (#8949)
* | | | | | | | | | | | | | | | | | 828d6ff7 export-lora : throw error if lora is quantized (#9002)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | fc4ca27b ci : fix github workflow vulnerable to script injection (#9008)
* | | | | | | | | | | | | | | | | 1f67436c ci : enable RPC in all of the released builds (#9006)
* | | | | | | | | | | | | | | | | 0fd93cde llama : model-based max number of graph nodes calculation (#8970)
* | | | | | | | | | | | | | | | | 84eb2f4f docs: introduce gpustack and gguf-parser (#8873)
* | | | | | | | | | | | | | | | | 1262e7ed grammar-parser : fix possible null-deref (#9004)
* | | | | | | | | | | | | | | | | df5478fb ggml: fix div-by-zero (#9003)
* | | | | | | | | | | | | | | | | 2589292c Fix a spelling mistake (#9001)
* | | | | | | | | | | | | | | | | d3ae0ee8 py : fix requirements check '==' -> '~=' (#8982)
* | | | | | | | | | | | | | | | | 5ef07e25 server : handle models with missing EOS token (#8997)
| |_|_|_|_|_|_|_|_|/ / / / / / /  
|/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | * c1b738ef server : fix parallel generation with very small batch sizes
| | | | | | | | | | | | | | | * 7eda5583 server : fix segfault on long system prompt
| | | | | | | | | | | | | | | | * faaac59d llama : support NUL bytes in tokens
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 4134999e gguf-py : Numpy dequantization for most types (#8939)
* | | | | | | | | | | | | | | | 8cd1bcfd flake.lock: Update (#8979)
* | | | | | | | | | | | | | | | a21c6fd4 update guide (#8909)
* | | | | | | | | | | | | | | | 33309f66 llama : check all graph nodes when searching for result_embd_pooled (#8956)
* | | | | | | | | | | | | | | | 7c5bfd57 Optimize Vulkan backend for better CPU performance and less GPU synchronization overhead. (#8943)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 6e02327e metal : fix uninitialized abort_callback (#8968)
* | | | | | | | | | | | | | | 7eb23840 llama : default n_swa for phi-3 (#8931)
* | | | | | | | | | | | | | | 7c3f55c1 Add support for encoder-only T5 models (#8900)
* | | | | | | | | | | | | | | 911b437f gguf-py : fix double call to add_architecture() (#8952)
* | | | | | | | | | | | | | | b72942fa Merge commit from fork
* | | | | | | | | | | | | | | 6afd1a99 llama : add support for lora adapters in T5 model (#8938)
* | | | | | | | | | | | | | | 272e3bd9 make : fix llava obj file race (#8946)
* | | | | | | | | | | | | | | 45a55b91 llama : better replace_all (cont) (#8926)
* | | | | | | | | | | | | | | 3071c0a5 llava : support MiniCPM-V-2.5 (#7599)
* | | | | | | | | | | | | | | 4305b57c sync : ggml
* | | | | | | | | | | | | | | 70c0ea35 whisper : use vulkan as gpu backend when available (whisper/2302)
* | | | | | | | | | | | | | | 5b2c04f4 embedding : add --pooling option to README.md [no ci] (#8934)
* | | | | | | | | | | | | | | 6f6496bb llama : fix typo in llama_tensor_get_type comment [no ci] (#8937)
* | | | | | | | | | | | | | | daef3ab2 server : add one level list nesting for embeddings (#8936)
* | | | | | | | | | | | | | | 345a686d llama : reduce useless copies when saving session (#8916)
| | | | | | | | | | | | | | | * 73bc9350 gguf-py : Numpy dequantization for grid-based i-quants
| | | | | | | | | | | | | | | * 5a9edda7 gguf-py : Numpy dequantization for most types
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 3a14e003 gguf-py : simplify support for quant types (#8838)
* | | | | | | | | | | | | | | afd27f01 scripts : sync cann files (#0)
* | | | | | | | | | | | | | | 366d486c scripts : fix sync filenames (#0)
* | | | | | | | | | | | | | | e44a561a sync : ggml
* | | | | | | | | | | | | | | f93d49ab ggml : ignore more msvc warnings (ggml/906)
* | | | | | | | | | | | | | | 5b33ea1e metal : fix struct name (ggml/912)
* | | | | | | | | | | | | | | 85fca8de metal : add abort callback (ggml/905)
* | | | | | | | | | | | | | | ebd541a5 make : clean llamafile objects (#8923)
| | | | | | | | | | | | | | | * 9329953a llama : avoid double tensor copy when saving session to buffer
| | | | | | | | | | | | | | | * dca7ad86 llama : avoid useless copies in dummy session writer
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 15fa07a5 make : use C compiler to build metal embed object (#8899)
| | | | | | | | | | | | | | | * 7764ab91 update guide
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | be55695e ggml-backend : fix async copy from CPU (#8897)
* | | | | | | | | | | | | | | 0478174d [SYCL] Updated SYCL device filtering  (#8901)
* | | | | | | | | | | | | | | a8dbc6f7 CUDA/HIP: fix tests/test-backend-ops (#8896)
* | | | | | | | | | | | | | | 506122d8 llama-bench : add support for getting cpu info on Windows (#8824)
* | | | | | | | | | | | | | | 725e3d94 quantize : update usage comment in quantize.cpp (#8889)
* | | | | | | | | | | | | | | 31958546 typo correction (#8891)
* | | | | | | | | | | | | | | 1e6f6554 server : add lora hotswap endpoint (WIP) (#8857)
* | | | | | | | | | | | | | | 641f5dd2 CUDA: fix padding logic for FP16/FP32 (#8884)
| | | | | | | | | | | | | | | * cad8abb4 add tool to allow plotting tensor allocation maps within buffers
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 5f4dcb1e simple : update name of executable to llama-simple (#8885)
* | | | | | | | | | | | | | | db20f50c cmake : Link vulkan-shaders-gen with pthreads (#8835)
* | | | | | | | | | | | | | | efda90c9 [Vulkan] Fix compilation of `vulkan-shaders-gen` on w64devkit after `e31a4f6` (#8880)
* | | | | | | | | | | | | | | 0bf16de0 contributing : add note about write access
* | | | | | | | | | | | | | | 2d5dd7bb ggml : add epsilon as a parameter for group_norm (#8818)
* | | | | | | | | | | | | | | cdd1889d convert : add support for XLMRoberta embedding models (#8658)
* | | | | | | | | | | | | | | c21a8964 [CANN]: Fix ggml_backend_cann_buffer_get_tensor (#8871)
* | | | | | | | | | | | | | | d4ff8471 [SYCL] correct cmd name (#8877)
* | | | | | | | | | | | | | | 0a4ce786 common : Changed tuple to struct (TODO fix) (#8823)
| | | | | | | | | | | | | | | * 6e299132 clip : style changes
| | | | | | | | | | | | | | | * 65f7455c Modify 2 notes
| | | | | | | | | | | | | | | * f3d400da remove uhd_image_embed
| | | | | | | | | | | | | | | * 72b96292 delete minicpmv-wrapper in pr
| | | | | | | | | | | | | | | * 107e1edb fix uhd code for review comment
| | | | | | | | | | | | | | | * 6fd0937e remove the extern "C", MINICPMV_API
| | | | | | | | | | | | | | | * fcde9971 remove load_image_size into clip_ctx
| | | | | | | | | | | | | | | * 3642be99 fix KEY_HAS_MINICPMV_PROJ
| | | | | | | | | | | | | | | * dad4abe1 add warn
| | | | | | | | | | | | | | | * 62fa15bc fix cmakefile
| | | | | | | | | | | | | | | * 4c755832 remove in line 33 directory in the /cmakelists.txt (not in example, in the main dir
| | | | | | | | | | | | | | | * be8b5b2f fix code review
| | | | | | | | | | | | | | | * 292a4690 change pr readme
| | | | | | | | | | | | | | | * 5959b14b fix llama-minicpmv-cli in cmake file
| | | | | | | | | | | | | | | * c5b68515 fix issues for merging
| | | | | | | | | | | | | | | * 3e6348b8 fix bug in clip
| | | | | | | | | | | | | | | * 977941d9 imitate reshape bug of python code
| | | | | | | | | | | | | | | * 4c67d7ce add space in "-1"
| | | | | | | | | | | | | | | * e68c8bc1 change n_layer
| | | | | | | | | | | | | | | * 8f035057 fix quality problem in pr code
| | | | | | | | | | | | | | | *   cb8cfb9d Merge pull request #15 from OpenBMB/master
| | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | *   77beb4d1 Merge branch 'prepare-PR-of-minicpm-v2.5' into master
| | | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | |/  
| | | | | | | | | | | | | | | |/|   
| | | | | | | | | | | | | | | * |   ee5b8509 Merge pull request #11 from OpenBMB/pr_add_all_in_llava
| | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | | * | efe4c617 put all code into llava dir
| | | | | | | | | | | | | | | | * | a95a6d99 receive review comments and modify
| | | | | | | | | | | | | | | * | |   c390dd4e Merge branch 'ggerganov:master' into prepare-PR-of-minicpm-v2.5
| | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | |/ /  
| | | | | | | | | | | | | | | |/| |   
| | | | | | | | | | | | | | | * | | a913ca4c receive review comments and modify
| | | | | | | | | | | | | | | * | | 88f5e6ab fix bug in bicubic resize when need resize iamge smaller
| | | | | | | | | | | | | | | * | | c38d152d fix warnings
| | | | | | | | | | | | | | | * | | 07f48f96 fix warnings
| | | | | | | | | | | | | | | * | | 02eb445d sync master
| | | | | | | | | | | | | | | * | |   28d4a7f9 Merge pull request #8 from OpenBMB/master
| | | | | | | | | | | | | | | |\ \ \  
| | | |_|_|_|_|_|_|_|_|_|_|_|_|/ / /  
| | |/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | * | |   8bd47ce5 Merge pull request #7 from OpenBMB/prepare-PR
| | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | * \ \   8767ce29 Merge branch 'prepare-PR-of-minicpm-v2.5' into prepare-PR
| | | | | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | | | | |/ / /  
| | | | | | | | | | | | | | | |/| | |   
| | | | | | | | | | | | | | | * | | | b37ab0b1 add link
| | | | | | | | | | | | | | | * | | | 9495504e replace and organize code
| | | | | | | | | | | | | | | * | | | 3c306f18 clear code
| | | | | | | | | | | | | | | * | | | 056d1781 rename wrapper
| | | | | | | | | | | | | | | * | | | 6366d62d updata cmakelist
| | | | | | | | | | | | | | | * | | | e73a0c7c updata cmakelist
| | | | | | | | | | | | | | | * | | | d8974b8e support ollama
| | | | | | | | | | | | | | | * | | | 8541e996 better pos_embed in clip
| | | | | | | | | | | | | | | * | | | 2997a680 change for ollama
| | | | | | | | | | | | | | | * | | | 18fe6209 change for ollama
| | | | | | | | | | | | | | | * | | | d9fbc1d1 add positions index
| | | | | | | | | | | | | | | * | | | b48708af random pos_embed
| | | | | | | | | | | | | | | * | | | 629420ee add result in readme
| | | | | | | | | | | | | | | * | | |   b31f51f5 Merge pull request #1 from harvestingmoon/minicpm-v2.5
| | | | | | | | | | | | | | | |\ \ \ \  
| | | | | | | | | | | | | | | | * | | | 94dcaba6 fixed line
| | | | | | | | | | | | | | | |/ / / /  
| | | | | | | | | | | | | | | * | | | 7573b634 Update README.md
| | | | | | | | | | | | | | | * | | | a491f45c change name in readme
| | | | | | | | | | | | | | | * | | | ec1cea71 add instructions in readme
| | | | | | | | | | | | | | | * | | | 0480d5fa add android readme
| | | | | | | | | | | | | | | * | | | 2b919034 add run android for termux in readme
| | | | | | | | | | | | | | | * | | | c536fa6e rename
| | | | | | | | | | | | | | | * | | | 7a49a6f6 init
| | | | | | | | | | | | | | | | | | | * 16dab13b correct cmd name
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | | bc0f887e cann: fix buffer_num and runtime speed slowly error (#8865)
* | | | | | | | | | | | | | | | | | | b42978e7 readme : add ramalama to the availables UI (#8811)
* | | | | | | | | | | | | | | | | | | b9dfc25c ggml : fix overflows in elu function (#8866)
* | | | | | | | | | | | | | | | | | | 1ef14b30 py: Add more authorship metadata from model card (#8810)
* | | | | | | | | | | | | | | | | | | d3f0c716 Stop the generation when <|eom_id|> token is encountered - needed for Llama 3.1 tool call support (#8858)
* | | | | | | | | | | | | | | | | | | e31a4f67 cmake: fix paths for vulkan shaders compilation on Windows (#8573)
* | | | | | | | | | | | | | | | | | | 400ae6f6 readme : update model list (#8851)
* | | | | | | | | | | | | | | | | | | f1ea5146 llama : better replace_all (#8852)
* | | | | | | | | | | | | | | | | | | 064cdc26 vulkan : fix Qantized Mat-Vec Mul on AMD GPUs for ncols < 64 (#8855)
* | | | | | | | | | | | | | | | | | | 5587e57a sync : ggml
* | | | | | | | | | | | | | | | | | | a3738b2f vulkan : implement Stable Diffusion operators (ggml/904)
* | | | | | | | | | | | | | | | | | | 655858ac ggml : move c parameter comment to ggml_rope_ext (ggml/901)
* | | | | | | | | | | | | | | | | | | c02b0a8a cann: support q4_0 model (#8822)
| |_|_|_|_|_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 0d6fb52b Install curl in runtime layer (#8693)
* | | | | | | | | | | | | | | | | | 978ba3d8 Server: Don't ignore llama.cpp params (#8754)
* | | | | | | | | | | | | | | | | | ecf6b7f2 batched-bench : handle empty `-npl` (#8839)
| | | | | | | | | | | | | | | | | | * bddcc5f9 llama : better replace_all
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 01aae2b4 baby-llama : remove duplicate vector include
* | | | | | | | | | | | | | | | | | 4b77ea95 flake.lock: Update (#8847)
* | | | | | | | | | | | | | | | | | 76614f35 ggml : reading the runtime sve config of the cpu (#8709)
| | | | | | | | | | | | | | | | | | * 229c35cb gguf-py : remove LlamaFileTypeMap
| | | | | | | | | | | | | | | | | | * e82ff5a3 gguf-py : fix BF16 numpy view type
| | | | | | | | | | | | | | | | | | * 861265b9 gguf-py : fix flake8 lint
| | | | | | | | | | | | | | | | | | * 5e27e7e1 convert_hf : simplify internal quantization type selection
| | | | | | | | | | | | | | | | | | * 1ac1a791 gguf-py : use classes for quants
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | b72c20b8 Fix conversion of unnormalized BF16->BF16 weights (#7843)
* | | | | | | | | | | | | | | | | | e09a800f cann: Fix ggml_cann_im2col for 1D im2col (#8819)
* | | | | | | | | | | | | | | | | | 0fbbd884 [SYCL] Fixing wrong VDR iq4nl value (#8812)
* | | | | | | | | | | | | | | | | | afbb4c13 ggml-cuda: Adding support for unified memory (#8035)
* | | | | | | | | | | | | | | | | | b7a08fd5 Build: Only include execinfo.h on linux systems that support it (#8783)
* | | | | | | | | | | | | | | | | | 7a11eb3a cuda : fix dmmv cols requirement to 2*GGML_CUDA_DMMV_X (#8800)
* | | | | | | | | | | | | | | | | | c8a00909 cann: support q8_0 for Ascend backend (#8805)
* | | | | | | | | | | | | | | | | | afbbcf3c server : update llama-server embedding flag documentation (#8779)
* | | | | | | | | | | | | | | | | | ed9d2854 Build: Fix potential race condition (#8781)
* | | | | | | | | | | | | | | | | | 398ede5e Adding Gemma 2 2B configs (#8784)
* | | | | | | | | | | | | | | | | | 44d28ddd cmake : fix use of external ggml (#8787)
* | | | | | | | | | | | | | | | | | 268c5660 nix: cuda: rely on propagatedBuildInputs (#8772)
* | | | | | | | | | | | | | | | | | 7e72aa74 py: add_array() will not add to kv store if value is an empty array (#8774)
* | | | | | | | | | | | | | | | | | 7c27a19b added android implementation of ggml_print_backtrace_symbols (#8751)
* | | | | | | | | | | | | | | | | | 140074bb flake.lock: Update (#8729)
* | | | | | | | | | | | | | | | | | 6e2b6000 cann: update cmake (#8765)
* | | | | | | | | | | | | | | | | | c887d8b0 [SYCL] Add `TIMESTEP_EMBEDDING` OP (#8707)
* | | | | | | | | | | | | | | | | | 75af08c4 ggml: bugfix: fix the inactive elements is agnostic for risc-v vector (#8748)
| | | | | | | | | | | | | | | | | | * eab4a882 Using dp4a ptx intrinsics for an improved Mul8MAT perf [By Alcpz]
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 439b3fc7 cuda : organize vendor-specific headers into vendors directory (#8746)
* | | | | | | | | | | | | | | | | | 0832de72 [SYCL] add conv support (#8688)
| |_|_|_|_|_|_|_|/ / / / / / / / /  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 6eeaeba1 cmake: use 1 more thread for non-ggml in CI (#8740)
* | | | | | | | | | | | | | | | | 4730faca chore : Fix vulkan related compiler warnings, add help text, improve CLI options (#8477)
| |_|_|_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 4c676c85 llama : refactor session file management (#8699)
* | | | | | | | | | | | | | | | e54c35e4 feat: Support Moore Threads GPU  (#8383)
* | | | | | | | | | | | | | | | 5e2727fe scripts : sync vulkan-shaders (#0)
* | | | | | | | | | | | | | | | 56f20aa2 scripts : sync ggml-aarch64 sources
* | | | | | | | | | | | | | | | 345c8c0c ggml : add missing semicolon (#0)
* | | | | | | | | | | | | | | | ae7985cd sync : ggml
* | | | | | | | | | | | | | | | a05ca936 ggml : loop tiling optimizations for scalar path (ggml/898)
* | | | | | | | | | | | | | | | 9f77d899 ggml: add support for float16 input tensors in pooling operations (ggml/895)
* | | | | | | | | | | | | | | | 203b7f15 vulkan : initialize vk_buffer_struct members to VK_NULL_HANDLE (ggml/893)
* | | | | | | | | | | | | | | | d2b851bf cmake : only enable GGML_NATIVE and x86 flags if not crosscompiling (ggml/885)
* | | | | | | | | | | | | | | | c12b6e8e ggml : remove unnecessary UNUSED macro call (ggml/880)
* | | | | | | | | | | | | | | | b5e95468 llama : add support for llama 3.1 rope scaling factors (#8676)
* | | | | | | | | | | | | | | | 92090eca llama : add function for model-based max number of graph nodes (#8622)
* | | | | | | | | | | | | | | | 9d03d085 common : add --no-warmup option for main/llama-cli (#8712)
* | | | | | | | | | | | | | | | bfb4c749 cann: Fix Multi-NPU execution error (#8710)
* | | | | | | | | | | | | | | | 2b1f616b ggml : reduce hash table reset cost (#8698)
* | | | | | | | | | | | | | | | 01245f5b llama : fix order of parameters (#8706)
* | | | | | | | | | | | | | | | 01aec4a6 server : add Speech Recognition & Synthesis to UI (#8679)
* | | | | | | | | | | | | | | | 41cd47ca examples : export-lora : fix issue with quantized base models (#8687)
* | | | | | | | | | | | | | | | 49ce0ab6 ggml: handle ggml_init failure to fix NULL pointer deref (#8692)
* | | | | | | | | | | | | | | | 4226a8d1 llama : fix build + fix fabs compile warnings (#8683)
* | | | | | | | | | | | | | | | bf5a81df ggml : fix build on Windows with Snapdragon X (#8531)
* | | | | | | | | | | | | | | | 88954f7f tests : fix printfs (#8068)
| | | | | | | | | | | | | | | | * 9cddd9ae llama : cast seq_id in comparison with unsigned n_seq_max
| | | | | | | | | | | | | | | | * ffd5117d llama : more graceful error handling of invalid session files
| | | | | | | | | | | | | | | | * 83e6a17d llama : fix session file loading
| | | | | | | | | | | | | | | | * c8b424fa llama : remove _context suffix for llama_data_context
| | | | | | | | | | | | | | | | * cddc899b llama : various integer type cast and format string fixes
| | | | | | | | | | | | | | | | * 9e22064a llama : fix uint64_t format type
| | | | | | | | | | | | | | | | * 8e39037b llama : refactor session file management
| | | | | | | | | | | | | | | | | * 9aeb0e1f sycl add conv support
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | ed67bcb2 [SYCL] fix multi-gpu issue on sycl (#8554)
* | | | | | | | | | | | | | | | | eddcb523 ggml : add and use ggml_cpu_has_llamafile() (#8664)
* | | | | | | | | | | | | | | | | be6d7c07 examples : remove `finetune` and `train-text-from-scratch` (#8669)
* | | | | | | | | | | | | | | | | 4b0eff3d docs : Quantum -> Quantized (#8666)
* | | | | | | | | | | | | | | | | 8a4bad50 llama: use sliding window for phi3 (#8627)
* | | | | | | | | | | | | | | | | 68504f09 readme : update games list (#8673)
* | | | | | | | | | | | | | | | | f19bf99c Build Llama SYCL Intel with static libs (#8668)
* | | | | | | | | | | | | | | | | 3a7ac530 readme : update UI list [no ci] (#8505)
* | | | | | | | | | | | | | | | | 96952e71 llama : fix `llama_chat_format_single` for mistral (#8657)
* | | | | | | | | | | | | | | | | 79167d9e Re-add erroneously removed -fsycl from GGML_EXTRA_LIBS (#8667)
* | | | | | | | | | | | | | | | | b115105f add llama_lora_adapter_clear (#8653)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | * 59345809 ggml : add and use ggml_cpu_has_llamafile()
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | de280085 examples : Fix `llama-export-lora` example (#8607)
| |_|_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | b841d074 server : fix URL.parse in the UI (#8646)
* | | | | | | | | | | | | | | 64cf50a0 sycl : Add support for non-release DPC++ & oneMKL (#8644)
* | | | | | | | | | | | | | | 938943cd llama : move vocab, grammar and sampling into separate files (#8508)
* | | | | | | | | | | | | | | 751fcfc6 Vulkan IQ4_NL Support (#8613)
* | | | | | | | | | | | | | | 46e47417 Allow all RDNA2 archs to use sdot4 intrinsic (#8629)
* | | | | | | | | | | | | | | e7e6487b contrib : clarify PR squashing + module names (#8630)
* | | | | | | | | | | | | | | 063d99ad [SYCL] fix scratch size of softmax (#8642)
| | | | | | | | | | | | | | | * fe28a7b9 llama : clean-up
| | | | | | | | | | | | | | | * dae3cae8 llama : suffix the internal APIs with "_impl"
| | | | | | | | | | | | | | | * 39fbaf9f llama : redirect external API to internal APIs
| | | | | | | | | | | | | | | * 66ac80f5 make : update llama.cpp deps [no ci]
| | | | | | | | | | | | | | | * 8fef5b18 llama : move tokenizers into llama-vocab
| | | | | | | | | | | | | | | * e7dffa6b llama : deprecate llama_sample_grammar
| | | | | | | | | | | | | | | * 689d3779 cont
| | | | | | | | | | | | | | | * b4b242e6 cont : pre-fetch rules
| | | | | | | | | | | | | | | * 5a71d1ae cont
| | | | | | | | | | | | | | | * 675f305f llama : move grammar code into llama-grammar
| | | | | | | | | | | | | | | * 0ddc8e36 llama : move sampling code into llama-sampling
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 081fe431 llama : fix codeshell support (#8599)
* | | | | | | | | | | | | | | d94c6e0c llama : add support for SmolLm pre-tokenizer (#8609)
* | | | | | | | | | | | | | | 566daa5a *.py: Stylistic adjustments for python (#8233)
* | | | | | | | | | | | | | | 6f11a83e llama : allow overrides for tokenizer flags (#8614)
* | | | | | | | | | | | | | | e093dd23 tests : re-enable tokenizer tests (#8611)
* | | | | | | | | | | | | | | 50e05353 llama : add Mistral Nemo inference support (#8604)
* | | | | | | | | | | | | | | 62815449 server : update doc to clarify n_keep when there is bos token (#8619)
* | | | | | | | | | | | | | | 04bab6b7 ggml: fix compile error for RISC-V (#8623)
* | | | | | | | | | | | | | | b7c11d36 examples: fix android example cannot be generated continuously (#8621)
* | | | | | | | | | | | | | | 45f2c19c flake.lock: Update (#8610)
| | | | | | | | | | | | | | | * 57349e1d llama : allow overrides for tokenizer flags
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 22f281aa examples : Rewrite pydantic_models_to_grammar_examples.py (#8493)
* | | | | | | | | | | | | | | 328884f4 gguf-py : fix some metadata name extraction edge cases (#8591)
* | | | | | | | | | | | | | | c69c6303 convert_hf : fix Gemma v1 conversion (#8597)
* | | | | | | | | | | | | | | 69c487f4 CUDA: MMQ code deduplication + iquant support (#8495)
* | | | | | | | | | | | | | | 07283b1a gguf : handle null name during init (#8587)
* | | | | | | | | | | | | | | 94036222 llama : add support for Tekken pre-tokenizer (#8579)
* | | | | | | | | | | | | | | 69b9945b llama.swiftui: fix end of generation bug (#8268)
* | | | | | | | | | | | | | | c3776cac gguf_dump.py: fix markddown kv array print (#8588)
| | | | | | | | | | | | | | | * 1932a1b8 gguf-py : do not use title case for naming convention
| | | | | | | | | | | | | | | * bf8e71b0 convert_lora : fix default filename
| | | | | | | | | | | | | | | * a3d154b2 gguf-py : add more name metadata extraction tests
| | | | | | | | | | | | | | | * 912e6fa5 gguf-py : more metadata edge cases fixes
| | | | | | | | | | | | | | | * 2164c9de gguf-py : fix some metadata name extraction edge cases
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | * c8ee1bcc Fix Vulkan matmul tests compile errors
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | * 50d1a035 convert_hf : fix Gemma v1 not setting BOS and EOS tokens
| | | | | | | | | | | | | | | * 5a9cb574 convert_hf : fix Gemma v1 conversion
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 87e397d0 ggml : fix quant dot product with odd number of blocks (#8549)
* | | | | | | | | | | | | | | 57b1d4f9 convert-*.py: remove add_name from ChatGLMModel class (#8590)
* | | | | | | | | | | | | | | d1975455 llama : bump max layers from 256 to 512 (#8530)
* | | | | | | | | | | | | | | be0cfb41 readme : fix server badge
* | | | | | | | | | | | | | | b57eb9ca ggml : add friendlier error message to fopen errors (#8575)
| | | | | | | | | | | | | | | * 38061254 gguf : handle null name during init
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | f299aa98 fix: typo of chatglm4 chat tmpl (#8586)
* | | | | | | | | | | | | | | 3d0e4367 convert-*.py: add general.name kv override (#8571)
* | | | | | | | | | | | | | | a15ef8f8 CUDA: fix partial offloading for ne0 % 256 != 0 (#8572)
* | | | | | | | | | | | | | | 705b7ecf cmake : install all ggml public headers (#8480)
* | | | | | | | | | | | | | | 0d2c7321 server: use relative routes for static files in new UI (#8552)
* | | | | | | | | | | | | | | 672a6f10 convert-*.py: GGUF Naming Convention Refactor and Metadata Override Refactor (#7499)
* | | | | | | | | | | | | | | 3807c3de server : respect `--special` cli arg (#8553)
* | | | | | | | | | | | | | | e02b597b lookup: fibonacci hashing, fix crashes (#8548)
* | | | | | | | | | | | | | | b3283448 build : Fix docker build warnings (#8535) (#8537)
* | | | | | | | | | | | | | | 30f80ca0 CONTRIBUTING.md : remove mention of noci (#8541)
* | | | | | | | | | | | | | | 1bdd8ae1 [CANN] Add Ascend NPU backend (#6035)
* | | | | | | | | | | | | | | da3913d8 batched: fix n_predict parameter (#8527)
* | | | | | | | | | | | | | | d65a8361 llama : disable context-shift for DeepSeek v2 (#8501)
| |_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 5e116e8d make/cmake: add missing force MMQ/cuBLAS for HIP (#8515)
* | | | | | | | | | | | | | 1666f92d gguf-hash : update clib.json to point to original xxhash repo (#8491)
* | | | | | | | | | | | | | 37b12f92 export-lora : handle help argument (#8497)
| | | | | | | | | | | | | | * f6ea7a09 llama : change fallback type IQ4_NL -> Q4_0
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 0efec577 llama : valign + remove unused ftype (#8502)
* | | | | | | | | | | | | | 7acfd4e8 convert_hf : faster lazy safetensors (#8482)
| | | | | | | | | | | | | | * b971122e convert_hf : fix memory leak in lazy MoE conversion
| | | | | | | | | | | | | | *   2a49a68d Merge branch 'master' into compilade/faster-lazy-safetensors
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 97bdd26e Refactor lora adapter support (#8332)
* | | | | | | | | | | | | | | 4db8f60f fix ci (#8494)
* | | | | | | | | | | | | | | 8fac431b ggml : suppress unknown pragma 'GCC' on windows (#8460)
* | | | | | | | | | | | | | | f17f39ff server: update README.md with llama-server --help output [no ci] (#8472)
* | | | | | | | | | | | | | | 9104bc20 common : add --no-cont-batching arg (#6358)
* | | | | | | | | | | | | | | fc690b01 docs: fix links in development docs [no ci] (#8481)
* | | | | | | | | | | | | | | 16bdfa42 [SYCL] add concat through dim 1/2 (#8483)
* | | | | | | | | | | | | | | 3dfda059 llama : de-duplicate deepseek2 norm
* | | | | | | | | | | | | | | bda62d79 Vulkan MMQ Fix (#8479)
* | | | | | | | | | | | | | | 090fca7a pydantic : replace uses of __annotations__ with get_type_hints (#8474)
| | | | | | | | | | | | | | * 7cda4dd7 convert_hf : faster lazy safetensors
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | aaab2419 flake.lock: Update (#8475)
* | | | | | | | | | | | | | 73cf442e llama : fix Gemma-2 Query scaling factors (#8473)
* | | | | | | | | | | | | | e236528e gguf_hash.py: Add sha256 (#8470)
* | | | | | | | | | | | | | fa79495b llama : fix pre-tokenization of non-special added tokens (#8228)
| | | | | | | | | | | | | | * f89eaa92 pydantic : fix Python 3.9 and 3.10 support
| | | | | | | | | | | | | | * eed299f0 pydantic : replace uses of __annotations__ with get_type_hints
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 17eb6aa8 vulkan : cmake integration (#8119)
* | | | | | | | | | | | | | c917b67f metal : template-ify some of the kernels (#8447)
* | | | | | | | | | | | | | 4e24cffd server : handle content array in chat API (#8449)
* | | | | | | | | | | | | | 6af51c0d main : print error on empty input (#8456)
* | | | | | | | | | | | | | f5322624 llama : suppress unary minus operator warning (#8448)
* | | | | | | | | | | | | | c3ebcfa1 server : ensure batches are either all embed or all completion (#8420)
* | | | | | | | | | | | | | 8a4441ea docker : fix filename for convert-hf-to-gguf.py in tools.sh (#8441)
* | | | | | | | | | | | | | 5aefbce2 convert : remove fsep token from GPTRefactForCausalLM (#8237)
* | | | | | | | | | | | | | 71c1121d examples : sprintf -> snprintf (#8434)
* | | | | | | | | | | | | | 370b1f7e ggml : minor naming changes (#8433)
* | | | | | | | | | | | | | b549a1bb [SYCL] fix the mul_mat_id ut issues (#8427)
* | | | | | | | | | | | | | 36864569 ggml : add NVPL BLAS support (#8329) (#8425)
* | | | | | | | | | | | | | b078c619 cuda : suppress 'noreturn' warn in no_device_code (#8414)
* | | | | | | | | | | | | | 808aba39 CUDA: optimize and refactor MMQ (#8416)
* | | | | | | | | | | | | | a977c115 gitignore : deprecated binaries
* | | | | | | | | | | | | | 9a55ffe6 tokenize : add --no-parse-special option (#8423)
* | | | | | | | | | | | | | 7a221b67 llama : use F32 precision in Qwen2 attention and no FA (#8412)
* | | | | | | | | | | | | | 278d0e18 Initialize default slot sampling parameters from the global context. (#8418)
| | | | | | | | | | | | | | * 59ce8531 test-tokenizer-random : reduce potential confilcts with #8379
| | | | | | | | | | | | | | * 1caa20fc convert_hf : reduce usages of UNKNOWN for InternLM2
| | | | | | | | | | | | | | *   afa61198 Merge branch 'master' into compilade/fix-mpt-pretok
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
| | | | | | | | | | | | | | * 98edea60 llama : add UNKNOWN tokens in the special tokens cache
| | | | | | | | | | | | | | * d4df7858 convert_hf : reduce usages of the UNKNOWN token type
| | | | | | | | | | | | | | * d6fe269c llama : fix command-r detokenization
| | | | | | | | | | | | | | * 31a1b0ee llama : fix Viking pre-tokenizer regex
| | | | | | | | | | | | | | * f9d42c59 convert_hf : identify more added control tokens for SPM tokenziers
| | | | | | | | | | | | | | * 6e351e04 convert_hf : identify which user-defined tokens are control tokens
| | | | | | | | | | | | | | * 56df1fcd llama : fix detection of control-like user-defined tokens
| | | | | | | | | | | | | | *   6b961e3d Merge branch 'master' into compilade/fix-mpt-pretok
| | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | * | d5d30b20 llama : pre-tokenize non-special user-defined tokens first
| | | | | | | | | | | | | | * |   ac0f33c9 Merge branch 'master' into compilade/fix-mpt-pretok
| | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | * | | db2ffd51 llama : fix mpt and olmo pre-tokenizer
| | | | | | | | | | | | | | | | | * ba06b2de tokenize : add --no-parse-special option
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | dd07a123 Name Migration: Build the deprecation-warning 'main' binary every time (#8404)
* | | | | | | | | | | | | | | | | f4444d99 [SYCL] Use multi_ptr to clean up deprecated warnings (#8256)
* | | | | | | | | | | | | | | | | 6b2a849d ggml : move sgemm sources to llamafile subfolder (#8394)
* | | | | | | | | | | | | | | | | 0f1a39f3 ggml : add AArch64 optimized GEMV and GEMM Q4 kernels (#5780)
* | | | | | | | | | | | | | | | | 83321c69 gguf-py rel pipeline (#8410)
* | | | | | | | | | | | | | | | | cc61948b llama : C++20 compatibility for u8 strings (#8408)
* | | | | | | | | | | | | | | | | 7a80710d msvc : silence codecvt c++17 deprecation warnings (#8395)
* | | | | | | | | | | | | | | | | a8be1e6f llama : add assert about missing llama_encode() call (#8400)
* | | | | | | | | | | | | | | | | e4dd31ff py : fix converter for internlm2 (#8321)
* | | | | | | | | | | | | | | | | 8f0fad42 py : fix extra space in convert_hf_to_gguf.py (#8407)
| | | | | | | | | | | | | | | | | * 117f7adb ggml : remove K_QUANTS_PER_ITERATION (#8306)
| | | | | | | | | | | | | | | | | * 91deef46 py : rename requirements for convert_legacy_llama.py
| | | | | | | | | | | | | | | | | * 902de882 gguf-py : use snake_case in scripts entrypoint export
| | | | | | | | | | | | | | | | | * 3e3cc710 cont : fix link
| | | | | | | | | | | | | | | | | * c172b322 cont
| | | | | | | | | | | | | | | | | * d8f2da6b cont
| | | | | | | | | | | | | | | | | * 39a41a53 py : switch to snake_case
| | | | | | | | |_|_|_|_|_|_|_|_|/  
| | | | | | | |/| | | | | | | | |   
| | | | | | | | | | | | | | | | | * ff137fbb Bump patch version for release
| | | | | | | | | | | | | | | | | * f6a33217 Upd gguf-py/readme
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | a59f8fdc Server: Enable setting default sampling parameters via command-line (#8402)
* | | | | | | | | | | | | | | | | fd560fe6 Update README.md to fix broken link to docs (#8399)
* | | | | | | | | | | | | | | | | e500d613 Deprecation warning to assist with migration to new binary names (#8283)
* | | | | | | | | | | | | | | | | a03e8dd9 make/cmake: LLAMA_NO_CCACHE -> GGML_NO_CCACHE (#8392)
* | | | | | | | | | | | | | | | | 5b0b8d8c sycl : Reenabled mmvq path for the SYCL Nvidia Backend (#8372)
* | | | | | | | | | | | | | | | | 9925ca40 cmake : allow external ggml (#8370)
* | | | | | | | | | | | | | | | | 9beb2dda readme : fix typo [no ci] (#8389)
* | | | | | | | | | | | | | | | | 7d0e23d7 gguf-py : do not use internal numpy types (#7472)
| | | | | | | | | | | | | | | | | *   aaf7bc89 Merge branch 'master' into compilade/gguf-py-fix-old-numpy
| | | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | | 7fdb6f73 flake.lock: Update (#8342)
* | | | | | | | | | | | | | | | | | a130ecce labeler : updated sycl to match docs and code refactor (#8373)
* | | | | | | | | | | | | | | | | | c4dd11d1 readme : fix web link error [no ci] (#8347)
* | | | | | | | | | | | | | | | | | 2ec846d5 sycl : fix powf call in device code (#8368)
* | | | | | | | | | | | | | | | | | 3f2d538b scripts : fix sync for sycl
* | | | | | | | | | | | | | | | | | 2ee44c9a sync : ggml
* | | | | | | | | | | | | | | | | | 6847d54c tests : fix whitespace (#0)
* | | | | | | | | | | | | | | | | | fde13b3b feat: cuda implementation for `ggml_conv_transpose_1d` (ggml/854)
* | | | | | | | | | | | | | | | | | 470939d4 common : preallocate sampling token data vector (#8363)
* | | | | | | | | | | | | | | | | | 6f0dbf6a infill : assert prefix/suffix tokens + remove old space logic (#8351)
* | | | | | | | | | | | | | | | | | ffd00797 common : avoid unnecessary logits fetch (#8358)
* | | | | | | | | | | | | | | | | | 04ce3a8b readme : add supported glm models (#8360)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 3fd62a6b py : type-check all Python scripts with Pyright (#8341)
* | | | | | | | | | | | | | | | | a8db2a9c Update llama-cli documentation (#8315)
* | | | | | | | | | | | | | | | | 4090ea55 ci : add checks for cmake,make and ctest in ci/run.sh (#8200)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | * 8334b5be gguf-py : do not use internal numpy types
| | | | | | | | | | | | | | | | * 86ccd309 ci : only show warnings and errors in python type-check
| | | | | | | | | | | | | | | | * 6ec70c93 tests : fix test-tokenizer-random.py
| | | | | | | | | | | | | | | | * 6f215f1f py : fix new type errors from master branch
| | | | | | | | | | | | | | | | *   0caf60a7 Merge branch 'master' into compilade/pyright-tests
| | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | f1948f1e readme : update bindings list (#8222)
* | | | | | | | | | | | | | | | | f7cab35e gguf-hash: model wide and per tensor hashing using xxhash and sha1 (#8048)
* | | | | | | | | | | | | | | | | 905942ab llama : support glm3 and glm4 (#8031)
* | | | | | | | | | | | | | | | | b5040086 llama : fix n_rot default (#8348)
* | | | | | | | | | | | | | | | | d39130a3 py : use cpu-only torch in requirements.txt (#8335)
* | | | | | | | | | | | | | | | | b81ba1f9 finetune: Rename command name in README.md (#8343)
* | | | | | | | | | | | | | | | | 210eb9ed finetune: Rename an old command name in finetune.sh (#8344)
* | | | | | | | | | | | | | | | | cb4d86c4 server: Retrieve prompt template in /props (#8337)
* | | | | | | | | | | | | | | | | 86e7299e added support for Authorization Bearer tokens when downloading model (#8307)
* | | | | | | | | | | | | | | | | 60d83a01 update main readme (#8333)
| | | | | | | | | | | | | | | | * 872aecbf ci : disable pip cache in type-check workflow
| | | | | | | | | | | | | | | | * 60c39aca server-tests : model metadata is a dict
| | | | | | | | | | | | | | | | * 959c057b server-tests : strip "chat" from base_url in oai_chat_completions
| | | | | | | | | | | | | | | | * 71b50a14 server-tests : add more type annotations
| | | | | | | | | | | | | | | | * fbf4a858 server-tests : use trailing slash in openai base_url
| | | | | | | | | | | | | | | | * e29fd963 py : type-check all Python scripts with Pyright
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | | * a44f22e7 py : use cpu-only torch in requirements.txt
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 87e25a1d llama : add early return for empty range (#8327)
* | | | | | | | | | | | | | | | 213701b5 Detokenizer fixes (#8039)
* | | | | | | | | | | | | | | | be20e7f4 Reorganize documentation pages (#8325)
* | | | | | | | | | | | | | | | 7ed03b89 llama : fix compile warning (#8304)
* | | | | | | | | | | | | | | | 1d894a79 cmake : add GGML_BUILD and GGML_SHARED macro definitions (#8281)
* | | | | | | | | | | | | | | | 1f3e1b66 Enabled more data types for oneMKL gemm_batch (#8236)
* | | | | | | | | | | | | | | | 148ec970 convert : remove AWQ remnants (#8320)
* | | | | | | | | | | | | | | | 2cccbaa0 llama : minor indentation during tensor loading (#8304)
* | | | | | | | | | | | | | | | 8e558309 CUDA: MMQ support for iq4_nl, iq4_xs (#8278)
* | | | | | | | | | | | | | | | 0a423800 CUDA: revert part of the RDNA1 optimizations (#8309)
* | | | | | | | | | | | | | | | d12f7810 llama : streamline embeddings from "non-embedding" models (#8087)
* | | | | | | | | | | | | | | | bcefa03b CUDA: fix MMQ stream-k rounding if ne00 % 128 != 0 (#8311)
* | | | | | | | | | | | | | | | 5a7447c5 readme : fix minor typos [no ci] (#8314)
* | | | | | | | | | | | | | | | 61ecafa3 passkey : add short intro to README.md [no-ci] (#8317)
* | | | | | | | | | | | | | | | aa5898dc llama : prefer n_ over num_ prefix (#8308)
* | | | | | | | | | | | | | | | 6c05752c contributing : update guidelines (#8316)
* | | | | | | | | | | | | | | | a9554e20 [SYCL] Fix WARP_SIZE=16 bug of Intel GPU (#8266)
* | | | | | | | | | | | | | | | e235b267 py : switch to snake_case (#8305)
* | | | | | | | | | | | | | | | f09b7cb6 rm get_work_group_size() by local cache for performance (#8286)
| |_|_|_|_|_|_|_|/ / / / / / /  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | a38b884c cli: add EOT when user hit Ctrl+C (#8296)
| |_|_|_|_|_|/ / / / / / / /  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | d7fd29ff llama : add OpenELM support (#7359)
* | | | | | | | | | | | | | 6f63d646 tokenize : add --show-count (token) option (#8299)
| | | | | | | | | | | | | | * f55b6473 llama : minor indentation during tensor loading
| | | | | | | | | | | | | | * 18e92879 llama : fix t5 uses of n_head and n_ff
| | | | | | | | | | | | | | *   c6ac1984 Merge branch 'master' into openelm
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 51d2ebad build: Export hf-to-gguf as snakecase
* | | | | | | | | | | | | | | 1e920018 doc: Add context for why we add an explicit pytorch source
* | | | | | | | | | | | | | | 01a5f065 chore: Remove rebase artifacts
* | | | | | | | | | | | | | | 07786a61 chore: Fixup requirements and build
* | | | | | | | | | | | | | | de14e2ea chore: ignore all __pychache__
* | | | | | | | | | | | | | | 82192291 fix: Update script paths in CI scripts
* | | | | | | | | | | | | | | b1c3f26e fix: Actually include scripts in build
* | | | | | | | | | | | | | | b0a46993 build(python): Package scripts with pip-0517 compliance
| | | | | | | | | | | | | | * 269e07bb llama : use const ref for print_f and fix division by zero
| | | | | | | | | | | | | | *   199d0fb0 Merge branch 'master' into pr/7359
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 807b0c49 Inference support for T5 and FLAN-T5 model families (#5763)
| | | | | | | | | | | | | | * 3fe395d2 llama : handle n_head == 0
| | | | | | | | | | | | | | *   22a648f8 Merge branch 'master' into pr/7359
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | f8c4c073 tests : add _CRT_SECURE_NO_WARNINGS for WIN32 (#8231)
* | | | | | | | | | | | | | | 402d6fef llama : suppress unref var in Windows MSVC (#8150)
* | | | | | | | | | | | | | | 20fc3804 convert : fix gemma v1 tokenizer convert (#8248)
* | | | | | | | | | | | | | | f6190247 [SYCL] Remove unneeded semicolons (#8280)
* | | | | | | | | | | | | | | d23287f1 Define and optimize  RDNA1 (#8085)
* | | | | | | | | | | | | | | 5f2d4e60 ppl : fix n_seq_max for perplexity (#8277)
| | | | | | | | | | | | | | * 9971c38a llama : do not print hparams for vocab-only models
| | | | | | | | | | | | | | * b59ddf94 llama : fix save/load state
| | | | | | | | | | | | | | * 29ab5a0e llama : use std::array for per-layer hparams
| | | | | | | | | | | | | | * e3e33c0c llama : minor spacing changes
| | | | | | | | | | | | | | * c8cdb48d llama : support all OpenELM models
| | | | | | | | | | | | | | *   51b2577d Merge branch 'master' into openelm
| | | | | | | | | | | | | | |\  
| | | | | | | |_|_|_|_|_|_|_|/  
| | | | | | |/| | | | | | | |   
| | | | | | | | | | | | | | * 60b2e1b9 fixup! Initial OpenELM support (270M only so far)
| | | | | | | | | | | | | | * aaabe2e3 Fill out missing entries in llama_model_type_name
| | | | | | | | | | | | | | * 217d8d7b Initial OpenELM support (270M only so far)
| | | | | | | | | | | | | | | * dcab343f use 1 seq for kl_divergence
| | | | | | | | | | | | | | | * 5cf23d11 ppl : fix n_seq_max for perplexity
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 916248af fix phi 3 conversion (#8262)
* | | | | | | | | | | | | | | f8d6a238 fix typo (#8267)
* | | | | | | | | | | | | | | fadde671 Dequant improvements rebase (#8255)
* | | | | | | | | | | | | | | a27152b6 fix: add missing short command line argument -mli for multiline-input (#8261)
* | | | | | | | | | | | | | | 3e2618bc Adding step to `clean` target to remove legacy binary names to reduce upgrade / migration confusion arising from #7809. (#8257)
* | | | | | | | | | | | | | | 07a3fc06 Removes multiple newlines at the end of files that is breaking the editorconfig step of CI. (#8258)
* | | | | | | | | | | | | | | 96896737 Add `JAIS` model(s) (#8118)
* | | | | | | | | | | | | | | 023b8807 convert-hf : print output file name when completed (#8181)
* | | | | | | | | | | | | | | 0e0590ad cuda : update supports_op for matrix multiplication (#8245)
* | | | | | | | | | | | | | | a9f3b102 [SYCL] Fix win build conflict of math library (#8230)
* | | | | | | | | | | | | | | d08c20ed [SYCL] Fix the sub group size of Intel (#8106)
* | | | | | | | | | | | | | | 5fac350b Fix gemma2 tokenizer convert (#8244)
* | | | | | | | | | | | | | | cb5fad4c CUDA: refactor and optimize IQ MMVQ (#8215)
* | | | | | | | | | | | | | | dae57a1e readme: add Paddler to the list of projects (#8239)
* | | | | | | | | | | | | | | 49122a87 gemma2: add sliding window mask (#8227)
* | | | | | | | | | | | | | | 0ddeff10 readme : update tool list (#8209)
* | | | | | | | | | | | | | | 3840b6f5 nix : enable curl (#8043)
* | | | | | | | | | | | | | | 257f8e41 nix : remove OpenCL remnants (#8235)
* | | | | | | | | | | | | | | 694c59cb Document BERT support. (#8205)
* | | | | | | | | | | | | | | 197fe6c1 [SYCL] Update SYCL-Rope op and Refactor (#8157)
| | | | | | | | | | | | | | | * 703764a3 convert : use non-fast T5 tokenizer
| | | | | | | | | | | | | | | * 17bb0eae llama : UGM tokenizer init with UNK tokens instead of PAD
| | | | | | | | | | | | | | | * 9eb5d561 convert : add t5 tokenizer tests
| | | | | | | | | | | | | | | * 6dc9eb40 llama : quantization-related fixes for T5
| | | | | | | | | | | | | | | * 7d7fff46 llama : whitespace formatting
| | | | | | | | | | | | | | | *   7293243d Merge remote-tracking branch 'upstream/master' into t5-clean-3
| | | | | | | | | | | | | | | |\  
| | | | | | | | |_|_|_|_|_|_|_|/  
| | | | | | | |/| | | | | | | |   
| | | | | | | | | | | | | | | * c4ded1a8 llama : make pos_bias contiguous for CUDA
| | | | | | | | | | | | | | | * bad0cafe llama : updated llm_build_ffn() calls to new API in build_t5()
| | | | | | | | | | | | | | | *   1c8d37a2 Merge branch 'ggerganov:master' into t5-clean-3
| | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | * | 45681a57 llama : add inference support and model types for T5 and FLAN-T5 model families
| | | | | | | | | | | | | | | | | * d4a1923d minor : remove parentheses
| | | | | | | | | | | | | | | | | * 32cd6f57 nix : remove OpenCL remnants
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | d0a7145b flake.lock: Update (#8218)
| |_|_|_|_|/ / / / / / / / / / /  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 9ef07800 Fix new line issue with chat template, disable template when in-prefix/suffix is set (#8203)
| |_|_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 1c5eba6f llama: Add attention and final logit soft-capping, update scaling factor to Gemma2 (#8197)
| | | | | | | | | | | | | | | * 51f0bd50 Remove custom pre attention scaling and use computed value instead.
| | | | | | | | | | | | | | | * a8942790 Add custom kq scaling from Gemma2Attention
| | | | | | | | | | | | | | | *   6f2464e3 Merge branch 'add-gemma2-soft-capping' of github.com:ggerganov/llama.cpp into add-gemma2-soft-capping
| | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | * 3a247181 Update src/llama.cpp
| | | | | | | | | | | | | | | * | bb715992 Add default value for attention and final logit softcap value
| | | | | | | | | | | | | | | * |   8edf73a7 Merge branch 'master' of github.com:ggerganov/llama.cpp into add-gemma2-soft-capping
| | | | | | | | | | | | | | | |\ \  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | | | | | /   
| | | | | | | | | | | | | | | |/    
* | | | | | | | | | | | | | | | 72272b83 fix code typo in llama-cli (#8198)
| | | | | | | | | | | | | | | * f4424c15 Disable flash attention for Gemma2
| | | | | | | | | | | | | | | * d1137c20 Add custom add_ functions
| | | | | | | | | | | | | | | * d3d3c4eb fix
| | | | | | | | | | | | | | | * 4d3f17b4 Add attention and final logit softcapping.
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 8748d8ac json: attempt to skip slow tests when running under emulator (#8189)
* | | | | | | | | | | | | | | 26a39bbd Add MiniCPM, Deepseek V2 chat template + clean up `llama_chat_apply_template_internal` (#8172)
| | | | | | | | | | | | | | | * 712e4d94 Generate full token count during warm up
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 38373cfb Add SPM infill support (#8016)
* | | | | | | | | | | | | | | b851b3fb cmake : allow user to override default options (#8178)
* | | | | | | | | | | | | | | 139cc621 `json`: restore default additionalProperties to false, fix some pattern escapes (#8180)
* | | | | | | | | | | | | | | e57dc620 llama: Add support for Gemma2ForCausalLM (#8156)
* | | | | | | | | | | | | | | a27aa50a Add missing items in makefile (#8177)
* | | | | | | | | | | | | | | cb0b06a8 `json`: update grammars/README w/ examples & note about additionalProperties (#8132)
* | | | | | | | | | | | | | | 558f44bf CI: fix release build (Ubuntu+Mac) (#8170)
* | | | | | | | | | | | | | | 8172ee9d cmake : fix deprecated option names not working (#8171)
* | | | | | | | | | | | | | | 16791b8f Add chatml fallback for cpp `llama_chat_apply_template` (#8160)
* | | | | | | | | | | | | | | ab367911 flake.lock: Update (#8071)
* | | | | | | | | | | | | | | 97877eb1 Control vector loading fixes (#8137)
* | | | | | | | | | | | | | | 38795265 Delete examples/llama.android/llama/CMakeLists.txt (#8165)
* | | | | | | | | | | | | | | 6030c612 Add Qwen2MoE 57B-A14B model identifier (#8158)
* | | | | | | | | | | | | | | 85a267da CUDA: fix MMQ stream-k for --split-mode row (#8167)
* | | | | | | | | | | | | | | f675b20a Added support for Viking pre-tokenizer (#8135)
* | | | | | | | | | | | | | | 911e35bb llama : fix CodeLlama FIM token checks (#8144)
| |_|_|_|_|/ / / / / / / / /  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | ac146628 Fix llama-android.cpp for error - "common/common.h not found" (#8145)
* | | | | | | | | | | | | | 9b31a40c clip : suppress unused variable warnings (#8105)
* | | | | | | | | | | | | | c70d117c scripts : fix filename sync
* | | | | | | | | | | | | | ae5d0f4b ci : publish new docker images only when the files change (#8142)
* | | | | | | | | | | | | | 31ec3993 ggml : add GGML_CUDA_USE_GRAPHS option, restore GGML_CUDA_FORCE_CUBLAS (cmake) (#8140)
* | | | | | | | | | | | | | c7ab7b61 make : fix missing -O3 (#8143)
* | | | | | | | | | | | | | f2d48fff sync : ggml
* | | | | | | | | | | | | | 4713bf30 authors : regen
* | | | | | | | | | | | | | 0e814dfc devops : remove clblast + LLAMA_CUDA -> GGML_CUDA (#8139)
* | | | | | | | | | | | | | a95631ee readme : update API notes
| | | | | | | | | | | | | | * 65f9293d devops : remove clblast + LLAMA_CUDA -> GGML_CUDA
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | f3f65429 llama : reorganize source code + improve CMake (#8006)
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
| | | | | | | | | | | | | * 1e6e363d test zero max buffer size
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 88540445 Clarify default MMQ for CUDA and LLAMA_CUDA_FORCE_MMQ flag (#8115)
* | | | | | | | | | | | | c8771ab5 CUDA: fix misaligned shared memory read (#8123)
* | | | | | | | | | | | | 494165f3 llama : extend llm_build_ffn() to support _scale tensors (#8103)
* | | | | | | | | | | | | 9b2f16f8 `json`: better support for "type" unions (e.g. nullable arrays w/ typed items) (#7863)
* | | | | | | | | | | | | 6777c544 `json`: fix additionalProperties, allow space after enum/const (#7840)
* | | | | | | | | | | | | 163d50ad fixes #7999 (adds control vectors to all `build_XXX()` functions in `llama.cpp` [needs testing] (#8060)
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 6fcbf682 llama : implement Unigram tokenizer needed by T5 and FLAN-T5 model families (#5763)
* | | | | | | | | | | | e6bf0077 llama : return nullptr from llama_grammar_init (#8093)
* | | | | | | | | | | | 84631fe1 `json`: support integer minimum, maximum, exclusiveMinimum, exclusiveMaximum (#7797)
* | | | | | | | | | | | dd047b47 disable docker CI on pull requests (#8110)
* | | | | | | | | | | | 925c3095 Add healthchecks to llama-server containers (#8081)
* | | | | | | | | | | | c8ad3595 Gguf dump start data offset via --data-offset and some extra refactor (#8054)
* | | | | | | | | | | | 49c03c79 cvector: better prompt handling, add "mean vector" method (#8069)
* | | | | | | | | | | | 48e6b92c Add chat template support for llama-cli (#8068)
* | | | | | | | | | | | 3791ad21 SimpleChat v3.1: Boolean chat request options in Settings UI, cache_prompt (#7950)
* | | | | | | | | | | | f702a90e Update control vector help (#8104)
* | | | | | | | | | | | 083bacce [SYCL] Re-enabled mul_mat_batched_sycl (#8095)
* | | | | | | | | | | | 2df373ac CUDA: fix matrix multiplication algorithm choice (#8102)
* | | | | | | | | | | | 3b099bcd CUDA: fix MMQ writeback for int8 tensor cores (#8100)
* | | | | | | | | | | | a818f302 CUDA: use MMQ instead of cuBLAS by default (#8075)
* | | | | | | | | | | | d62e4aaa gguf-py : fix tensor groups for encoder-decoder models in gguf-dump.py (#8090)
* | | | | | | | | | | | 9a590c82 CUDA: optimize MMQ int8 tensor core performance (#8062)
* | | | | | | | | | | | 52fc8705 Option to split during conversion (#6942)
* | | | | | | | | | | | 8cb508d0 disable publishing the full-rocm docker image (#8083)
* | | | | | | | | | | | 646ef4a9 embedding : more cli arguments (#7458)
* | | | | | | | | | | | de0d6a68 gguf-py, convert-hf : model conversion support for T5 and FLAN-T5 model variants (#5763)
| |_|_|_|_|_|_|_|/ / /  
|/| | | | | | | | | |   
* | | | | | | | | | | 95f57bb5 ggml : remove ggml_task_type and GGML_PERF (#8017)
* | | | | | | | | | | e112b610 llama : add support for BitnetForCausalLM (#7931)
* | | | | | | | | | | 6a2f298b server : fix JSON-Scheme typo (#7975)
* | | | | | | | | | | 11318d9a Fix typo in llama_set_embeddings comment (#8077)
* | | | | | | | | | | b6b9a8e6 fix CI failures (#8066)
* | | | | | | | | | | 45c0e2e4 Refactor Vulkan backend to allow multiple contexts (#7961)
* | | | | | | | | | | b5a5f34e Removing extra blank lines that were breaking Lint. (#8067)
* | | | | | | | | | | 3e58b0ee cvector: fix CI + correct help message (#8064)
* | | | | | | | | | | adf480c3 cvector-generator: Moe Moe Fixie-Fixie for Lots of Formats~! ♡(ᐢ ᴥ ᐢ)♡ (#8052)
* | | | | | | | | | | 3aa184a8 convert-hf : change assert to exception (#8015)
* | | | | | | | | | | 5b48cd53 Update llama-quantize ppl/file size output from LLaMA-v1 to Llama-3 values (#8058)
* | | | | | | | | | | c5a8d4b7 JSON Schema to GBNF integration tests (#7790)
* | | | | | | | | | | 557b653d vulkan: detect multiple devices by deviceUUID instead of deviceID (#8022)
* | | | | | | | | | | 7d5e8777 ggml : AVX IQ quants (#7845)
* | | | | | | | | | | a927b0f3 llama : optimize long word tokenization with WPM (#8034)
* | | | | | | | | | | 80ea089d llama : allow pooled embeddings on any model (#7477)
* | | | | | | | | | | 0e64591e swiftui : enable stream updating (#7754)
| | | | | | | | | | | * ff0aa3ab fix part of mul_mat_id
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | b1ef562b requirements : Bump torch and numpy for python3.12 (#8041)
* | | | | | | | | | | 17b291a6 convert-hf : Fix the encoding in the convert-hf-to-gguf-update.py (#8040)
* | | | | | | | | | | abd894ad common: fix warning (#8036)
* | | | | | | | | | | de391e4c [SYCL] Fix windows build and inference (#8003)
* | | | | | | | | | | d50f8897 CUDA: stream-k decomposition for MMQ (#8018)
* | | | | | | | | | | 2075a66a metal : fix `ggml_metal_supports_op` for BF16 (#8021)
* | | | | | | | | | | ba589931 server : fix smart slot selection (#8020)
* | | | | | | | | | | a7854743 un-ignore `build-info.cmake` and `build-info.sh` (#7996)
* | | | | | | | | | | 9c77ec1d ggml : synchronize threads using barriers (#7993)
* | | | | | | | | | | a04a953c codecov : remove (#8004)
* | | | | | | | | | | 623494a4 [SYCL] refactor (#6408)
* | | | | | | | | | | 37bef894 tokenizer : BPE fixes (#7530)
* | | | | | | | | | | 91c188d6 Only use FIM middle token if it exists (#7648)
* | | | | | | | | | | 84f6de17 Fix no gcc pragma on Windows (#7751)
* | | | | | | | | | | 61665277 Allow compiling with CUDA without CUDA runtime installed (#7989)
* | | | | | | | | | | b96f9afb chore: clean useless beam search param (#7985)
* | | | | | | | | | | 11937781 readme : update UI list (#7943)
* | | | | | | | | | | 5326bcce ggml : sync
* | | | | | | | | | | e6ecc2be whisper : use ggml_backend_sched (whisper/2239)
* | | | | | | | | | | a94e6ff8 update: support Qwen2-57B-A14B (#7835)
* | | | | | | | | | | 5b6da187 Make updates to type cast based on compiler instead of OS (#7851)
* | | | | | | | | | | 7c26775a llama : disable FA if KV head size do not match (#7982)
| | | | | | | | | | | * f3974cab all matrix multiplication backend
| | | | | | | | | | | | * ce6e28cc Update ggml-sycl.cpp
| | | | | | | | | | | | * 6f661257 Revert "Minor arithmetic improvement to mmvq wrapper kernel (#7172)"
| | | | | | | | | | | | | * ef79941a llama : disable FA if KV head size do not match
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | b473e950 Add Nix and Flox install instructions (#7899)
* | | | | | | | | | | | | 99052cd2 sched : offload_op also requires supports_op (#7977)
* | | | | | | | | | | | | c637fcd3 fix: divide 0 exception in mamba (#7932)
* | | | | | | | | | | | | 6a2f0b34 Implement non-mapped async IO for CUDA on Windows.  (#7896)
* | | | | | | | | | | | | 21be9cab rpc : fix load/store misaligned addresses (#7948)
* | | | | | | | | | | | | 006167aa gguf-dump.py: add --markdown dump output (#7853)
* | | | | | | | | | | | | df68d4fa [SYCL] Update README-sycl.md for Chapter "Recommended release" and "News" (#7946)
* | | | | | | | | | | | | 43b35e38 Add support for sqrt on CUDA (#7953)
* | | | | | | | | | | | | 19b7a836 cuda : fix bounds check for src0 rows in MMVQ kernel (whisper/2231)
* | | | | | | | | | | | | b5fcf8ef ggml : fix and optimize ppc64le (ggml/849)
* | | | | | | | | | | | | 398105ff ggml : remove duplicate include of ggml-common.h (ggml/853)
* | | | | | | | | | | | | bc6c457f flake.lock: Update (#7951)
* | | | | | | | | | | | | 52399254 unicode : avoid char32_t (#7957)
* | | | | | | | | | | | | 6fe1c627 readme : update UI list [no ci] (#7958)
* | | | | | | | | | | | | cddaf028 ggml : fix handling of zero blocks in IQ quants (#7955)
| | | | | | | | | | | | | * a235b7c5 Vectorize q load
| | | | | | | | | | | | | * 604ef6bf Store scales in local mem
| | | | | | | | | | | | | * cb3fb420 Single load for half2
| | | | | | | | | | | | | * 4a481556 Remove double lines
| | | | | | | | | | | | | *   ff076b88 Merge pull request #7920 from ggerganov/codeplay/revert-host-alloc
| | | | | | | | | | | | | |\  
| | | | | | | | | | | | | |/  
| | | | | | | | | | | | |/|   
| | | | | | | | | | | | * | 18133cab Revert "use the correct SYCL context for host USM allocations"
| | | | | | | | | | | | | *   b2c8c831 Merge pull request #7919 from ggerganov/codeplay/unify-rope-sycl
| | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | * ded54b5d Replace powf with sycl::pow in ggml-sycl.cpp
| | | | | | | | | | | | | |/  
| | | | | | | | | | | | |/|   
| | | | | | | | | | | | * | abd7c7b8 Formatting
| | | | | | | | | | | | * | 0c0f3f00 [SYCL] Update unsupported ops
| | | | | | | | | | | | * | 9b81b572 [SYCL] unify rope norm/neox
| | | | | | | | | | | | |/  
| | | | | | | | | | | | | * 98f948b9 unicode : avoid char32_t
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | c8a82194 github : update pr template
| | | | | | | | | | | | | * 28f7a4d0 ggml : fix handling of zero blocks in IQ quants
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 7c7836d9 Vulkan Shader Refactor, Memory Debugging Option (#7947)
* | | | | | | | | | | | | 0c7b3595 Add `cvector-generator` example (#7514)
| | | | | | | | | | | | | * e9f2abfc bitnet : pad tensors to 256
| | | | | | | | | | | | | * 569a03ed finish i2_s/i8_s vec_dot x86 simd
| | | | | | | | | | | | | * 95dced07 i2_s to absmax
| | | | | | | | | | | | | * 7a8961ff delete redundant
| | | | | | | | | | | | | * 5e5eee7b fix whitespace
| | | | | | | | | | | | | * f395dd9c change table name
| | | | | | | | | | | | | *   c0cd08d4 Merge branch 'ggerganov:master' into bitnet
| | | | | | | | | | | | | |\  
| | | | | | | | | | | | | * \   2322e9db Merge branch 'ggerganov:master' into bitnet
| | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | * | | de1d5073 remove unused
| | | | | | | | | | | | | * | | c0fd4df8 fix merge
| | | | | | | | | | | | | * | |   841c903f Merge branch 'ggerganov:master' into bitnet
| | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | * | | | abd798d7 fix code
| | | | | | | | | | | | | * | | | 65ac3a36 fix
| | | | | | | | | | | | | * | | | 344467f2 fix code
| | | | | | | | | | | | | * | | | 97d22be5 fix codestyle
| | | | | | | | | | | | | * | | | 3a0f8b06 clean code 2
| | | | | | | | | | | | | * | | | 1c5a8b7f clean code
| | | | | | | | | | | | | * | | | dbee0a86 move i2 to quantize
| | | | | | | | | | | | | * | | | ca090855 move i2s to quantize v1
| | | | | | | | | | | | | * | | | 4e1ab506 finish bitnet i2 e2e
| | | | | | | | | | | | | * | | | 2a01a7ce remove unsed
| | | | | | | | | | | | | * | | | 5e596601 finish f16 hf bitnet e2e
| | | | | | | | | | | | | * | | | 1f2e0ee0 finish bitnet e2e
| | | | | | | | | | | | | * | | | 57dfc3bc hf bitnet e2e v2
| | | | | | | | | | | | | * | | | 076b4a19 hf bitnet v1
| | | | | | | | | | | | | | | | | * 34bdbed4 rpc : fix load/store misaligned addresses
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 7b2f4a7d [SYCL] remove global variables (#7710)
| |_|_|_|_|_|_|_|_|_|/ / / / / /  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | f8ec8877 ci : fix macos x86 build (#7940)
* | | | | | | | | | | | | | | | 76d66ee0 CUDA: faster q2_K, q3_K MMQ + int8 tensor cores (#7921)
* | | | | | | | | | | | | | | | 66ef1cee metal : utilize max shared memory for mul_mat_id (#7935)
* | | | | | | | | | | | | | | | e65bbf60 llama-bench : fix RPC indication (#7936)
* | | | | | | | | | | | | | | | 6fcd1331 llama : more checks before assuming FIM tokens (#7644)
* | | | | | | | | | | | | | | | 41b9260f convert : add Poro-34B-chat tokenizer support (#7713)
| | | | | | | | | | | | | | | | * eaf34ba0 metal : utilize max shared memory for mul_mat_id
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 172c8256 rpc : fix ggml_backend_rpc_supports_buft() (#7918)
* | | | | | | | | | | | | | | | a55eb1bf readme : Remove outdated instructions from README.md (#7914) [no ci]
* | | | | | | | | | | | | | | | f578b86b move BLAS to a separate backend (#6210)
* | | | | | | | | | | | | | | | 1c641e6a `build`: rename main → llama-cli, server → llama-server, llava-cli → llama-llava-cli, etc... (#7809)
| |_|_|_|/ / / / / / / / / / /  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 96355290 CUDA: fix broken oob check for FA vec f32 kernel (#7904)
| |_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | |   
| | | | | | | | | | | | | | * 46325233 Revert 7777
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | a9cae480 tests : add non-cont unary tests (#7857)
| | | | | | | | | | | | | | * 8412561c ggml : update unary asserts and "supports_op"
| | | | | | | | | | | | | | * ebf95c22 tests : add non-cont unary tests
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | bfaa676b ggml : improve ggml_is_contiguous logic (#7856)
* | | | | | | | | | | | | | 704a35b1 server : restore numeric prompts (#7883)
* | | | | | | | | | | | | | dcf75270 update intel docker oneapi-basekit to 2024.1.1-devel-ubuntu22.04 (#7894)
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | f2b5764b Fix a typo and add Fedora 40 pacakge to install for Vulkan (#7794) [no ci]
* | | | | | | | | | | | | 73bac2b1 vulkan: select only one device for single gpu with multiple drivers (#7582)
* | | | | | | | | | | | | ef52d1d1 Update Vulkan RoPE implementation (#7818)
* | | | | | | | | | | | | 14f83526 fix broken link in pr template (#7880) [no ci]
* | | | | | | | | | | | | 6fe42d07 github: move PR template to .github/ root (#7868)
* | | | | | | | | | | | | 148995e5 llama-bench: more compact markdown tables (#7879)
* | | | | | | | | | | | | 4bfe50f7 tests : check the Python version (#7872)
* | | | | | | | | | | | | bdcb8f42 CUDA: int8 tensor cores for MMQ (q4_K, q5_K, q6_K) (#7860)
| | | | | | | | | | | | | * cd026b48 ggml : support more contiguous cases
| | | | | | | | | | | | | * f2a029bd ggml : improve ggml_is_contiguous logic
| | | | | | | | | | | | | | * 4356325e tests : check the Python version
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | c2ce6c47 fix CUDA CI by using a windows-2019 image (#7861)
| |_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | b61eb964 json: refine constraint for whitespace to avoid runaways yet allow pretty print (#7866)
* | | | | | | | | | | | | 396b18df `json`: document schema conversion in GBNF readme, align manual grammar examples & converters (#7841)
* | | | | | | | | | | | | 864a99e7 cmake : fix CMake requirement for CUDA (#7821)
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | fd5ea0f8 ci : try win-2019 on server windows test (#7854)
* | | | | | | | | | | | c28a8390 examples : remove --instruct remnants (#7846)
* | | | | | | | | | | | d9da0e49 server : improve "prompt" handling (#7847)
* | | | | | | | | | | | 1f0dabda CUDA: use tensor cores for MMQ (#7676)
* | | | | | | | | | | | af4ae502 use the correct SYCL context for host USM allocations (#7777)
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
| | | | | | | | | | | * 4bb03cad ci : disable server-windows workflow
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
| | | | | | | | | | | * 9e4d62e6 server : improve "prompt" handling
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
| | | | | | | | | | | * 956bb145 examples : remove --instruct remnants
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 10ceba35 flake.lock: Update (#7838)
* | | | | | | | | | | e95beeb1 imatrix : handle partial entries (#7833)
* | | | | | | | | | | 57bf62ce docs: Added initial PR template with directions for doc only changes and squash merges [no ci] (#7700)
* | | | | | | | | | | 3e2ee443 server: do not remove whitespace at the start of a completion chunk (#7830)
* | | | | | | | | | | 42b53d19 CUDA: revise q8_1 data layout for mul_mat_q (#7824)
* | | | | | | | | | | 2decf57b convert-hf : set the model name based on cli arg, if present (#7693)
* | | | | | | | | | | 5795b941 convert-hf : match model part name prefix and suffix (#7687)
* | | | | | | | | | | ed9f2521 gguf-py : decouple adding metadata from writing in GGUFWriter (#7827)
* | | | | | | | | | | fe1e3917 Revert "[SYCL] Update rpc-server.cpp to include SYCL backend (#7682)" (#7808)
| |_|_|/ / / / / / /  
|/| | | | | | | | |   
* | | | | | | | | | d4d915d3 url: save -mu downloads to new cache location (#7826)
* | | | | | | | | | 7a16ce7d server : smart slot selection using Longest Common Prefix (#7728)
* | | | | | | | | | da799b41 vulkan : reuse parent extra for views (#7806)
* | | | | | | | | | c00fad71 gguf-split : change binary multi-byte units to decimal (#7803)
* | | | | | | | | | 27615f5a cmake : fix BUILD_SHARED_LIBS=ON build (#7784)
* | | | | | | | | | 7027b27d server: update cache_prompt documentation [no ci] (#7745)
* | | | | | | | | | a5cabd76 server : do not get prompt in infill mode (#7286)
* | | | | | | | | | d5c938cd [SYCL] fix softmax r2r result wrong issue (#7811)
* | | | | | | | | | c9ee7118 check for nans in imatrix and quantize (#7807)
| | | | | | | | | | * d857e519 quantize : check imatrix for nan/inf values
| | | | | | | | | | * e2ea071c imatrix : detect nan/inf values
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | ee459f40 server : fix --threads-http arg (#7801)
| | | | | | | | | | * 731e7528 server : fix --threads-http arg
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | f83351f9 imatrix : migrate to gpt_params (#7771)
* | | | | | | | | | ad675e1c Added support for . (any character) token in grammar engine. (#6467)
* | | | | | | | | | a143c043 README minor fixes (#7798) [no ci]
* | | | | | | | | | 55b2d084 grammars: x{min,max} repetition operator (#6640)
* | | | | | | | | | f5d7b268 llama : add jina v2 base code (#7596)
* | | | | | | | | | 2d08b7fb docker : build only main and server in their images (#7782)
* | | | | | | | | | d67caea0 docker : add openmp lib (#7780)
| | | | | | | | | | * f7d4b7c3 build only main and server in their docker images
| | | | | | | | | | * 3d2e79da add openmp lib to dockerfiles
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 7672adee Fix encoding in python scripts (#7733)
* | | | | | | | | | 7d1a378b CUDA: refactor mmq, dmmv, mmvq (#7716)
* | | | | | | | | | 2b338967 ggml : refactor rope norm/neox (#7634)
* | | | | | | | | | 9973e81c readme : remove -ins (#7759)
* | | | | | | | | | c90dbe02 Fix per token atrributes bits (#7749)
* | | | | | | | | | b90dc566 Allow number of nodes in CUDA graph to change (#7738)
* | | | | | | | | | 1442677f common : refactor cli arg parsing (#7675)
* | | | | | | | | | 554c247c ggml : remove OpenCL (#7735)
* | | | | | | | | | 0cd6bd34 llama : remove beam search (#7736)
* | | | | | | | | | 5ca0944a readme : remove obsolete Zig instructions (#7471)
| | | | | | | | | | * 0085f949 server : add /v1/completion endpoint
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | adc9ff38 llama-bench : allow using a different printer for stderr with -oe (#7722)
* | | | | | | | | | 987d743d Improve hipBLAS support in CMake (#7696)
* | | | | | | | | | b226c122 refine .gitignore (#7688)
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 3b38d486 Per token attributes (#7685)
* | | | | | | | | 6d161694 ggml : prevent builds with -ffinite-math-only (#7726)
| |_|_|_|_|/ / /  
|/| | | | | | |   
* | | | | | | | bde7cd3c llama : offload to RPC in addition to other backends (#7640)
* | | | | | | | a5735e44 ggml : use OpenMP as a thread pool (#7606)
* | | | | | | | 0b832d53 make: fix debug options not being applied to NVCC (#7714)
* | | | | | | | 3d7ebf63 Vulkan Mixture of Experts (MoE) support (#7628)
* | | | | | | | a10cda58 cmake : add pkg-config spec file for llama.cpp (#7702)
* | | | | | | | 6f28a333 llama : MiniCPM support tied embeddings (#7664)
* | | | | | | | 549279d8 llama : avoid double token-to-piece cache (#7654)
* | | | | | | | 9e405b6e kompute : implement op_getrows_f32 (#6403)
* | | | | | | | 3413ae21 fix bug introduced in using calloc (#7701)
* | | | | | | | 1669810d flake.lock: Update (#7686)
* | | | | | | | 7c4e5b7e chore : add ignore rule for generated server themes (#7689)
* | | | | | | | 9422c5e3 [SYCL] Update rpc-server.cpp to include SYCL backend (#7682)
* | | | | | | | e141ce62 Fix FlashAttention debug test, FP32 assert (#7684)
* | | | | | | | 2e666832 server : new UI (#7633)
* | | | | | | | 2ac95c9d SimpleChat: Simple histogram/repeatMatching driven garbageTrimming, Settings UI, Streaming mode, OpenAi Compat (Model, Authorization Bearer), Save/Restore session, Auto Settings UI (#7548)
| |_|/ / / / /  
|/| | | | | |   
* | | | | | | 750f60c0 CUDA: fix Pascal FA, deq. KV to FP16 for batch > 8 (#7681)
* | | | | | | 9b596417 CUDA: quantized KV support for FA vec (#7527)
* | | | | | | a323ec60 server : update js (#7670)
* | | | | | | 0515ad93 convert-hf : Handle NotImplementedError in convert-hf-to-gguf (#7660)
* | | | | | | c8047d53 scripts: update compare_llama_bench.py [no ci] (#7673)
* | | | | | | 30e238b2 Improve HIP compatibility (#7672)
| | | | | | | * 5f8720fb add rpc-server to Makefile
| | | | | | | * a7060dff - fix copy_tensor being called on the src buffer instead of the dst buffer
| | | | | | | * 6c276deb llama : offload to RPC in addition to other backends
| | | | | | | | * 956af155 server : update js
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 16926dff readme : link homebrew discussion
* | | | | | | | 0c27e6f6 ggml : fix loongson compile warnings (#7537)
* | | | | | | | 2e32f874 Somehow '**' got lost (#7663)
* | | | | | | | 1af511fc Add convert.py removal to hot topics (#7662)
* | | | | | | | 0541f062 [no ci] docs: add aikit to readme (#7650)
* | | | | | | | 9022c336 Fixed painfully slow single process builds. (#7326)
* | | | | | | | 5921b8f0 llama : cache llama_token_to_piece (#7587)
* | | | | | | | 5dcdf946 Fix conan badge display [no ci] (#7645)
* | | | | | | | 2e2340de Add brew installation instruction to README [no ci] (#7616)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 7846540b readme : add Conan badge (#7638)
| | | | | | | * 77c16ee0 tests : disable json test due to lack of python on the CI node
| | | | | | | * 50fb3d34 Fix loongarch quantize test fail.
| | | | | | | * fd5de67b ggml : fix loongson compile warnings
| | | | | | | | * d32a8f61 backup
| | | | | | | | * 1f80e0e4 seperate DPCT helpers outside remove global variables and pack into context
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | e6157f94 github: add contact links to issues and convert question into research [no ci] (#7612)
* | | | | | | | 9c4c9cc8 Move convert.py to examples/convert-legacy-llama.py (#7430)
* | | | | | | | 59b0d077 faster avx512 exp implementation (#7551)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | d5c05821 ggml : fix loongarch build (O2 issue) (#7636)
* | | | | | | 972b555a README: explain parallel build [no ci] (#7618)
* | | | | | | 3854c9d0 [SYCL] fix intel docker (#7630)
* | | | | | | eb57fee5 gguf-py : Add tokenizer.ggml.pre to gguf-new-metadata.py (#7627)
* | | | | | | 55d62262 metal : remove invalid asserts (#7617)
* | | | | | | 975ec63f metal : add missing asserts (#7617)
| | | | | | | * 8a8f8b95 llama : print a log of the total cache size
| | | | | | | * 1494a184 llama : throw on unknown tokenizer types
| | | | | | | * 21ccd645 llama : use vectors and avoid has_cache
| | | | | | | * 9964cd02 llama : cache llama_token_to_piece
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | fb76ec31 ggml : fix YARN + add tests + add asserts (#7617)
* | | | | | | cce3dcff cuda : non-cont concat support (#7610)
* | | | | | | 210d9917 llama-bench : add support for the RPC backend (#7435)
* | | | | | | 87bdf2a1 ggml : use atomic_flag for critical section (#7598)
* | | | | | | 00281b7b scripts : remove mpi remnants
* | | | | | | 2ab97728 sync : ggml
* | | | | | | 72de268b ggml : restore ggml_rope_xpos_inplace (ggml/0)
* | | | | | | 0e8d8bfd Add Arc A750 and Arch linux to readme-sycl.md as verified GPU model and Linux distro (#7605)
* | | | | | | 504f0c34 ggml : fix typo in ggml.c (#7603)
* | | | | | | b864b50c [SYCL] Align GEMM dispatch (#7566)
* | | | | | | 02c1ecad Tokenizer WPM fixes (#7500)
* | | | | | | 6bd12ce4 sycl : fix assert (#7563)
| |/ / / / /  
|/| | | | |   
* | | | | | 5442939f llama : support small Granite models (#7481)
| |_|/ / /  
|/| | | |   
* | | | | 56411a95 vulkan: properly initialize vulkan devices for LLAMA_SPLIT_MODE_NONE (#7552)
* | | | | 2b737caa rpc : resource management rework (#7562)
* | | | | ee3dff6b Add support for DeepseekV2ForCausalLM (#7519)
* | | | | edc29433 tests : fix test-tokenizer-0.sh
* | | | | 8b99e2aa llama : handle unknown utf8 bytes (#7588)
* | | | | 271ff3fc github: add refactor to issue template (#7561)
* | | | | e2b06507 [SYCL]fix ggml_sycl_mul_mat_id() to match the change of api (#7436)
* | | | | 0548a418 ggml : generalize GGML_OP_CONCAT (#7563)
* | | | | 9335b969 server: do not remove whitespace at the start of a completion chunk (#7524)
* | | | | c4176715 Markdownish code block fix (#7571)
* | | | | 74b239b3 llava : update clip.h (#7580)
* | | | | 852aafb1 update HIP_UMA #7399 (#7414)
* | | | | 0136966d adding in x64 targets to cmake presets (#7574)
| | | | | * 1ca802a3 parallelize fattn compilation test
| | | | | * f4003cfb fix nwarps > batch size
| | | | | * f0877604 add q8_0 q4_0 tests
| | | | | * 3194a010 fix commented-out kernel variants
| | | | | * 462add6a try CI fix
| | | | | * 672244a8 CUDA: quantized KV support for FA vec
| |_|_|_|/  
|/| | | |   
* | | | | 10b1e458 make: add --device-debug to NVCC debug flags (#7542)
* | | | | 197c0068 Allow multiple copy function pointers for CUDA graph kernel param updates (#7565)
* | | | | 95f84d5c Fix q_xxs using mul_mat_q (#7459)
* | | | | 5487593b Add freq factors (#7495)
* | | | | 1d8fca72 metal : add GGML_OP_REPEAT kernels (#7557)
| | | | | * ddc59e8e wipwipwiwpip
| | |_|_|/  
| |/| | |   
| * | | | fc59407e convert-hf : support Mini-Jamba conversion
| * | | | ea2e63e9 convert-hf : check for unprocessed Jamba experts
| * | | | 61a88a1d llama : fix BERT inference without KV cache
| * | | |   0fd13e94 Merge branch 'master' into compilade/refactor-kv-cache
| |\ \ \ \  
| * | | | | cbc743e6 llama : support Jamba
| * | | | | 7e13f19f llama : rethink recurrent state cell counts
| * | | | |   3b57b55c Merge branch 'master' into compilade/refactor-kv-cache
| |\ \ \ \ \  
| | | |_|/ /  
| | |/| | |   
| * | | | |   b7ec12eb Merge branch 'master' into compilade/refactor-kv-cache
| |\ \ \ \ \  
| * | | | | | b6fafd17 llama : remove useless return value for some llama_cache_* functions
| * | | | | |   c460ff1a Merge branch 'master' into compilade/refactor-kv-cache
| |\ \ \ \ \ \  
| * | | | | | | a09db95e llama : rename many llama_kv_cache_* functions
| * | | | | | |   d66849f6 Merge branch 'master' into compilade/refactor-kv-cache
| |\ \ \ \ \ \ \  
| * | | | | | | | 0c8b3b20 llama : correctly handle more edge cases for the rs cache
| * | | | | | | | 0028010d llama : state checkpoints for recurrent models
| * | | | | | | | 8db1e4d4 llama : use std::find for seq_nodes in llama_rs_cache
| * | | | | | | | 271104c6 wip: llama : separate recurrent states from the KV cache
| | | | | | | | | * 4b177010 Fix q_xxs using mul_mat_q
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 62bfef51 metal : disable FA kernel for HS=256 (#7556)
* | | | | | | | | eaf6e031 llama : add comments about experimental flags (#7544)
| | | | | | | | | * 1c6cde92 metal : disable FA kernel for HS=256
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | d6ef0e77 github: add self sorted issue ticket forms (#7543)
* | | | | | | | | dff451cf flake.lock: Update (#7540)
* | | | | | | | | d298382a main: replace --no-special with --special (#7534)
* | | | | | | | | 32a28217 Fix aya-23 conversion scripts (#7539)
* | | | | | | | | c429b33b llama : add Smaug 70B support (#7402)
* | | | | | | | | 9146d36f Readme: add akx/ggify to tools (#1484)
* | | | | | | | | b9adcbbf SimpleChat Completion Mode flexibility and cleanup, Settings gMe, Optional sliding window (#7480)
| | | | | | | | | * 11f78c6a convert-hf : adapt ArcticModel to use yield too
| | | | | | | | | *   96a299ff Merge branch 'master' into compilade/lazier-moe-convert-hf
| | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 9588f196 train : change default FA argument (#7528)
* | | | | | | | | | 3cbd23ed labeler: added Apple Metal detector (+Kompute) (#7529)
* | | | | | | | | | 00c63907 main : don't print special tokens with --grammar (#6923)
* | | | | | | | | | faa0e697 ggml: aarch64: SVE kernels for q8_0_q8_0, q4_0_q8_0 vector dot (#7433)
* | | | | | | | | | 9791f402 android : module (#7502)
* | | | | | | | | | 902184dd fix missing slash in `fs_get_cache_directory()` (#7503)
* | | | | | | | | | 57684331 Make tokenize CLI tool have nicer command line arguments. (#6188)
* | | | | | | | | | b83bab15 gguf-py : fix and simplify quantized shape round-trip (#7483)
| |_|_|_|_|/ / / /  
|/| | | | | | | |   
* | | | | | | | | d041d2ce flake.lock: Update (#7232)
* | | | | | | | | 27891f6d docker.yml: disable light-intel and server-intel test (#7515)
* | | | | | | | | fbca2f27 Add support for ArcticForCausalLM (#7020)
| | | | | | | | * d703fa9f convert-hf : fix flake8 indentation lint
| | | | | | | | * 93b9baee convert-hf : reduce stacked MoE conversion RAM usage by a third
| | | | | | | | | * dd14d818 Update main-intel.Dockerfile base image to 2024.1.0
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 0df0aa8e add build shared lib in win release package (#7438)
* | | | | | | | | 74f33adf readme : remove trailing space (#7469)
* | | | | | | | | 1debe727 ggml : silence UB sanitizer error during iq2_xxs quantization (#0)
* | | | | | | | | 007489e8 Fix phi3 chat template confusion with zephyr (#7449)
* | | | | | | | | 8b94e799 readme : add Bunny in supported models [no ci] (#7469)
* | | | | | | | | 3015851c llama : add getters for n_threads/n_threads_batch (#7464)
* | | | | | | | | 55ac3b7a ci : use Pythia models instead of OpenLlama (#7470)
* | | | | | | | | dacfcebd readme : add GPT-NeoX + Pythia to the list of supported models (#7491)
| |_|_|_|_|/ / /  
|/| | | | | | |   
* | | | | | | | 9b82476e Add missing inference support for GPTNeoXForCausalLM (Pythia and GPT-NeoX base models) (#7461)
* | | | | | | | a61a94e5 llama : rename n_ctx -> cache.size, less confusing (#0)
* | | | | | | | 152da28a labeler.yml: add embedding label detector [no ci] (#7482)
* | | | | | | | d48c88cb ggml : remove ggml_flash_attn and ggml_flash_ff (#7463)
* | | | | | | | e84b71c2 ggml : drop support for QK_K=64 (#7473)
* | | | | | | | 1b1e27cb Update vulkan rope implementation to support frequency factors (#7475)
* | | | | | | | fbf777d2 main : minor (#7462)
| | | | | | | | * c5fe1d6c gguf-py : remove unused import
| | | | | | | | * 2ff601fc gguf-py : fix and simplify quantized shape round-trip
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | * 518b7526 cuda uma test
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | cd93a28c CUDA: fix FA out-of-bounds reads (#7479)
| |_|_|_|/ / /  
|/| | | | | |   
* | | | | | | 1e374365 SimpleChat: a simple and dumb web front end for testing /chat/completions and /completions end points and try chat (#7350)
* | | | | | | 197ff914 build : remove zig (#7471)
* | | | | | | 6ff13987 common : normalize naming style (#7462)
* | | | | | | 38c03478 CUDA: fix FA out-of-bounds writes (#7465)
* | | | | | | b18532a4 phi3 : duplicate rope factors in each layer (#7447)
* | | | | | | fcda1128 vulkan: add workaround for iterator boundary check to fix clang-cl debug build (#7426)
* | | | | | | 03d8900e llama : add missing model type names (#7445)
* | | | | | | 9b3d8331 cuda : fix compile warning (#7454)
* | | | | | | 95fb0aef CUDA: remove incorrect precision check (#7454)
* | | | | | | 3e5faa85 cuda : fix rope + add tests (#7452)
* | | | | | | 201cc11a llama : add phi3 128K model support (#7225)
* | | | | | | 6369bf04 metal : handle F16 inf values, fix FA partial offload (#7434)
* | | | | | | e402de36 `grammars`: fix resampling logic regression (#7424)
* | | | | | | fcf6538b CUDA: fix unused warning in mmq.cu (#7442)
* | | | | | | c3f8d583 tests : test-tokenizer-0.sh print more info (#7402)
* | | | | | | 11474e75 examples: cache hf model when --model not provided (#7353)
* | | | | | | d8ee9022 CUDA: deduplicate mmq code (#7397)
* | | | | | | d7e852c1 Tokenizer SPM fixes for phi-3 and llama-spm (bugfix) (#7425)
| | | | | | | * e9095e60 async direct io per tensor test
| | | | | | | * 46db3506 address review comments
| | | | | | | * 1b17ed7a Direct I/O and Transparent HugePages
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 917dc8cf Tokenizer SPM fixes for phi-3 and llama-spm (#7375)
* | | | | | | fabf30b4 llama : remove Persimmon (#7408)
* | | | | | | 20385ceb perplexity: update README FP16 results [no ci] (#7413)
* | | | | | | db10f013 rpc : track allocated buffers (#7411)
* | | | | | | 3bc10cb4 server : fix temperature + disable some tests (#7409)
* | | | | | | 6bf9b66f [SYCL] Update SYCL upscale operation (#7321)
| | | | | | | * a041ced0 wip
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 26cd4237 Update README.md (#7410)
* | | | | | | 213e90ed ggml-opencl, llama: using reserve() if count already known (#7272)
* | | | | | | 65c58207 ggml : add loongarch lsx and lasx support (#6454)
* | | | | | | 1cc0155d server : tuning tests (#7388)
* | | | | | | e932094d server : return error on too large embedding input (#7389)
* | | | | | | 2789baf4 tests : fix --keep_split -> --keep-split (#7374)
* | | | | | | 33c8d50a Add provisions for windows support for BF16 code including CMake provision for enabling AVX512_BF16 (#7258)
* | | | | | | d359f309 llama : remove MPI backend (#7395)
* | | | | | | 1ea2a003 quantize : fix --keep-split check (#7374)
* | | | | | | f030ec1f Vulkan Embedding Fix (#7360)
* | | | | | | e4e6f67b ggml : fix another case of quants nans (#7387)
* | | | | | | 5ca49cbe ggml: implement quantized KV cache for FA (#7372)
* | | | | | | 1b01f06d server: add test for token probs (#7347)
* | | | | | | 41858392 server: fix seed being reported back (#7382)
* | | | | | | 6aade19e Add StableLM2 pre-tokenizer (#7349)
* | | | | | | ab33f7a3 cuda : clear error after buffer allocation failure (#7376)
* | | | | | | e23b974f labeler.yml: Use settings from ggerganov/llama.cpp [no ci] (#7363)
* | | | | | | 854d365a cmake : update android comments (#7341)
* | | | | | | f5bf7617 Capture CUDA logging output (#7298)
* | | | | | | 059031b8 ci : re-enable sanitizer runs (#7358)
* | | | | | | 511182ea android : use "ci-android" branch for CI (#7341)
* | | | | | | 133d99c5 CUDA: deduplicate FlashAttention code (#7352)
* | | | | | | cb42c294 server: correct --threads documentation [no ci] (#7362)
* | | | | | | d233b507 cuda : add half2 __shfl_xor() for ROCm 5.5 (#7263)
* | | | | | | 0f98acfa llama : add support for larger Granite Code Models (20B, 34B) (#7324)
* | | | | | | ca57e0f3 perplexity : ndot progress and show stats with < 100 tasks (#7348)
| |_|_|_|/ /  
|/| | | | |   
* | | | | | c1b295ee Update and fix Vulkan soft_max and argsort implementations (#7237)
* | | | | | de731963 github-actions-labeler: initial commit (#7330)
* | | | | | b49a13dd convert : fix set_vocab_sentencepiece (#6866)
* | | | | | 05834841 ggml : fix quants nans when all the group weights are very close to zero (#7313)
* | | | | | ef277de2 cmake : fix typo in AMDGPU_TARGETS (#7356)
* | | | | | b43272af Unicode codepoint flags for custom regexs (#7245)
* | | | | | 0fc1e820 CUDA: faster large batch FA without tensor cores (#7314)
* | | | | | 82ca83db ROCm: use native CMake HIP support (#5966)
* | | | | | f4bd8b3d rpc : set SO_REUSEADDR for the server socket (#7320)
| | | | | | * 007f2ece cmake : provide binary dir
| | | | | | * 99d1e7eb android : do not fetch, use add_subdirectory instead
| | | | | | * 2117b303 ggml : disable SIMD exp and silu for 32-bit ARM
| | | | | | * 87259373 android : use "ci-android" branch for CI
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 51e9d025 Added a single test function script and fix debug-test.sh to be more robust (#7279)
* | | | | | d273c140 py : convert-hf-to-gguf-update improvements (#7340)
* | | | | | 27b04069 llama : use n_embd_head_v when reshaping kqv (#7327)
* | | | | | 29c60d8c tokenization: add warning for double BOS (#7332)
* | | | | | 359cbe3f ggml-quants, llama : removed excess checks (#7274)
* | | | | | e18bc6aa convert : fix Qwen/Qwen-7b conversion (#7308)
* | | | | | ee94172d server : add support for the RPC backend (#7305)
* | | | | | 934266c0 ggml : rewrite silu and softmax for cpu (#7154)
| | | | | | * 6b2f4964 wip
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 9c4fdcbe [Server] Added --verbose option to README [no ci] (#7335)
* | | | | | 24ecb581 Revert "server bench: fix bench not waiting for model load (#7284)" (#7334)
| | | | | | * a085a832 tmp
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 9afdffe7 rpc : get available mem for the CPU backend
* | | | | | 3b3963c5 rpc : add command line arg for specifying backend memory
* | | | | | dda64fc1 convert : get general.name from model dir, not its parent (#5615)
* | | | | | 0350f581 grammar, json, llama: replace push on emplace if it possible (#7273)
* | | | | | ad52d5c2 doc: add references to hugging face GGUF-my-repo quantisation web tool. (#7288)
* | | | | | 172b7821 ci: fix bin/Release path for windows-arm64 builds (#7317)
* | | | | | 13ad16af Add support for properly optimized Windows ARM64 builds with LLVM and MSVC (#7191)
| | | | | | * 5de9b743 sched : support async weight copy
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 8f7080bf readme : remove stray double quote (#7310)
* | | | | | e1b40ac3 ggml : use dynamic thread scheduling for matrix multiplication (#6915)
* | | | | | dc020985 Avoid unnecessarily disabling CUDA graphs (#7302)
* | | | | | 344f9126 ggml : tag ggml_tensor::backend as deprecated (#7290)
* | | | | | 9a17ab91 Add missing " (#7303)
* | | | | | ea3b0590 embedding : free the batch after execution (#7297)
* | | | | | 29499bb5 sync : ggml
* | | | | | 48aa8fd1 ggml : add `ggml_upscale_ext` (ggml/814)
* | | | | | 583fd6b0 server bench: fix bench not waiting for model load (#7284)
* | | | | | 9f773486 script : sync ggml-rpc
* | | | | | e8a7fd4f metal : support FA without mask + add asserts (#7278)
* | | | | | a5e3fde8 sync : ggml
* | | | | | f308ea70 metal : tune soft_max number of threads (whisper/0)
* | | | | | c3c88f29 ggml : try fix ppc64 (whisper/0)
* | | | | | 182adefc ggml : expose SSE3 and SSSE3 for MSVC when AVX is available (whisper/2128)
* | | | | | 0d26d8cc ggml : optimize for ppc64le using VSX intrinsics (ggml/784)
* | | | | | 4f026363 server: free sampling contexts on exit (#7264)
* | | | | | 1265c670 Revert "move ndk code to a new library (#6951)" (#7282)
* | | | | | 5e31828d ggml : add RPC backend (#6829)
* | | | | | 54160020 llama : disable pipeline parallelism with nkvo (#7265)
* | | | | | efc8f767 move ndk code to a new library (#6951)
* | | | | | e0f55618 Add left recursion check: quit early instead of going into an infinite loop (#7083)
* | | | | | 27f65d62 docs: Fix typo and update description for --embeddings flag (#7026)
| | | | | | *   284870c8 Merge branch 'master' into fix-convert-modelname
| | | | | | |\  
| |_|_|_|_|_|/  
|/| | | | | |   
| | | | | | * 941de117 convert : get general.name from model dir, not its parent
| | | | | | | * 94061d58 llama : disable pipeline parallelism with nkvo
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | ee522250 convert-hf : support direct Q8_0 conversion (#7234)
* | | | | | | 614d3b91 llama : less KV padding when FA is off (#7257)
* | | | | | | 30e70334 llava-cli: fix base64 prompt (#7248)
* | | | | | | 1c570d8b perplexity: add BF16 vs. FP16 results (#7150)
* | | | | | | 948f4ec7 [SYCL] rm wait() (#7233)
* | | | | | | 9aa67249 llama : rename jina tokenizers to v2 (#7249)
| | | | | | | * 33a004e9 llama : more metal-friendly KV cache PAD
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | b1f8af18 convert.py: Outfile default name change and additional metadata support (#4858)
* | | | | | | e586ee42 change default temperature of OAI compat API from 0 to 1 (#7226)
* | | | | | | cbf75894 [SYCL] Add oneapi runtime dll files to win release package (#7241)
* | | | | | | 0d5cef78 [SYCL] update CI with oneapi 2024.1 (#7235)
| |_|_|/ / /  
|/| | | | |   
* | | | | | dc685be4 CUDA: add FP32 FlashAttention vector kernel (#7188)
| |_|_|/ /  
|/| | | |   
* | | | | 6f1b6360 cmake : fix version cmp (#7227)
* | | | | b228aba9 remove convert-lora-to-ggml.py (#7204)
* | | | | 7bd4ffb7 metal : fix warnings (skipme) (#0)
* | | | | 1622ac02 sync : ggml
* | | | | 6aeff24f metal : fix indent (ggml/0)
* | | | | 325756d2 ggml : resolve merge (ggml/0)
* | | | | fed01084 Scripting & documenting debugging one test without anything else in the loop. (#7096)
* | | | | 72c177c1 fix system prompt handling (#7153)
| | | | | * 65a1a585 convert-hf : add missing ftype to Baichuan and Xverse
| | | | | * 2b1e5ea3 convert-hf: add missing ftype
| | | | | * d7e199e4 convert-hf : support q8_0 conversion
| |_|_|_|/  
|/| | | |   
* | | | | 5a419926 convert-hf : support bfloat16 conversion (#7158)
* | | | | fae9d234 sync : ggml
* | | | | f5ef34e4 feat: implemented sigmoid function (ggml/806)
* | | | | ef0d5e3e build: fix and ignore msvc warnings (ggml/805)
* | | | | 3292733f convert : skip unaccessible HF repos (#7210)
* | | | | 98863133 server : free llama_batch on exit (#7212)
* | | | | f99e1e45 llama : lookup word in vocab before doing BPE merges (#7193)
* | | | | 5ae3426b server: fix reported top tokens for temperature 0 (#7203)
* | | | | b83cc3f5 llama : add Jina Embeddings architecture (#6826)
* | | | | 9cb317f7 ggml : full ALiBi support (#7192)
* | | | | e8496488 llama-bench : add pp+tg test type (#7199)
* | | | | 18e43766 metal : fix flash attention kernel requirements (#7169)
* | | | | 8c660242 convert : print "ignore_merges" field
* | | | | 25c6e82e llama : use n_vocab to differentiate between mistral 7B and llama3 8B (#7200)
* | | | | 4e388097 Fix memory bug in grammar parser (#7194)
* | | | | f89fe273 Main+: optionally allow special tokens from user in interactive mode (#7097)
| | | | | * 03e940cd convert : fix convert for refact models
| | | | | * 0faf92e7 ggml : require mask when using ALiBi
| | | | | * 397b1f8f vulkan : add dev notes
| | | | | * 536983b1 ggml : fix assert message
| | | | | * 865af990 ggml : ggml_flash_attn_ext() support ALiBi (CUDA)
| | | | | * f7055d31 ggml : fix warning
| | | | | * 97c27f59 ggml : ggml_flash_attn_ext() support ALiBi (Metal)
| | | | | * 166e60bf ggml : ggml_flash_attn_ext() support ALiBi (CPU)
| | | | | * d0592d49 ggml : update ggml_soft_max_ext() CUDA, SYCL
| | | | | * 7fdca334 ggml : full ALiBi support
| |_|_|_|/  
|/| | | |   
* | | | | d11afd66 llava : fix moondream support (#7163)
* | | | | 8c570c94 Minor arithmetic improvement to mmvq wrapper kernel (#7172)
* | | | | eaf4bd8b eval-callback : fix conversion to float (#7184)
* | | | | befddd0f Vulkan Bugfixes and Improvements (#7084)
* | | | | d46dbc76 readme : add scheduled server workflow status badge
* | | | | 0961d866 readme : add app (#6371)
* | | | | 43248e55 llama3 custom regex split (#6965)
* | | | | a743d76a CUDA: generalize FP16 fattn vec kernel (#7061)
* | | | | f31ec120 Add warning if token is invalid (#7173)
* | | | | fd9f92b1 llama : update llama_timings.n_p_eval setting (#7160)
* | | | | 22842164 gguf-py : add special token modification capability (#7166)
* | | | | 47345248 opencl : alignment size converted from bits to bytes (#7090)
| | | | | * e0af2df6 convert-hf : support outtype templating in outfile name
| | | | | * d3d32a63 convert-hf : remove a semicolon because flake8 doesn't like it
| | | | | * 58b515ca convert-hf : add --outtype auto-f16
| | | | | * 95930da3 convert-hf : get bit-exact same output as ./quantize
| | | | | * 3801db12 convert-hf : add missing space after comma
| | | | | * 59f5a27f gguf-py : flake8 fixes
| | | | | * 6f8d2800 convert-hf : support bfloat16 conversion
| | | | | | * fecb81e3 metal : fix ggml_metal_supports_op
| | | | | | * 1174def5 metal : fix flash attention kernel requirements
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 07cd41d0 TypoFix (#7162)
| |_|_|_|/  
|/| | | |   
* | | | | 4426e298 cmake : fix typo (#7151)
* | | | | f98eb31c convert-hf : save memory with lazy evaluation (#7075)
* | | | | bc4bba36 Introduction of CUDA Graphs to LLama.cpp (#6766)
| | | | | * 494f70f9 cmake : fix typo
| |_|_|_|/  
|/| | | |   
* | | | | c12452c7 JSON: [key] -> .at(key), assert() -> GGML_ASSERT (#7143)
* | | | | 9da243b3 Revert "llava : add support for moondream vision language model (#6899)"
* | | | | bd1871fa server : add themes + favicon (#6848)
* | | | | 26458af1 metal : use `vm_allocate` instead of `posix_memalign` on macOS (#7078)
| | | | | *   bffdaf40 Merge branch 'master' into compilade/lazy-convert-hf
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 83330d8c main : add --conversation / -cnv flag (#7108)
* | | | | | 465263d0 sgemm : AVX Q4_0 and Q8_0 (#6891)
* | | | | | 911b3900 server : add_special option for tokenize endpoint (#7059)
* | | | | | ad211ede convert.py : --vocab-only generates false but valid params (#7027)
* | | | | | 229ffff8 llama : add BPE pre-tokenization for Qwen2 (#7114)
* | | | | | 1fd9c174 clean up json_value & server_log (#7142)
* | | | | | 4cd621c2 convert : add BPE pre-tokenization for DBRX (#7132)
* | | | | | 7e0b6a7b py : also print the normalizers
* | | | | | acdce3cd compare-llama-bench.py: add missing basicConfig (#7138)
| | | | | * 94e667a9 gguf-py : add tqdm as a dependency
| | | | | *   68c5ac62 Merge branch 'master' into compilade/lazy-convert-hf
| | | | | |\  
| | | | | * | 62303e7f convert-hf : minor changes for consistency
| | | | | * | bc78bf4c convert-hf : faster model parts loading
| | | | | * | 98db4347 convert-hf : remove einops requirement for InternLM2
| | | | | * | 0c383328 convert-hf : flake8 doesn't like lowercase L as a variable name
| | | | | * | f09674fb convert-hf : save memory with lazy evaluation
| | | | | | | * 0fc560fe ci : enable git lfs for build.yml
| | | | | | | * db5c2ad3 Revert "tmp : dummy change to trigger ci"
| | | | | | | * 97e40df5 tmp : dummy change to trigger ci
| | | | | | | * 837f426f ci : try lfs true
| | | | | | | * 9d13776f ci : deps before checkout
| | | | | | | * 2c7ff2c7 ci : add git-lfs
| | | | | | | * 0dc0e9aa models : convert vocab files to LFS
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 38554160 ggml : introduce bfloat16 support (#6412)
* | | | | | | c0e6fbf8 metal : fix unused warning
* | | | | | | c780e753 Further tidy on Android instructions README.md (#7077)
* | | | | | | 48b2f9c1 Fixed save_imatrix to match old behaviour for MoE (#7099)
* | | | | | | af0a5b61 server: fix incorrectly reported token probabilities (#7125)
* | | | | | | b6aa6702 Fix OLMo HF to GGUF conversion (#6910)
* | | | | | | 260b7c65 server : update readme with undocumented options (#7013)
* | | | | | | 53d6c52e readme : update hot topics
* | | | | | | 3af34c1d main : update log text (EOS to EOG) (#7104)
* | | | | | | 04976db7 docs: fix typos (#7124)
* | | | | | | 947d3ad2 ci : add GG_BUILD_EXTRA_TESTS_0 env (#7098)
* | | | | | | 858f6b73 Add an option to build without CUDA VMM (#7067)
* | | | | | | b3a995b4 flake.lock: Update (#7079)
| |_|_|_|_|/  
|/| | | | |   
| | | | | | * c32d39ce Merge branch 'master' into compilade/convert-hf-refactor
| | | | | |/| 
| |_|_|_|_|/  
|/| | | | |   
* | | | | | bcdee0da minor : fix trailing whitespace
* | | | | | 628b2991 Adding support for the --numa argument for llama-bench. (#7080)
* | | | | | 8f8acc86 Disable benchmark on forked repo (#7034)
* | | | | | ca363260 readme : add note that LLaMA 3 is not supported with convert.py (#7065)
* | | | | | 889bdd76 command-r : add BPE pre-tokenization (#7063)
* | | | | | 6fbd4322 py : logging and flake8 suppression refactoring (#7081)
* | | | | | 84250014 gguf-split: add --no-tensor-first-split (#7072)
* | | | | | cf768b7e Tidy Android Instructions README.md (#7016)
* | | | | | fcd84a0f Fix Linux /sys cpu path to guess number of cores (#7064)
* | | | | | 03fb8a00 If first token generated from the server is the stop word the server will crash (#7038)
* | | | | | 92139b90 tests : add test-tokenizer-0.sh + fix some tokenizers (#7036)
| | | | | * 215a0d38 convert-hf : fix Refact conversion
| | | | | * f2099c50 convert-hf : align the message logged for converted tensors
| | | | | * 98f2d0e0 convert-hf : more consistent formatting of cmdline args
| | | | | *   3e5e0dce Merge branch 'master' into compilade/convert-hf-refactor
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | a2ac89d6 convert.py : add python logging instead of print() (#6511)
* | | | | | 433def28 llama : rename ctx to user_data in progress_callback (#7045)
| | | | | *   6a54973d Merge branch 'master' into compilade/convert-hf-refactor
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 60325fa5 Remove .attention from skipped tensors to match more accurately (#7051)
* | | | | | 6ecf3189 chore: fix typo in llama.cpp (#7032)
* | | | | | b0d943de Update LOG_IMPL and LOG_TEE_IMPL (#7029)
* | | | | | 8d608a81 main : fix off by one error for context shift (#6921)
* | | | | | 3ea0d360 Server: add tests for batch size, different seeds (#6950)
* | | | | | 1613ef8d CUDA: CUDART < 11.7 workaround for __hmax, __hmax2 (#7019)
* | | | | | c4ec9c0d ci : exempt confirmed bugs from being tagged as stale (#7014)
* | | | | | a8f9b076 perplexity: more statistics, added documentation (#6936)
| | | | | * 13f4cf70 convert-hf : use a plain class for Model, and forbid direct instantiation
| | | | | * ce067af1 convert-hf : use an ABC for Model again
| | | | | * 644c2696 convert-hf : sort model part names
| | | | | * 639b374b convert-hf : convert norms to f32 by default
| | | | | * 21068b6b convert-hf : display tensor shape
| | | | | * dcd8dfa1 convert : use a string for the SentencePiece tokenizer path
| | | | | * 3870164f convert-hf : allow unusual model part names
| | | | | * 56f60f5d convert-hf : flake8 linter doesn't like semicolons
| | | | | * cde9ea65 convert-hf : simplify MoE weights stacking
| | | | | * 698f0b34 convert-hf : remove unused n_dims in extra_*_tensors
| | | | | * c33775bc convert : upgrade to sentencepiece v0.2.0
| | | | | *   0d720acb Merge branch 'master' into compilade/convert-hf-refactor
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | f364eb6f switch to using localizedDescription (#7010)
* | | | | | 77e15bec metal : remove deprecated error code (#7008)
* | | | | | a68a1e7e metal : log more info on error (#6987)
* | | | | | 9c67c277 ggml : add Flash Attention (#5021)
* | | | | | 952d03db convert : use utf8 encoding (#7000)
* | | | | | 8843a98c Improve usability of --model-url & related flags (#6930)
* | | | | | b8c1476e Extending grammar integration tests (#6644)
* | | | | | 5539e6fd main : fix typo in comment in main.cpp (#6985)
| | | | | * 47e02eb7 convert-hf : begin refactoring write_tensor
| |_|_|_|/  
|/| | | |   
* | | | | b8a7a5a9 build(cmake): simplify instructions (`cmake -B build && cmake --build build ...`) (#6964)
| | | | | * c240ae23 ci : fix arg order
| | | | | * e180fcd3 metal : fix max nsg
| | | | | *   ca0275ce Merge branch 'master' into gg/flash-attn
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | d2c898f7 ci : tmp disable gguf-split (#6983)
| | | | | *   a1616e9f Merge branch 'master' into gg/flash-attn
| | | | | |\  
| | | | |_|/  
| | | |/| |   
| | | | | * 9e387606 llama : add static reminder for llama_state_get_size
| | | | | * 4f4c0249 metal : remove tmp log
| | | | | * 1e590ac3 llama : update llama_state_get_size after v_trans field
| | | | | * 0fc5c5eb llama : disallow incompatible states
| | | | | * bab346ba llama : fix copy-paste errors, add TODO
| | | | | * c225609f llama : llama_kv_cache_clear zeroes data + fix save-load seq
| | | | | * ac1c6d91 ci : add CUDA save-load-state tests
| | | | | *   09d0381c Merge branch 'master' into gg/flash-attn
| | | | | |\  
| | | | | * | 1fd5bc3d llama : support save/load state with FA enabled
| | | | | * |   cb3547ac Merge branch 'master' into gg/flash-attn
| | | | | |\ \  
| | | | | * | | ff2c64a9 tests : remove TMP_ATTN_BENCH
| | | | | * | |   1f77f497 Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \  
| | | | | * | | | ce281b90 llama : disable FA for AMD
| | | | | * | | |   8937ec53 Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \ \  
| | | | | * | | | | 751591d5 server : add help for --flash-attn arg
| | | | | * | | | | d228bf85 cont
| | | | | * | | | | 56657e52 llama : fix n_batch requirements
| | | | | * | | | | 19e8982f llama : prep ALiBi support for BERT models
| | | | | * | | | | 78d363b0 llama : replace bool need_kq_pos with use_alibi
| | | | | * | | | | 3864eea4 ggml : add TODO's for F16/F32 mask/pos support in other backends
| | | | | * | | | | c1293697 cuda : try to fix __hgt2_mask
| | | | | * | | | | c70bfd7b cuda : "constexpr dim3" -> "const dim3"
| | | | | * | | | | 5408d555 cuda : uint -> uint32_t
| | | | | * | | | | f725ca90 ggml : ggml_soft_max support F16/F32 mask/pos
| | | | | * | | | | c11d05fe llama : force disable flash attention for incompatible models
| | | | | * | | | | cb76d747 ggml : fix num dimensions in ggml_flash_attn_ext
| | | | | * | | | | a39217d4 common : print --flash-attn in help
| | | | | * | | | | 871fcb6e ggml : fix soft_max with bias on CPU
| | | | | * | | | | 3badef1f ggml : fix avx512 const correctness
| | | | | * | | | | 52945429 tests : remove benchmarks
| | | | | * | | | |   29f6ad8d Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \ \ \  
| | | | | * | | | | | bc346166 metal : minor
| | | | | * | | | | | 1a88565b metal : clean-up kernel code
| | | | | * | | | | | 97eaece7 metal : clean-up
| | | | | * | | | | | 703c6e65 ggml : fix arm fp16 store on windows
| | | | | * | | | | | e32b2817 llama : adapt build_olmo to changes
| | | | | * | | | | |   1db66c1d Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \ \ \ \  
| | | | | * | | | | | | 74d57f95 llama : simplify llama_build_kv_store
| | | | | * | | | | | | 9ca86987 batched-bench : add fattn arg
| | | | | * | | | | | | c16a7c26 metal : use F32 attention accumulators
| | | | | * | | | | | |   fa9e8c66 Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \ \ \ \ \  
| | | | | * | | | | | | | 105332cc metal : add BS=1 kernel for flash attention (#6508)
| | | | | * | | | | | | | 260cdb2d llama-bench : add -fa,--flash-attn arg
| | | | | * | | | | | | | 87968de9 fix KQ FP32 precision fpr parallel_blocks > 1
| | | | | * | | | | | | | 2f538b95 Add __hgt2_mask implementation for CUDA 11
| | | | | * | | | | | | | 0bc67dd1 Calculate KQ as FP32 if KQV has GGML_PREC_F32
| | | | | * | | | | | | | a5b0e2de store temp KQ in registers
| | | | | * | | | | | | | ef9e1593 flush softmax exp below threshold to 0
| | | | | * | | | | | | | 6a3b8423 fix flash_attn_vec_f16 race condition
| | | | | * | | | | | | | 34f93bbb CUDA: refactor host code, dyn. par. blocks
| | | | | * | | | | | | | 5668c79e server: bench: enable flash_attn param
| | | | | * | | | | | | | 40538572 server: support flash_attn param
| | | | | * | | | | | | | 599ce84a llama : flash_attn cparam + fix defrag
| | | | | * | | | | | | |   2c41180e Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \ \ \ \ \ \  
| | | | | * \ \ \ \ \ \ \ \   89961dea Merge branch 'master' into gg/flash-attn
| | | | | |\ \ \ \ \ \ \ \ \  
| | | | | * | | | | | | | | | ee19a4ab fix KV cache padding, NaN from INFINITY (#6438)
| | | | | * | | | | | | | | | c63dfdf7 fix cmake build
| | | | | * | | | | | | | | | bb0d51ac fix excessive KQ_b loads
| | | | | * | | | | | | | | | e1ecd3b1 fix compile warnings
| | | | | * | | | | | | | | | 3f777acf Multiple parallel blocks for batch size 1
| | | | | * | | | | | | | | | 68d793be no ncols == 64
| | | | | * | | | | | | | | | cca6d027 4 warps, 256 stride for all D
| | | | | * | | | | | | | | | 269374ed adjust kernel selection logic
| | | | | * | | | | | | | | | 81da9198 no vec for hs, no hs==256 ncols==32 for Volta
| | | | | * | | | | | | | | | d59ac670 16 cols for Phi-2
| | | | | * | | | | | | | | | 75aa7b4b CUDA: faster FlashAttention, kernel for bs == 1
| | | | | | | | | | | | | | | * 5ddad95e ci : tmp disable gguf-split
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 544f1f10 ggml : fix __MSC_VER -> _MSC_VER (#6977)
* | | | | | | | | | | | | | | ffe66657 llava-cli : multiple images (#6969)
| |_|/ / / / / / / / / / / /  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 24affa7d readme : update hot topics
* | | | | | | | | | | | | | f4ab2a41 llama : fix BPE pre-tokenization (#6920)
* | | | | | | | | | | | | | 3f167476 sampling : use std::random_device{}() for default random seed (#6962)
* | | | | | | | | | | | | | 3055a418 convert : fix conversion of some BERT embedding models (#6937)
* | | | | | | | | | | | | | 577277ff make : change GNU make default CXX from g++ to c++ (#6966)
* | | | | | | | | | | | | | ca7f29f5 ci : add building in MSYS2 environments (Windows) (#6967)
* | | | | | | | | | | | | | c4f708a9 llama : fix typo LAMMAFILE -> LLAMAFILE (#6974)
* | | | | | | | | | | | | | e00b4a8f Fix more int overflow during quant (PPL/CUDA). (#6563)
* | | | | | | | | | | | | | 7bb36ccf gguf : enforce that tensor names are unique (#6905)
* | | | | | | | | | | | | | ce023f6f add device version in device list (#6959)
| | | | | | | | | | | | | | * 80cb3127 tests : disable test-tokenizer-1-bpe due to slowness
| | | | | | | | | | | | | | * 3202676f llama : more prominent warning for old BPE models
| | | | | | | | | | | | | | * 6d6ce939 tests : use faster bpe test
| | | | | | | | | | | | | | * 9a7d430f tests : disable obsolete
| | | | | | | | | | | | | | * 120cf37d models : add phi-3, mpt, gpt-2, starcoder
| | | | | | | | | | | | | | * c21ab183 scripts : ignore new update script in check-requirements.sh
| | | | | | | | | | | | | | * af05268c unicode : cleanup
| | | | | | | | | | | | | | * c68d2596 tests : add more vocabs and tests
| | | | | | | | | | | | | | * 43708d22 tests : refactor vocab tests
| | | | | | | | | | | | | | * ef4cca9e cmake : refactor test targets
| | | | | | | | | | | | | | * 7b1210f6 lint : fix
| | | | | | | | | | | | | | * 78081502 convert : exercise contractions
| | | | | | | | | | | | | | * 0f9058ce convert : add comments
| | | | | | | | | | | | | | * 02fd977f convert : remove unused functions
| | | | | | | | | | | | | | * e8dd4a14 lint : fix
| | | | | | | | | | | | | | * 491f2339 lint : fix
| | | | | | | | | | | | | | * 1545550e unicode : normalize signatures
| | | | | | | | | | | | | | * 1c888eb4 convert : add falcon
| | | | | | | | | | | | | | * 4e3e6d8e lint : update
| | | | | | | | | | | | | | * 76429736 convert : add convert-hf-to-gguf-update.py
| | | | | | | | | | | | | | * ee6d1b3f unicode : simplify
| | | | | | | | | | | | | | * e972e6cb unicode : clean-up
| | | | | | | | | | | | | | *   d63cc906 Merge branch 'master' into gg/bpe-preprocess
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 6e472f58 flake.lock: Update
* | | | | | | | | | | | | | | 4dba7e81 Replace "alternative" boolean operator in conditional compilation directive (#6949)
* | | | | | | | | | | | | | | b7368332 ci: server: tests python env on github container ubuntu latest / fix n_predict (#6935)
* | | | | | | | | | | | | | | 928e0b70 Reset schedule earlier to allow overlap with ggml graph computation on device (#6933)
* | | | | | | | | | | | | | | 0c4d489e quantize: add imatrix and dataset metadata in GGUF (#6658)
* | | | | | | | | | | | | | | 017e6999 add basic tensor data validation function (#6884)
| | | | | | | | | | | | | | * b97add52 unicode : category support via std::regex
| | | | | | | | | | | | | | * 581c4a02 unicode : try fix windows
| | | | | | | | | | | | | | * 91eaa414 unicode : support \p{N}, \p{L} and \p{P} natively
| | | | | | | | | | | | | | * ce5485ae unicode : always use std::wregex
| | | | | | | | | | | | | | * 2affd0b2 unicode : set bomb
| | | | | | | | | | | | | | * a22645c2 unicode : set bomb
| | | | | | | | | | | | | | * 4434c9d6 minor
| | | | | | | | | | | | | | * ad929833 llama : adapt punctuation regex + add llama 3 regex
| | | | | | | | | | | | | | * 96965f67 models : add llama v3 vocab file
| | | | | | | | | | | | | | * c160818e wip
| | | | | | | | | | | | | | * a774d708 make : add test-tokenizer-0-llama-v3
| | | | | | | | | | | | | | * 8791e94e lint : fix
| | | | | | | | | | | | | | * 1b9b79dd convert : fix pre-tokenizer type writing
| | | | | | | | | | | | | | * 43e12ce8 llama : use new pre-tokenizer type
| | | | | | | | | | | | | | * 9b4d63ae convert : add "tokenizer.ggml.pre" GGUF KV (wip)
| | | | | | | | | | | | | | *   e3f6dc74 Merge branch 'master' into gg/bpe-preprocess
| | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | e2764cd7 gguf : fix mismatch between alloc and free functions (#6929)
* | | | | | | | | | | | | | | 4b1c3c98 llamafile : use 64-bit integers in sgemm (#6928)
* | | | | | | | | | | | | | | bbe3c6e7 ci: server: fix python installation (#6925)
* | | | | | | | | | | | | | | 7f5ff558 server: stop generation at `n_ctx_train` if `n_predict` is not set (#6638)
* | | | | | | | | | | | | | | 9e4e077e ci: server: fix python installation (#6922)
| | | | | | | | | | | | | | * e9891769 unicode : first try custom implementations
| | | | | | | | | | | | | | * e8c206be unicode : shot in the dark to fix tests on Windows
| | | | | | | | | | | | | | * 4907e41a llama : towards llama3 tokenization support (wip)
| | | | | | | | | | | | | | * ed42711b gguf-py : reader prints warnings on duplicate keys
| | | | | | | | | | | | | | * e1b2bf78 tests : add sample usage
| | | | | | | | | | | | | | * aeafb43e tests : remove and rename tokenizer test scripts
| | | | | | | | | | | | | | * d999cf65 unicode : remove redundant headers
| | | | | | | | | | | | | | * 7a44e443 tests : add tokenizer tests for numbers
| | | | | | | | | | | | | | * c56e19db lint : fix whitespaces
| | | | | | | | | | | | | | * 06d3e693 unicode : fix? unicode_wstring_to_utf8
| | | | | | | | | | | | | | * 36d98326 Fixed issue with gpt2 regex custom preprocessor
| | | | | | | | | | | | | | * 75358036 Fixed issues
| | | | | | | | | | | | | | * feeaf4f3 Added needed functionality, testing remains
| | | | | | | | | | | | | | * 7e308ed2 Adding unicode regex function
| | | | | | | | | | | | | | * a5710a41 Adding unicode regex mappings
| | | | | | | | | | | | | | * 4c3e882a Refactored code
| | | | | | | | | | | | | | * c8e7d952 Updated/merged the deepseek coder pr
| | | | | | | | | | | | | | * 4056dc5b added and refactored unicode_regex_split and related functions
| | | | | | | | | | | | | | * 1c924e4b Resolved issues
| | | | | | | | | | | | | | * 54f93eb5 Moved header files
| | | | | | | | | | | | | | * d2cfc222 Moved regex patterns to unicode.cpp and updated unicode.h
| | | | | | | | | | | | | | * 6fbab2db merged the changes from deepseeker models to main branch
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 83b72cb0 Merge pull request from GHSA-p5mv-gjc5-mwqv
* | | | | | | | | | | | | | d4a9afc1 ci: server: fix python installation (#6918)
* | | | | | | | | | | | | | 7d641c26 ci: fix concurrency for pull_request_target (#6917)
* | | | | | | | | | | | | | 5790c8da bench: server add stop word for PHI-2 (#6916)
* | | | | | | | | | | | | | 46e12c46 llava : add support for moondream vision language model (#6899)
* | | | | | | | | | | | | | dba497e0 cmake : restore LLAMA_LLAMAFILE_DEFAULT
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | fa0b4ad2 cmake : remove obsolete ANDROID check
* | | | | | | | | | | | | d6e1d44f llama : synchronize before get/set session data (#6911)
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 853d06ff ci : tmp disable slow tests
* | | | | | | | | | | | 3fe0596c readme : update model list (#6908)
* | | | | | | | | | | | 0ead1f10 llama : check that all the tensor data is in the model file (#6885)
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 51543729 ggml : fix redefinition of vaddvq_f32 for 32-bit ARM (#6906)
* | | | | | | | | | | 4ab99d8d clip : rename lerp function to avoid conflict (#6894)
* | | | | | | | | | | 54770413 ggml : fix MIN / MAX macros (#6904)
| | | | | | | | | | | * 8c259f6f ggml : fix MIN / MAX macros
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | aa750c1e tests : minor bash stuff (#6902)
* | | | | | | | | | | 1966eb26 quantize : add '--keep-split' to quantize model into shards (#6688)
* | | | | | | | | | | 784e11de README: add graphic for matrix multiplication (#6881)
* | | | | | | | | | | b4e4b8a9 llama : add llama_get_pooling_type function (#6862)
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 3fe847b5 server : do not apply Markdown formatting in code sections (#6850)
* | | | | | | | | | 37246b10 common : revert showing control tokens by default for server (#6860)
* | | | | | | | | | 28103f48 Server: fix seed for multiple slots (#6835)
* | | | | | | | | | c0d1b3e0 ggml : move 32-bit arm compat in ggml-impl.h (#6865)
* | | | | | | | | | abd33140 llama : add phi 3 chat template (#6857)
* | | | | | | | | | 3fec68be convert : add support of codeqwen due to tokenizer (#6707)
* | | | | | | | | | c8297c6a llama : add phi3 support (#6852)
| | | | | | | | | | * 5dcccb3a convert : fix tokenizer conversion
| | | | | | | | | | * 17327372 convert : add phi-3 support
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 4e96a812 [SYCL] Windows default build instructions without -DLLAMA_SYCL_F16 flag activated (#6767)
* | | | | | | | | | 192090ba llamafile : improve sgemm.cpp (#6796)
* | | | | | | | | | e931888d ggml : fix calloc argument ordering. (#6820)
* | | | | | | | | | 8960fe86 llama : fix typo in <|im_end|> token text (#6745)
* | | | | | | | | | c0956b09 ci: fix job are cancelling each other (#6781)
* | | | | | | | | | e9b4a1bf flake.lock: Update
* | | | | | | | | | 5cf5e7d4 `build`: generate hex dump of server assets during build (#6661)
* | | | | | | | | | 40f74e4d llama : add option to render special/control tokens (#6807)
* | | | | | | | | | b9cc76d8 ggml : fix ggml_backend_cpu_supports_op() for CPY (#0)
* | | | | | | | | | 7dbdba56 llama : add llama-3 chat template (#6751)
* | | | | | | | | | c1386c93 gguf-py : add IQ1_M to GGML_QUANT_SIZES (#6761)
* | | | | | | | | | e8d35f47 doc : add link to falcon (#6789)
* | | | | | | | | | 2cca09d5 readme : add Fedora instructions (#6783)
* | | | | | | | | | 89b0bf0d llava : use logger in llava-cli (#6797)
* | | | | | | | | | b97bc396 llama : support Llama 3 HF conversion (#6745)
* | | | | | | | | | b8109bc0 doc : server tests require llama to be built with curl enabled (#6788)
* | | | | | | | | | aed82f68 common : try to fix Android CI (#6780)
* | | | | | | | | | 0e4802b2 ci: add ubuntu latest release and fix missing build number (mac & ubuntu) (#6748)
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 637e9a86 server: static: upstream upgrade (#6765)
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 9958c81b Implement the OLMo architecture (#6741)
* | | | | | | | 8b1b1f49 train : add general name (#6752)
* | | | | | | | bca40e98 fix wrong parameter in cmd in readme-sycl.md (#6755)
| | | | | | | | * 124e4dce Update
| | | | | | | | * 907df445 Hack test-bench
| | | | | | | | | * 37507069 llama : add llama_token_is_eog()
| | | | | | | | | * f3105b9e Accept suggestion
| | | | | | | | | * 112c4c4e style
| | | | | | | | | * d79ab101 Support Llama 3 conversion
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 0d56246f ggml : group all experts in a single ggml_mul_mat_id (#6505)
* | | | | | | | | 03c0946d convert : support models with multiple chat templates (#6588)
| |_|_|_|_|_|/ /  
|/| | | | | | |   
* | | | | | | | e11b2e6e Qwen2 : assume tied weights if lm_head/output weights is missing (#6738)
* | | | | | | | c71bfd73 llama : fix compatibility with old 2 expert models (#6735)
* | | | | | | | 3b8f1ec4 llamafile : tmp disable + build sgemm.o when needed (#6716)
* | | | | | | | 8dd1ec8b readme : add UI (#6724)
| |_|_|_|_|/ /  
|/| | | | | |   
* | | | | | | facb8b56 convert : fix autoawq gemma (#6704)
* | | | | | | 532c1737 llama : make general.name optional (#6709)
* | | | | | | 666867b7 ggml : fix llamafile sgemm wdata offsets (#6710)
| | | | | | | * f02ea667 ggml : temporary disable llamafile sgemm until fixed
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 8cc91dc6 ggml : add llamafile sgemm (#6414)
* | | | | | | dbceec87 llama : add StableLM2 12B (#6635)
* | | | | | | f4dea7da llama : add qwen2moe (#6074)
| | | | | | | * eedd42e3 KV Cache defrag hash overflow - TMP Fix by @slaren #6685
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 8a56075b gritlm : add --outdir option to hf.sh script (#6699)
* | | | | | | 58227ffd perplexity : require positive --ctx-size arg (#6695)
* | | | | | | 4fbd8098 gguf : add special tokens metadata for FIM/Infill (#6689)
* | | | | | | 7593639c `main`: add --json-schema / -j flag (#6659)
* | | | | | | 132f5579 llama : fix restoring the number of outputs from state files (#6687)
* | | | | | | 3272896d server : revert "minor layout improvements" (#6684)
* | | | | | | 7fc16a2c swift : linux support (#6590)
* | | | | | | 17e98d4c fix mul_mat_id() for new input, make the ut pass (#6682)
* | | | | | | 1958f7e0 llama : add missing kv clear in llama_beam_search (#6664)
* | | | | | | 04fbc5f2 Add Command R chat template (#6650)
* | | | | | | f184dd92 flake.lock: Update (#6669)
* | | | | | | 422c2aff Added support for GGML_OP_CLAMP in Metal (#6662)
* | | | | | | 8800226d Fix --split-max-size (#6655)
* | | | | | | e689fc4e [bug fix] convert github repository_owner to lowercase (#6673)
* | | | | | | a4ec34e1 convert : enable the `--use-temp-file` cli flag (#6645)
* | | | | | | de17e3f7 fix memcpy() crash, add missed cmd in guide, fix softmax (#6622)
* | | | | | | b5e7285b CUDA: fix matrix multiplication logic for tests (#6667)
* | | | | | | 4bd0f93e model: support arch `DbrxForCausalLM` (#6515)
* | | | | | | ab9a3240 JSON schema conversion: ⚡️ faster repetitions, min/maxLength for strings, cap number length (#6555)
* | | | | | | fbbc030b metal : unify mul_mv_id kernels (#6556)
* | | | | | | 4cc120c7 infill : add download instructions for model (#6626)
* | | | | | | 24ee66ed server : coherent log output for KV cache full (#6637)
* | | | | | | 91c73601 llama : add gguf_remove_key + remove split meta during quantize (#6591)
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 5c4d767a chore: Fix markdown warnings (#6625)
* | | | | | ef21ce4c imatrix : remove invalid assert (#6632)
| | | | | | * 8b495540 imatrix : remove invalid assert
| |_|_|_|_|/  
|/| | | | |   
* | | | | | dee7f8d6 Correct free memory and total memory. (#6630)
* | | | | | 81da18e7 eval-callback: use ggml_op_desc to pretty print unary operator name (#6631)
* | | | | | 9ed2737a ci : disable Metal for macOS-latest-cmake-x64 (#6628)
* | | | | | 04a5ac21 Optimization: eliminate addition of redundant stacks when advancing grammar. (#6616)
* | | | | | f7001ccc As suggested by @slaren, disabling Metal for test to fix CI build on OSX from #6576 (#6619)
* | | | | | a474f50e Refactor Error Handling for CUDA (#6575)
* | | | | | cbaadc92 grammars: 1.5x faster inference w/ complex grammars (vector reserves / reuses) (#6609)
* | | | | | 1bbdaf6e ci: download artifacts to release directory (#6612)
* | | | | | f4183afe scripts : add --outdir option to hf.sh (#6600)
* | | | | | b804b1ef eval-callback: Example how to use eval callback for debugging (#6576)
* | | | | | 8228b66d gguf : add option to not check tensor data (#6582)
* | | | | | b3a96f27 minor layout improvements (#6572)
* | | | | | 4f407a0a llama : add model types for mixtral (#6589)
* | | | | | 65c64dc3 convert.py : add consolidated.safetensors for mixtral 8x22b (#6587)
* | | | | | 67fac4b9 docs : how to add a model (#6565)
* | | | | | 29122d32 readme : fix ROCm link (#6579)
* | | | | | b231b37b readme : update UI list (#6560)
| |/ / / /  
|/| | | |   
* | | | | ba5e134e readme: fix typo in amdgpu target name (#6573)
* | | | | 1b67731e BERT tokenizer fixes (#6498)
* | | | | c4a3a4ff sync : ggml
* | | | | 400d5d72 server : detect search query to start webchat (#6554)
* | | | | 5dc9dd71 llama : add Command R Plus support (#6491)
* | | | | e11a8999 license : update copyright notice + add AUTHORS (#6405)
* | | | | cc4a9542 llama : fix attention layer count sanity check (#6550)
* | | | | cecd8d3c Comment explaining a decision (#6531)
* | | | | b73e564b quantize : fix precedence of cli args (#6541)
* | | | | e3c337d8 llama : support negative ith in llama_get_ API (#6519)
* | | | | beea6e1b llama : save and restore kv cache for single seq id (#6341)
* | | | | 87fb5b42 remove row=1 cond (#6532)
* | | | | d752327c Adding KodiBot to UI list (#6535)
* | | | | 855f5440 Change Windows AMD example to release build to make inference much faster. (#6525)
* | | | | b909236c flake.lock: Update (#6517)
* | | | | e0717e75 Add GritLM as supported models. (#6513)
* | | | | c3724779 sync : ggml
* | | | | f77261a7 ggml: bypass code incompatible with CUDA < 11.1 (whisper/2020)
* | | | | 43e8995e scripts : sync ggml-cuda folder
* | | | | 9472bce3 Run make to build the project (#6457)
* | | | | d4f220a5 support/fix OPs GGML_TYPE_IQ4_NL, GGML_TYPE_IQ4_XS, GGML_TYPE_IQ3_XXS, GGML_TYPE_IQ3_S, GGML_TYPE_IQ2_XXS, GGML_TYPE_IQ2_XS, GGML_TYPE_IQ2_S, GGML_TYPE_IQ1_S, GGML_TYPE_IQ1_M (#6521)
* | | | | 54ea0698 sync : ggml
* | | | | b66aec67 backend : fix typo in scheduler documentation (ggml/781)
* | | | | 57dd02c4 Tests: Added integration tests for GBNF parser  (#6472)
* | | | | 75cd4c77 ci: bench: support sse and fix prompt processing time / server: add tokens usage in stream OAI response (#6495)
* | | | | a8bd14d5 gguf.py : add licence and version to gguf writer (#6504)
* | | | | d0f5deeb readme : update UI list (#6503)
* | | | | 87e21bba bench : make n_batch and n_ubatch configurable in Batched bench (#6500)
* | | | | 1b496a74 [SYCL] Fixed minor bug when enabling FP16 for non intel targets (#6464)
| |_|_|/  
|/| | |   
| | | | * 072e0a4d scipts : add LICENSE and gen-authors.sh to sync
| | | | * 0e0d4e82 authors : update
| | | | * 805d7050 license : add AUTHORS
| | | | | * a37696d4 speculative : more robust tokenizer comparison
| | | | | * 92591c12 examples : rely on new behavior of add_special
| | | | | * d1a1b614 spm : fix special_add_bos default
| | | | | * 45983e3a convert : remove now-unused ignore_nonllama parameter
| | | | | * 909f6be2 convert scripts : fix python 3.8 compatibility
| | | | | * 6a9d3c09 convert : fix Tensor type annotations
| | | | | *   0d052cbe Merge branch 'master' into ceb/bert-tokenizer-fixes
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | a307375c readme : add Dot to UI list (#6487)
* | | | | | b660a572 readme : fix typo (#6481)
* | | | | | 0a1d889e server: add cURL support to server Dockerfiles (#6474)
* | | | | | 7dda1b72 ci: exempt master branch workflows from getting cancelled (#6486)
* | | | | | c666ba26 build CI: Name artifacts (#6482)
* | | | | | 2e66913e server: allow penalizing repetition of newlines on server webpage (#6431)
* | | | | | 8120efee ci: bench fix concurrency for workflow trigger dispatch with sha1 (#6478)
* | | | | | a74401f0 Correct README link (#6458)
* | | | | | 7a2c9263 ci: bench: add more ftype, fix triggers and bot comment (#6466)
* | | | | | 4bcd6b95 common: remove duplicate check for curl (#6471)
* | | | | | 9b84ae18 examples : add GBNF validator program (#5948)
* | | | | | 4399f13f server : remove obsolete --memory-f32 option
* | | | | | 1a43c725 server : add option to disable KV offload (#6468)
* | | | | | 72d73af6 convert : fix for lint error complaining of bare except (#6470)
|/ / / / /  
* | | | | 5fb1574c A few small fixes to server's README docs (#6428)
* | | | | 60cdf40c server : handle exception on wrong type in request (#6452)
* | | | | bb43cf7e llama : add SEA-LION support (#6448)
* | | | | 9f62c017 ci : update checkout, setup-python and upload-artifact to latest (#6456)
* | | | | 5d4f12e4 server: add cURL support to `server.Dockerfile` (#6461)
* | | | | 154d4ee3 readme : add feature-rich rust bindings (#6465)
* | | | | e69945d9 security : create policy (#6354)
* | | | | db214fa5 Missing tokenizer.model error during gguf conversion (#6443)
* | | | | 1ff4d9f3 Add OpenChat, Alpaca, Vicuna chat templates (#6397)
* | | | | 076b0864 readme : update hot topics
* | | | | 08a0c020 ggml : mul_mat_id use the same tensor for all the experts (#6387)
* | | | | 52604860 [SYCL] Disable iqx on windows as WA (#6435)
* | | | | f87f7b89 flake.lock: Update (#6402)
* | | | | 33a52448 compare-llama-bench.py: fix long hexsha args (#6424)
* | | | | 226e8193 ci: server: verify deps are coherent with the commit (#6409)
* | | | | c50a82ce readme : update hot topics
| |_|/ /  
|/| | |   
* | | | 37e7854c ci: bench: fix Resource not accessible by integration on PR event (#6393)
* | | | c342d070 Fedora build update (#6388)
* | | | f7fc5f6c split: allow --split-max-size option (#6343)
* | | | ba0c7c70 Vulkan k-quant mmq and ggml-backend offload functionality (#6155)
* | | | d48ccf3a sync : ggml (#6351)
* | | | 06957477 [Model] Add support for xverse (#6301)
* | | | cfde806e ci : fix BGE wget (#6383)
* | | | b9102879 readme : add project (#6356)
* | | | 80939870 cmake : add explicit metal version options (#6370)
* | | | 057400a3 llama : remove redundant reshape in build_kv_store (#6369)
* | | | b75c3816 convert : allow conversion of Mistral HF models (#6144)
* | | | bfe7dafc readme : add notice for UI list
| | | * 88035827 llama : handle added special tokens like HF does
| | | * 748fc8ba convert-hf-to-gguf : fix BERT abuse of LlamaHfVocab
| | | | * 4c190ba6 cuda : reduce registers
| | | | * 5dd355fe cuda : bump nwarps by 1
| | | |/  
| | |/|   
| | * | 08e69c50 cuda : adapt soft_max to F16 mask and pos
| | * |   3e318e76 Merge branch 'master' into gg/flash-attn
| | |\ \  
| |_|/ /  
|/| | |   
* | | | 5106ef48 [SYCL] Revisited & updated SYCL build documentation (#6141)
| |_|/  
|/| |   
* | | be55134a convert : refactor vocab selection logic (#6355)
* | | 66ba5602 llava : fix MobileVLM (#6364)
* | | 0308f5e3 llama : fix command-r inference when omitting outputs (#6367)
* | | 28cb9a09 ci: bench: fix master not schedule, fix commit status failed on external repo (#6365)
| | * 57c03b78 metal : improve perf via smaller int registers
| | * 6be02b59 cuda : fix build
| | *   013721df Merge branch 'master' into gg/flash-attn
| | |\  
| | * | e425810b tests : add hs=256
| | * | 09532120 ggml : fix CPU soft_max
| | * | 3a468e6f llama : fix type of KQ_mask and KQ_pos
| | * |   9495d398 Merge branch 'master' into gg/flash-attn
| | |\ \  
| | * | | 58c7f616 ggml : fix F16 store (ARM NEON)
| | * | |   e307882c Merge branch 'master' into gg/flash-attn
| | |\ \ \  
| | * | | | 6aefd112 llama : adapt new models to F16 KQ_mask
| | * | | |   02a645e7 Merge branch 'master' into gg/flash-attn
| | |\ \ \ \  
| | * | | | | f249c997 llama : adapt to F16 KQ_pos
| | * | | | |   31109ca0 Merge branch 'master' into gg/flash-attn
| | |\ \ \ \ \  
| | * \ \ \ \ \   6875997f Merge branch 'master' into gg/flash-attn
| | |\ \ \ \ \ \  
| | | | | | | | | * 64b7d858 llama : fix command-r inference
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | cfc4d75d doc: fix outdated default value of batch size (#6336)
* | | | | | | | | 6902cb7f server : stop gracefully on SIGTERM (#6348)
* | | | | | | | | d2d8f389 nix: removed unnessesary indentation
* | | | | | | | | d39b308e nix: moved blas availability check to package inputs so it is still overridable
* | | | | | | | | c8739766 using blas.meta.available to check host platform
* | | | | | | | | dbb03e2b only using explicit blas if hostPlatform is allowed
* | | | | | | | | e9f17dc3 nix: .#windows: proper cross-compilation set-up
* | | | | | | | | 22a462cc nix: package: don't introduce the dependency on python
* | | | | | | | | f6a0f5c6 nix: .#widnows: init
* | | | | | | | | d0e2f641 doc: fix typo in MobileVLM-README.md (#6181)
* | | | | | | | | 25f4a613 [SYCL] fix set main gpu crash (#6339)
* | | | | | | | | a016026a server: continuous performance monitoring and PR comment (#6283)
* | | | | | | | | 53c7ec53 nix: ci: dont test cuda and rocm (for now)
* | | | | | | | | e5b89a44 ggml : fix bounds checking of zero size views (#6347)
* | | | | | | | | 3a034597 make : whitespace
* | | | | | | | | 1e13987f embedding : show full embedding for single prompt (#6342)
* | | | | | | | | e82f9e2b [SYCL] Fix batched impl for NVidia GPU (#6164)
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | cbc83436 Make IQ1_M work for QK_K = 64 (#6327)
* | | | | | | | e562b971 common : change --no-penalize-nl to --penalize-nl (#6334)
* | | | | | | | 2ab4f00d llama2c : open file as binary (#6332)
* | | | | | | | 1740d6dd readme : add php api bindings (#6326)
* | | | | | | | 0642b22c server: public: use relative routes for static files (#6325)
* | | | | | | | a4f569e8 [SYCL] fix no file in win rel (#6314)
* | | | | | | | 32c8486e wpm : portable unicode tolower (#6305)
* | | | | | | | 557410b8 llama : greatly reduce output buffer memory usage (#6122)
* | | | | | | | 55c1b2a3 IQ1_M: 1.75 bpw quantization (#6302)
* | | | | | | | e097633f convert-hf : fix exception in sentencepiece with added tokens (#6320)
* | | | | | | | d25b1c31 quantize : be able to override metadata by key (#6321)
* | | | | | | | deb72401 embedding : adjust `n_ubatch` value (#6296)
* | | | | | | | 3d032ece server : add `n_discard` parameter (#6300)
| | | | | | | | * 87a6088f rename unicodedata.{cpp,h} to unicode-data.{cpp,h}
| | | | | | | | * 0a0ef09a zig: add unicodedata.cpp
| | | | | | | | * bb27cd95 swift : add unicodedata.cpp
| | | | | | | | * 89e60cbf make : fix unicodedata.o build
| | | | | | | | * b460e7f5 wpm : portable unicode tolower
| | | | | | | | * e5ddf2fc llama : split unicodedata.cpp from unicode.cpp
| | | | | | | | * b80c0af0 wpm : use C locale for ispunct/isspace
| | | | | | | | | * 9c5fd6be minor : spacing
| | | | | | | | | * fc4c2a6f quantize: be able to override metadata by key
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | e190f1fc nix: make `xcrun` visible in Nix sandbox for precompiling Metal shaders (#6118)
* | | | | | | | | 28034596 cuda : rename build flag to LLAMA_CUDA (#6299)
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | b06c16ef nix: fix blas support (#6281)
* | | | | | | | 1f2fd4e7 tests : include IQ2_XXS and IQ2_XS in test-quantize-fns (#6303)
| | | | | | | | * 6f20e267 Include IQ2_XXS and IQ2_XS in teet-quantize-fns
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 43139cc5 flake.lock: Update (#6266)
* | | | | | | | 2f34b865 cuda : fix LLAMA_CUDA_F16 build (#6298)
| | | | | | | | * 210e4691 cuda : fix LLAMA_CUDA_F16 build
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | ae1f211c cuda : refactor into multiple files (#6269)
* | | | | | | | ad3a0505 Server: clean up OAI params parsing function (#6284)
* | | | | | | | 95ad616c [SYCL] fix SYCL backend build on windows is break by LOG() error (#6290)
* | | | | | | | 64e7b47c examples : add "retrieval" (#6193)
* | | | | | | | 7733f0c7 ggml : support AVX512VNNI (#6280)
* | | | | | | | a32b77c4 Fix heap corruption from wmode out-of-bound writes on windows (#6272)
* | | | | | | | a0e584de imatrix : fix wname for mul_mat_id ops (#6271)
* | | | | | | | 7aed0ffe Fixed lookup compilation issues on Windows (#6273)
* | | | | | | | ea279d56 ci : close inactive issue, increase operations per run (#6270)
* | | | | | | | 586e7bc5 sampling : deduplicated code for probability distribution access (#6240)
* | | | | | | | ddf65685 [SYCL] offload op (#6217)
* | | | | | | | d03224ac Support build win release for SYCL  (#6241)
* | | | | | | | 94d1b3b4 use _wfopen instead of fopen on Windows (#6248)
* | | | | | | | 95562175 gitignore : gguf-split
* | | | | | | | f482bb2e common: llama_load_model_from_url split support  (#6192)
* | | | | | | | 1997577d server: docs: `--threads` and `--threads`, `--ubatch-size`, `--log-disable` (#6254)
* | | | | | | | 476b0251 llama : add grok-1 support (#6204)
* | | | | | | | 21cad01b split: add gguf-split in the make build target (#6262)
* | | | | | | | 1b26aebe server: flush stdout after logging in both text and json layout (#6253)
* | | | | | | | 50ccaf5e lookup: complement data from context with general text statistics (#5479)
| | | | | | | | * d05c13b3 llama : fix BPE LF token on MSVC
| | | | | | | | * 6f4fd8f1 use wide versions of file path functions on Windows
| | | | | | | | * 14eebe23 ggml : fix missing #defines before windows.h
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 56a00f0a common : default --hf-file to --model (#6234)
* | | | | | | | 92397d87 convert-llama2c-to-ggml : enable conversion of GQA models (#6237)
* | | | | | | | 1d0331c1 quantize: options for output and token embedding tensors qtype (#6239)
* | | | | | | | dba1af61 llama_model_loader: support multiple split/shard GGUFs (#6187)
* | | | | | | | ee804f62 ci: apply concurrency limit for github workflows (#6243)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 80bd33bc common : add HF arg helpers (#6234)
* | | | | | | e80f06d2 llama : correction of the attn.v.weight quantization for IQ3_XS (#6209)
* | | | | | | f77a8ffd tests : conditional python & node json schema tests (#6207)
* | | | | | | 72114edf json-schema-to-grammar : fix order of props + non-str const/enum (#6232)
* | | | | | | 2f0e81e0 cuda : add LLAMA_CUDA_NO_PEER_COPY to workaround broken ROCm p2p copy (#6208)
| | | | | | | * 0e826d12 quantize: be able to specify the token embedding tensor type
| | | | | | | * 7883796f quantize: be able to specify the output tensor type
| | | | | | | | * 8c3d5b5a common : remove defaults
| | | | | | | | * 1b2f0a9e common : add HF arg helpers
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 29ab270e readme : add RecurseChat to the list of UIs (#6219)
* | | | | | | | 6b8bb3a3 server : fix n_keep always showing as 0 in response (#6211)
* | | | | | | | 68e210b3 server : enable continuous batching by default (#6231)
* | | | | | | | b3e94f26 metal : proper assert for mat-mat memory alignment (#6225)
| |_|_|_|_|_|/  
|/| | | | | |   
| | | | | | | * 12aa74ba minor : spacing
| | | | | | | * 2605c139 Update build.yml
| | | | | | | * 3e9d3dbf Update build.yml
| | | | | | | * 6014a631 Update build.yml
| | | | | | | * 927be9b5 add test in build action
| | | | | | | * 284800b1 convert-llama2c-to-ggml: enable conversion of multiqueries, #5608
| | | | | | | | * 31f2d03f server : enable continuous batching by default
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | b2075fd6 ci : add CURL flag for the mac builds (#6214)
| | | | | | | | * 072c56fc metal : fix the fix
| | | | | | | | * 3966d681 readme : add notice about the bug fix
| | | | | | | | * 2f8be164 metal : proper assert for mat-mat memory alignment
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 95d576b4 metal : pad n_ctx by 32 (#6177)
* | | | | | | | 59c17f02 add blog link (#6222)
* | | | | | | | fa046eaf Fix params underscore convert to dash. (#6203)
* | | | | | | | be07a032 server : update readme doc from `slot_id` to `id_slot` (#6213)
* | | | | | | | d0a71233 cuda : disable host register by default (#6206)
| | | | | | | | * a710d58d Try fix quantized k-cache on ROCm
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | f372c49c Corrected typo to wrong file (#6199)
* | | | | | | | 924ce1dc tests : disable system() calls (#6198)
* | | | | | | | 03a8f8fa cuda : fix LLAMA_CUDA_F16 build (#6197)
* | | | | | | | cfd3be76 ggml : same IQ4_NL quantization for CPU/CUDA/Metal (#6196)
* | | | | | | | 5b7b0ac8 json-schema-to-grammar improvements (+ added to server) (#5978)
* | | | | | | | 1943c019 ci : fix indentation error (#6195)
* | | | | | | | 5e43ba87 build : add mac pre-build binaries (#6182)
| | | | | | | | * 68e4fed4 Now fix test-quantize-fns
| | | | | | | | * 30eef31b Make quantize_row_iq4_nl do the same thing is quantization on CUDA
| | | | | | | | * cd4a7c4c Make quantize_row_iq4_nl do the same thing is quantization on CUDA
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 76aa30a2 Add ability to use Q5_0, Q5_1, and IQ4_NL for quantized K cache (#6183)
* | | | | | | | c5b8595e Add nvidia and amd backends (#6157)
* | | | | | | | 42e21c68 cuda : fix conflict with std::swap (#6186)
* | | | | | | | 1c51f98a cuda : print the returned error when CUDA initialization fails (#6185)
* | | | | | | | f9c7ba34 llava : update MobileVLM-README.md (#6180)
* | | | | | | | 272935b2 llava : add MobileVLM_V2 backup (#6175)
* | | | | | | | ccf58aa3 cuda : refactor to remove global resources (#6170)
* | | | | | | | 91f8ad16 Server: version bump for httplib and json (#6169)
* | | | | | | | 6b7e76d2 gitignore : ignore curl-related files
* | | | | | | | bc0baab2 server : allow to override -ngl in tests (#6170)
* | | | | | | | d795988d Revert "llava : add a MobileVLM_V2-1.7B backup (#6152)"
* | | | | | | | f8c4e745 llava : add a MobileVLM_V2-1.7B backup (#6152)
* | | | | | | | 47cc7a7b Server: Handle n_keep parameter in the request (#6174)
* | | | | | | | bd60d82d server tests : more pythonic process management; fix bare `except:` (#6146)
* | | | | | | | 6c0b2877 update readme sycl for new update (#6151)
* | | | | | | | d26e8b66 increase igpu cluster limit (#6159)
| | | | | | | | * 9a424a38 server : fix tests expecting old repeat penalty
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | d8b009a9 Remove undeed header file. (#6158)
* | | | | | | | d0d5de42 gguf-split: split and merge gguf per batch of tensors (#6135)
* | | | | | | | b80cf3b2 common : disable repeat penalties by default (#6127)
* | | | | | | | 970a4806 ci : exempt some labels from being tagged as stale (#6140)
* | | | | | | | 4c28b825 common : print usage on '-h' and '--help' (#6145)
* | | | | | | | 2d15886b flake.lock: Update
* | | | | | | | d199ca79 mpt : implement backwards compatiblity with duped output tensor (#6139)
* | | | | | | | 104f5e0f clip : fix memory leak (#6138)
* | | | | | | | 5e1b7f94 backend : set max split inputs to GGML_MAX_SRC (#6137)
* | | | | | | | ac9ee6a4 ci : disable stale issue messages (#6126)
* | | | | | | | 4f6d1337 ci : temporary disable sanitizer builds (#6128)
* | | | | | | | 2bf8d0f7 backend : offload large batches to GPU (#6083)
* | | | | | | | 496bc79b common : tidy-up argument parsing (#6105)
* | | | | | | | 9b03719a convert : add support for CamembertModel architecture (#6119)
* | | | | | | | 3a6efdd0 convert : use f32 outtype for bf16 tensors (#6106)
* | | | | | | | d01b3c4c common: llama_load_model_from_url using --model-url (#6098)
* | | | | | | | cd776c37 ci : close all stale issues at once (#6115)
* | | | | | | | dc0f6125 ggml:fix finding transfer queue family index error (#6094)
* | | | | | | | c47cf414 ggml : add AVX512F SIMD (#6088)
* | | | | | | | b5f4ae09 gritlm : add initial README.md (#6086)
* | | | | | | | dfbfdd60 readme : add wllama as a wasm binding (#6100)
* | | | | | | | 15961ec0 common : refactor nested if causing error C1061 on MSVC (#6101)
* | | | | | | | a56d09a4 ci : close inactive issue with workflow (#6053)
* | | | | | | | d84c4850 llama : fix Baichuan2 13B (#6092)
* | | | | | | | 877b4d0c llama : add support for control vectors (#5970)
* | | | | | | | 12247f4c llama : add Command-R support (#6033)
* | | | | | | | 4e9a7f7f llava : change API to pure C style for Rust FFI bindgen (#6079)
* | | | | | | | 3020327f cuda : disable unused cudaLaunchHostFunc code (#6078)
* | | | | | | | 46acb367 fix set main gpu error (#6073)
* | | | | | | | 131b0584 make : ggml-metal.o depends on ggml.h
* | | | | | | | 753e36f6 [SYCL] Fix non-intel device selection (#6042)
* | | | | | | | 7ce2c77f gguf : add support for I64 and F64 arrays (#6062)
* | | | | | | | aab606a1 llama : add Orion chat template (#6066)
* | | | | | | | b0bc9f4a llama-bench : use random tokens to improve accuracy with mixtral (#6069)
* | | | | | | | 4755afd1 llama : fix integer overflow during quantization (#6063)
* | | | | | | | 6e0438da gguf : fix resource leaks (#6061)
* | | | | | | | 72710770 gguf-py : bump version to 0.8.0 (#6060)
* | | | | | | | 69ff6139 llama : support models without vocabulary (#5798)
* | | | | | | | 044ec4b2 embedding : add EOS token if not present (#899)
| | | | | | | | * 0a9bc301 control-vectors : minor code style updates
| | | | | | | | *   42abb46c Merge branch 'master' into vgel/repeng
| | | | | | | | |\  
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 77178eed gguf-py : fix dtype check (#6045)
* | | | | | | | | 15a33326 readme : improve readme for Llava-1.6 example (#6044)
* | | | | | | | | 43241adf server: disable debug release type sanitizer, simplify trigger (#6047)
* | | | | | | | | a44bc969 llama : fix typo
* | | | | | | | | 2c4fb692 llama : optimize defrag moves + fix fragmentation calculation (#6037)
* | | | | | | | | 3ca23481 gguf-py : add support for I8, I16 and I32 (#6045)
* | | | | | | | | 3fe8d7a1 ggml : designate enum vals for integer types (#6050)
* | | | | | | | | 68265ebf embedding : print all resulting embeddings (#899)
* | | | | | | | | 381da2d9 metal : build metallib + fix embed path (#6015)
* | | | | | | | | 0fd6c1f0 embedding : print cosine similarity (#899)
* | | | | | | | | 19885d20 readme : update details about running llama in Termux on Android (#6039)
* | | | | | | | | 76a936c8 readme : update API changes and hot topics
* | | | | | | | | 46362837 grammar : handle missing "root" node (#6004)
* | | | | | | | | f30ea47a llama : add pipeline parallelism support (#6017)
* | | | | | | | | d8fd0ccf test-backend-ops : skip CPU backend by default (#6028)
* | | | | | | | | b3d97860 Update get version (#6025)
* | | | | | | | | 99b71c06 Server: Use multi-task for embeddings endpoint (#6001)
* | | | | | | | | 306d34be ci : remove tidy-review (#6021)
| | | | | | | | * 6b905660 control vector api and implementation
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | * abf0afd0 ci : fix iOS builds to use embedded library
| | | | | | | | * ed0f77b1 metal : fix embeded library build
| | | | | | | | * 35d5a02b metal : fix embed build + update library load logic
| | | | | | | | * 34cdece3 metal : build metallib + fix embed path
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | * 9f805264 Attempt 2
| | | | | | | | * 9188523f iq1_s[SYCL]: remove unnecessary (unused) data
| | | | | | | | * da5a6f05 iq1_s: attempt to fix SYCL
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 8030da7a ggml : reuse quantum structs across backends (#5943)
* | | | | | | | 184215e7 ggml : fix UB in IQ2_S and IQ3_S (#6012)
* | | | | | | | 48358b2e sycl : update IQ1_S kernels (WIP - not working!) (#5995)
* | | | | | | | 5cdb3717 grammar : fix unnecessarily retained pointer to rules (#6003)
* | | | | | | | 44ca159f 1.5 bit: we can do even better (#5999)
* | | | | | | | 05b06210 llama : more consistent names of count variables (#5994)
* | | | | | | | 83796e62 llama : refactor unicode stuff (#5992)
* | | | | | | | 828defef Update server docker image URLs (#5997)
| | | | | | | | * 5440a127 iq1_s: fix dequantize on the CPU
| | | | | | | | * 436c65e1 iq1_s: very slightly faster dequantize on Metal
| | | | | | | | * da4528bc iq1_s: make Metal work with new version
| | | | | | | | * 4fba3e00 iq1_s: make Neon work with new version.
| | | | | | | | * c09f7349 iq1_s: make scalar and AVX2 work with the new version
| | | | | | | | * 82380acf iq1_s: we can do even better
| | | | | | | | | * 76be02ae sycl : fix grid type
| | | | | | | | | * cb5a702e sycl : iq1s_grid -> iq1s_grid_gpu
| | | | | | | | | * 77d586f5 sycl : try to fix after IQ1_S changes
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | caa106d4 Server: format error to json (#5961)
* | | | | | | | | 3202361c ggml, ci : Windows ARM runner and build fixes (#5979)
* | | | | | | | | 332bdfd7 server : maintain chat completion id for streaming responses (#5988)
* | | | | | | | | ecab1c75 cmake : fix subdir for `LLAMA_METAL_EMBED_LIBRARY` (#5985)
* | | | | | | | | ee35600b llama : fix F16/F32 downcast + improve names (#5980)
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | be858f62 Better 1.5 bit quantization  (#5971)
* | | | | | | | ef3ced26 [SYCL] Add q3_s and q1_s (#5886)
| | | | | | | | *   989e15b3 Merge branch 'master' into sycl_q3s_q1s
| | | | | | | | |\  
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 3814a073 [SYCL] Add support for SYCL Nvidia target (#5738)
* | | | | | | | | bb6d00bb metal : move mm_id indices to shared mem (#5982)
* | | | | | | | | 7ab7b733 android : fix utf8 decoding error (#5935)
* | | | | | | | | d9f65c97 readme : update hot topics
* | | | | | | | | b838b53a sync : ggml
* | | | | | | | | df4dc3e7 ggml : try fix 32-bit arm compat (whisper/1938)
* | | | | | | | | bf47a5ee ggml : remove __constant__ specifier for CUDA tables (#5940)
* | | | | | | | | fa8a809a server: ci: windows build and tests (#5968)
* | | | | | | | | bcebd7db llama : add support for GritLM (#5959)
* | | | | | | | | 2960eae8 grammar : verify parsed state (#5950)
* | | | | | | | | c7854147 nix: update flake.lock (#5969)
* | | | | | | | | 621e86b3 server: benchmark: chat/completions scenario and other llm servers comparison (#5941)
* | | | | | | | | 77d1ac7e server : print chat template info
* | | | | | | | | d894f352 perplexity : support using multiple sequences to allow larger batch sizes (#5946)
* | | | | | | | | 098dbaab readme : update hot topics
* | | | | | | | | 8380ecfb ggml : fix unnecessary f32 -> f16 -> f32 casts (mmla) (#5951)
* | | | | | | | | 58308a0e server : fix metrics init (#5964)
* | | | | | | | | 5b097973 ggml : remove old quantization functions (#5942)
* | | | | | | | | 97c09585 server : clarify some items in the readme (#5957)
* | | | | | | | | fb215c38 server : normalize embeddings (#5956)
* | | | | | | | | 2c4f566c tests : gitignore ggml-common.h
* | | | | | | | | 0db32bea server : fix passing prompt as tokens (#5955)
* | | | | | | | | 8a3012a4 ggml : add ggml-common.h to deduplicate shared code (#5940)
* | | | | | | | | 9674aaf3 server : simplify logic for empty prompts (#5953)
* | | | | | | | | 950ba1ab Server: reorganize some http logic (#5939)
* | | | | | | | | e1fa9569 server : add SSL support (#5926)
* | | | | | | | | fd72d2d2 server: tests: add truncated prompt tests, better kv cache size (#5933)
* | | | | | | | | c2101a2e llama : support Mamba Selective State Space Models (#5328)
* | | | | | | | | 515f7d0d llama : fix quantization of shared token_embd (#5944)
* | | | | | | | | 76e86882 server: metrics: add llamacpp:prompt_seconds_total and llamacpp:tokens_predicted_seconds_total, reset bucket only on /metrics. Fix values cast to int. Add Process-Start-Time-Unix header. (#5937)
* | | | | | | | | e457fb35 llama : assume tied weights if lm_head/output weights is missing (#5824)
* | | | | | | | | af37fd8b server : fix EOS token detection with disabled cache (#5938)
* | | | | | | | | 581ed5c4 log : fix MSVC compile errors (#5643)
| | | | | | | | * 7bb53142 increase grid space
| | | | | | | | * 94f33d7a rm macro
| | | | | | | | * c810047b enable ops
| | | | | | | | * 50f9ba35 fix build
| | | | | | | | * 600193ca fix build
| | | | | | | | * c9995363 fix build
| | | | | | | | * 6fd581e0 fix compilation
| | | | | | | | * ad251954 Add q3_s and q1_s
| | | | | | | | | * b54afce9 mostly style fixes; fix KQ_mask comment
| | | | | | | | | * 03acc82a Clean-up GritLM sample code.
| | | | | | | | | * bd3d9fbf allow to toggle embedding mode
| | | | | | | | | * f618e506 add to gitignore
| | | | | | | | | * 1ab6aeee gritlm embeddings are back babeee
| | | | | | | | | * 97936078 rebase to new embed
| | | | | | | | | * 805ae529 comment out debug printing
| | | | | | | | | * a71842d7 tabs to spaces
| | | | | | | | | * e79195fc gritlm results match
| | | | | | | | | * 4be8fb18 add gritlm example
| | | | | | | | | | * 0ba20ed9 llama : compute BERT graph with F16 K, V
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 6cdabe65 llama-bench : add embeddings option (#5924)
* | | | | | | | | | 89fb735f Revert "[SYCL] fix error when set main gpu to non-zero (#5901)" (#5918)
* | | | | | | | | | 55a2a900 server : add `/v1/completions` endpoint (#5914)
* | | | | | | | | | 2002bc96 server : refactor (#5882)
| | | | | | | | | | * b5b02703 Revert "[SYCL] fix error when set main gpu to non-zero (#5901)"
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | ceca1aef [SYCL] fix error when set main gpu to non-zero (#5901)
* | | | | | | | | | e04e04f8 ggml : use SYS_get_cpu if SYS_getcpu is not defined (#5906)
* | | | | | | | | | e25fb4b1 ggml : use `uint8x16_t` return type for `ggml_vqtbl1q_u8` (#5894)
* | | | | | | | | | 1e35d619 convert : remove AWQ remnants (#5768)
* | | | | | | | | | 8ced9f7e add wait() to make code stable (#5895)
* | | | | | | | | | 652ca2bd compare-llama-bench.py : remove mul_mat_q (#5892)
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | bd836944 quants : use MM256_SET_M128I consistently to fix gcc 7 build (#5889)
* | | | | | | | | 3de31677 grammars : blacklists character control set (#5888)
* | | | | | | | | 82cb31eb Revert "grammars : don't allow to output unescaped new line in string (#5885)"
* | | | | | | | | b1a4e994 grammars : don't allow to output unescaped new line in string (#5885)
* | | | | | | | | 61d1c88e Vulkan Improvements (#5835)
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 21b08674 [SYCL] fix mul_mat fault in CI/unit-test (#5862)
* | | | | | | | 6a87ac3a fix editorconfig check break (#5879)
* | | | | | | | 29eee404 fix speculative decoding build on windows (#5874)
* | | | | | | | 1d41d6f7 nix: static build (#5814)
* | | | | | | | 29ae62d2 llama : fix embeddings (#5796)
* | | | | | | | e0843afe flake : fix
* | | | | | | | a1c6d96e ggml : fix unknown status (#0)
* | | | | | | | efd8533e sync : ggml
* | | | | | | | 9fa26273 ggml : introduce ggml_status (ggml/750)
| |_|_|_|_|/ /  
|/| | | | | |   
* | | | | | | fe52be11 cmake : handle cases where git index is not found in .git (#5844)
* | | | | | | 6d341ab6 speculative : implement stochastic speculative sampling (#5625)
* | | | | | | 4ffcdce2 add alias for chat template (#5858)
| | | | | | | * 31cecc87 iq3_s_mult_shuffle: use lookup table on Metal
| | | | | | | * 93034df7 iq3_s_mult_shuffle: use lookup table on CUDA
| | | | | | | * 6d15da1e iq3_s_mult_shuffle: use new multiplier and cleanup
| | | | | | | * b1d753be iq3_s_mult: remove SLOW_MULT option
| | | | | | | * a6a263b9 iq3_s_mult_shuffle: works on ARM_NEON and Metal
| | | | | | | * b5874822 iq3_s_mult_shuffle: mult + shuffle based codebook
| | | | | | | * b48bf8b4 iq3_s_mult: scalar dot product
| | | | | | | * f2c2bd6b iq3_s_mult: also CUDA
| | | | | | | * e5e72562 iq3_s_mult: back to blocks of 32
| | | | | | | * f4cb4eac iq3_s_mult: play with blocks of 16
| | | | | | | * dbe98dfe iq3_s_mult: another alternative multiplier
| | | | | | | * 8b713a98 iq3s_mult: quantization tuning
| | | | | | | * 5b9c8785 iq3s_mult: ARM and Metal
| | | | | | | * b6402fa7 iq3_s_mult: ifdef'd slow / fast versions
| | | | | | | * 726aed30 iq3_s_mult: alternative multiplier / bit twidling
| | | | | | | * fe3c20b2 iq3_s_mult: quantization tuning
| | | | | | | * 3000e0ac iq3_s_mult: Metal works - slower than lookup
| | | | | | | * bf90920f iq3_s_mult: ARM_NEON works - 13 t/s
| | | | | | | * 0fe9cd48 WIP
| | | | | | | * e43e81a5 WIP
| | | | | | | * 160aceca iq3_s_multiplier: CUDA and AVX2 works
| | | | | | | * 4c21c826 WIP
| | | | | | | * 1cc7cb2b iq3_s(multiplier): use SIMD also in dequantize
| | | | | | | * 9c752ff0 Trying IQ3_S without a lookup table
| | | | | | | | * 4ec0e9ab wip
| | | | | | | | * e66da356 llama : add pooling switch
| | | | | | | | * 9bbeb0f1 embeddings : fix llama_batch_init arg
| | | | | | | | * eb425962 llama : do not use KV cache for non-causal models
| | | | | | | | * d0347840 llama : fix embeddings
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | a0fc6266 sync : ggml
* | | | | | | | 7d43c585 add some new ops, fix some operators and add batch operations to certain operators. (ggml/747)
* | | | | | | | 82f3e668 common : use LLAMA_DEFAULT_SEED (#5855)
* | | | | | | | 5a51cc1b main : support special tokens as reverse/anti prompt (#5847)
* | | | | | | | 67be2ce1 cuda : fix data race in soft max (#5853)
| |_|_|_|/ / /  
|/| | | | | |   
* | | | | | | 231ae28f readme : add API changes section
* | | | | | | 475df1d6 llama : allow for user specified embedding pooling type (#5849)
* | | | | | | 87c2e8b2 gguf-dump : support i-quants (#5841)
* | | | | | | de9692a7 llama : fix llama_copy_state_data with fragmented KV cache (#5840)
* | | | | | | e6029348 ci : schedule slow server tests only on Release or on demand (#5839)
* | | | | | | 8ef969af server : init http requests thread pool with --parallel if set (#5836)
* | | | | | | fa974646 flake.lock: Update (#5842)
| | | | | | | * eb0bf32c server: tests: schedule slow dispatch only on release or on demand
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 97311342 server: tests: passkey challenge /  self-extend with context shift demo (#5832)
* | | | | | | 4a6e2d61 llama : add abort_callback to interrupt computation (#5409)
* | | | | | | 494c8703 ggml : fix IQ3_S AVX implementation (#5834)
* | | | | | | 4d4d2366 convert : automatically fall back to HfVocab if tokenizer.model doesn't exist (#5821)
* | | | | | | c7a0ad8e convert-hf : make model class definitions self-contained (#5825)
* | | | | | | bbde6eb2 ggml : IQ3_S improvements (#5829)
* | | | | | | ef2cd694 scripts : add pod-llama.sh
* | | | | | | 6c32d8c7 llama : refactor internal quantization functions (#5830)
* | | | | | | 802da009 llama : fix segfault from unknown model arch name (#5820)
* | | | | | | 71564139 Support multiple GPUs (split mode) on SYCL backend (#5806)
* | | | | | | 9bf297a0 workflows : remove nocleanup arg for check-requirements.sh (#5826)
* | | | | | | cb5e8f7f build(nix): Introduce flake.formatter for `nix fmt` (#5687)
* | | | | | | da3b9ba2 convert-hf-to-gguf : require einops for InternLM2ForCausalLM (#5792)
| | | | | | | * 0b673ca1 s/_MODEL_CLASSES/_model_classes/
| | | | | | | * 7f0a1d66 convert-hf : make model class definitions self-contained
| | | | | | | * 95845d17 convert-hf : make actual types match annotations
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | c29af7e2 llama : add StarCoder2 support (#5795)
* | | | | | | 38d16b14 server : remove api_like_OAI.py proxy script (#5808)
* | | | | | | c2224f00 ggml-vulkan: fix VULKAN_CHECK_RESULTS flag, which was previously broken (#5813)
| | | | | | | * d4dfc250 Fix ARM_NEON
| | | | | | | * 93bce3c9 iq3_s: use new grid everywhere
| | | | | | | * 11d4e099 iq3_s: PPL improvement
| | | | | | | * 7b629c3b iq3_s: minor improvement on Metal
| | | | | | | * 9c5b594c iq3_s: another small ARM_NEON improvement
| | | | | | | * 1e949891 iq3_s: somewhat faster ARM_NEON dot product
| | | | | | | * 39e3a429 iq3_s: somewhat faster AVX2 dot product
| | | | | | | | * f8ab5391 convert : update help string
| | | | | | | | * 767aef90 docs : s/LLaMa/LLaMA/
| | | | | | | | * 17d22efa convert : automatically fall back to HfVocab if needed
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | e7433867 gemma : fix bfloat16 -> float16 conversion issue (#5810)
* | | | | | | | f49a5356 common : fix flag `--logits-all` to `--all-logits` (#5805)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 3ab8b3a9 llama : cleanup unused mmq flags (#5772)
* | | | | | | 9600d59e unicode : switch to multimap based nfd_map (#5799)
* | | | | | | 5cb02b4a server: allow to override threads server pool with --threads-http (#5794)
* | | | | | | 6ea0f010 ci : add Ubuntu 22 Vulkan CI run (#5789)
* | | | | | | f105471e server : fix newlines in help (#5785)
* | | | | | | 38d15216 [SYCL] Use batched mul_mat pathway (#5591)
* | | | | | | 052051d8 Server: normalize naming (#5779)
| | | | | | | * 9862d59c llama : change starcoder2 rope type
| | | | | | | * b67b8f64 handle `rope-theta`
| | | | | | | * fdd886f7 remove redundant changes
| | | | | | | * 5c06625f Update llama.cpp
| | | | | | | * 10aa6e92 resolve comments
| | | | | | | * d62ce1c6 skip rope freq and rotary embeddings from being serialized
| | | | | | | * 6c108068 handle rope type
| | | | | | | * ab4eab3a Add support for starcoder2
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | d5ab2975 llama : constified `llama_set_state_data`'s `src` (#5774)
* | | | | | | 87c91c07 ci : reduce 3b ppl chunks to 1 to avoid timeout (#5771)
* | | | | | | 317709b2 make portability_enumeration_ext apple only (#5757)
* | | | | | | 08c5ee87 llama : remove deprecated API (#5770)
* | | | | | | 78aacf36 awq-py : remove (#5768)
* | | | | | | 8c0e8f4e sync : ggml
* | | | | | | 2774b0c9 add google magika inference example (ggml/748)
* | | | | | | 5f706718 Introduce backend GUIDs (ggml/743)
* | | | | | | a693bea1 server : hit Ctrl+C twice to exit (#5734)
* | | | | | | adcb12a9 llama : fix non-quantization of expert gating tensors (#5754)
* | | | | | | 177628bf llama : improve BERT tokenization (#5740)
* | | | | | | 6c441686 readme : add link to LLaVA 1.6 models (#5758)
* | | | | | | efc72253 server : add "/chat/completions" alias for "/v1/...` (#5722)
* | | | | | | 7c4263d4 ggml : make i-quants work with super-blocks of 64 (CPU,Metal) (#5760)
| |_|_|_|_|/  
|/| | | | |   
* | | | | | cb49e0f8 Attempt to fix android build (#5752)
| | | | | | * f0cbb6dd iq1_s: turn off SIMD implementation for QK_K = 64 (it does not work)
| | | | | | * 47d52b2b Q2_K: fixed bug in imatrix quantization for QK_K = 64
| | | | | | * 2540a290 Make CUDA compile with QK_K = 64
| | | | | | * de64e061 QK_K = 64 tests pass on ARM_NEON and Metal
| | | | | | * 28e6146c iq2_xs: attempt to fix AVX dot product for QK_K = 64
| | | | | | * 13ba37f1 WIP: make i-quants work for QK_K = 64
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 0becb22a IQ4_XS: a 4.25 bpw quantization (#5747)
| | | | | | * 14d75706 llama : add llama_kv_cache_compress (EXPERIMENTAL)
| |_|_|_|_|/  
|/| | | | |   
* | | | | | c24a2a6e cuda : replace remaining shfl_xor with calls to warp_reduce functions (#5744)
* | | | | | 1f30b7a9 ggml-quants : fix avx2 iq1_s vec_dot when compiled with gcc (#5742)
* | | | | | 9d533a77 llama : fix defrag bugs + add parameter (#5735)
* | | | | | cbbd1efa Makefile: use variables for cublas (#5689)
* | | | | | b11a93df fix server hangs on empty prompt (#5733)
* | | | | | a33e6a0d Adding IQ2_S and IQ2_M to complete coverage of the 2-3 bit quantization range (#5721)
* | | | | | 47bb7b48 CUDA: fix DEBUG_CUDA_MALLOC (#5729)
| |_|_|_|/  
|/| | | |   
* | | | | c4d7f817 readme : update ui list (#5731)
* | | | | e849078c [SYCL] Add support for soft_max ALiBi (#5639)
* | | | | 67fd3313 unicode : reuse iterator (#5726)
* | | | | 4804215c server: CI fix trailing space (#5728)
* | | | | 8a533f0d server: CI tests reduce build matrix (#5725)
* | | | | 269de86b llama : fix Gemma rope type (#5691)
* | | | | c3937339 flake.lock: Update
* | | | | e3965cf3 server: tests - slow inference causes timeout on the CI (#5715)
* | | | | 8b350356 server: docs - refresh and tease a little bit more the http server (#5718)
* | | | | bf08e006 llama : refactor k-shift implementation + KV defragmentation (#5691)
* | | | | f7625019 server : fix crash when system prompt is bigger than batch size (#5714)
* | | | | abbabc5e ggml-quants : provide ggml_vqtbl1q_u8 for 64bit compatibility (#5711)
* | | | | f1a98c52 make : fix nvcc version is empty (#5713)
* | | | | 7d548a18 readme : add Msty to UI list (#5618)
* | | | | 930b1780 server: logs - unified format and --log-format option (#5700)
* | | | | d52d7819 server: concurrency fix + monitoring - add /metrics prometheus compatible endpoint (#5708)
* | | | | 12894088 cmake : fix compilation for Android armeabi-v7a (#5702)
* | | | | ab336a9d code : normalize enum names (#5697)
* | | | | 69917dfa py : fix StableLM conversion after config.json changes (#5703)
* | | | | 9e359a4f server: continue to update other slots on embedding concurrent request (#5699)
* | | | | 4c4cb307 IQ3_S: a much better alternative to Q3_K (#5676)
* | | | | 525213d2 server: init functional tests (#5566)
* | | | | fd43d66f server : add KV cache quantization options (#5684)
* | | | | 54fbcd2c convert : fix missing ftype for gemma (#5690)
| | | | | * 608f4498 swift : fix build
| | | | | * fff1e8a5 batched.swift : fix build
| | | | | * 8772658b ggml : add I32 <-> F32 conversion
| | | | | * fc775366 llama : switch to floating-point token positions
| |_|_|_|/  
|/| | | |   
* | | | | 15499eb9 mpt : do not duplicate token_embd.weight on disk (#5670)
* | | | | 96633eec gemma : use more bits for the token_embd.weight tensor (#5650)
* | | | | 847eedbd py : add Gemma conversion from HF models (#5647)
* | | | | 7e4f339c ggml : always define ggml_fp16_t as uint16_t (#5666)
* | | | | 334f76fa sync : ggml
* | | | | efd56b1c ggml : 32-bit arm compat (whisper/1891)
* | | | | 201294ae nix: init singularity and docker images (#5056)
* | | | | 5a9e2f60 py : minor fixes (#5668)
* | | | | 373ee3fb Add Gemma chat template (#5665)
| | | | | * 56c04715 py : minor fixes
| |_|_|_|/  
|/| | | |   
* | | | | 4cb4d8b2 workflows: nix: hardcode cachix ids, build unconditionally (#5663)
* | | | | 3a03541c minor : fix trailing whitespace (#5638)
* | | | | 56d03d92 readme : update hot topics
* | | | | a46f5074 server : fallback to chatml, add AlphaMonarch chat template (#5628)
* | | | | c5688c62 server : clarify some params in the docs (#5640)
* | | | | 4ef245a9 mpt : add optional bias tensors (#5638)
* | | | | 973053d8 llama : fix loading models with shared tok_embd and output (#5651)
* | | | | 7c8bcc11 Add docs for llama_chat_apply_template (#5645)
| | | | | * 5271c756 llama : fix K-shift with quantized K (wip)
| |_|_|_|/  
|/| | | |   
* | | | | 7fe4678b llama : fix session save/load with quantized KV (#5649)
* | | | | ba2135cc gemma : allow offloading the output tensor (#5646)
* | | | | 89febfed examples : do not assume BOS when shifting context (#5622)
* | | | | 5022cf24 sync : ggml
* | | | | 1ecea255 server: health: fix race condition on slots data using tasks queue (#5634)
* | | | | a00a35ce readme : add LocalAI to the availables UI (#5629)
* | | | | eccd7a26 sync : ggml (#5633)
* | | | | c14f72db readme : update hot topics
* | | | | cc6cac08 llava : add --skip-unknown to 1.6 convert.py (#5632)
* | | | | 580111d4 llama : add `gemma` model (#5631)
* | | | | 88c46cbd [SYCL] conext add name (#5624)
* | | | | a14679cc IQ4_NL: 4-bit non-linear quants with blocks of 32 (#5590)
* | | | | 6560bed3 server : support llava 1.6 (#5553)
* | | | | 06bf2cf8 make : fix debug build with CUDA (#5616)
* | | | | 4ed8e4fb llava : add explicit instructions for llava-1.6 (#5611)
* | | | | 9c405c9f Server: use llama_chat_apply_template (#5593)
* | | | | 5207b3fb readme : update UI list (#5605)
* | | | | 8dbbd757 metal : add build system support for embedded metal library (#5604)
* | | | | c0a8c6db server : health endpoint configurable failure on no slot (#5594)
* | | | | b9111bd2 Update ggml_sycl_op_mul_mat_vec_q (#5502)
* | | | | 633782b8 nix: now that we can do so, allow MacOS to build Vulkan binaries
* | | | | 22f83f0c Enable Vulkan MacOS CI
* | | | | bb9dcd56 Refactor validation and enumeration platform checks into functions to clean up ggml_vk_instance_init()
* | | | | f50db6ae Add check for VK_KHR_portability_enumeration for MoltenVK support
* | | | | d8c05451 Add preprocessor checks for Apple devices.
* | | | | 42f664a3 Resolve ErrorIncompatibleDriver with Vulkan on MacOS.
* | | | | 5dde5408 Allow for Vulkan build with Accelerate.
* | | | | 40c3a6c1 cuda : ignore peer access already enabled errors (#5597)
|/ / / /  
* | | | f24ed14e make : pass CPPFLAGS directly to nvcc, not via -Xcompiler (#5598)
* | | | 9d679f0f examples : support minItems/maxItems in JSON grammar converter (#5039)
* | | | 1387cf60 llava : remove extra cont (#5587)
* | | | 6fd41379 llava : replace ggml_cpy with ggml_cont
* | | | 337c9cbd sync : ggml
* | | | a3145bdc ggml-alloc : apply ggml/731
* | | | 890559ab metal : option to embed MSL source into compiled binary (whisper/1842)
* | | | d0e3ce51 ci : enable -Werror for CUDA builds (#5579)
* | | | 68a6b98b make : fix CUDA build (#5580)
* | | | 70d45af0 readme : fix typo in README-sycl.md (#5353)
| |_|/  
|/| |   
| | | *   412735ec Merge branch 'master' into gg/metal-batched
| | | |\  
| |_|_|/  
|/| | |   
* | | | 13e2c771 cmake : remove obsolete sycl compile flags (#5581)
* | | | f53119ce minor : fix trailing whitespace (#5538)
* | | | 70847553 llava : avoid changing the original BakLLaVA model (#5577)
* | | | 4480542b baby-llama : allocate graphs in ggml_context (#5573)
* | | | 11b12de3 llama : add llama_chat_apply_template() (#5538)
* | | | 3a9cb4ca cuda, metal : fix nans in soft_max (#5574)
* | | | 769a716e readme : update (#5572)
* | | | f0d1fafc ggml : android and old glibc NUMA incompatibility bugfixes (#5557)
* | | | a0c2dad9 build : pass all warning flags to nvcc via -Xcompiler (#5570)
* | | | 14278f55 ggml : restore vec dot stride arg names (#5453)
| | | * 5a668ea0 metal : trying bs = 512 performance (wip)
| | | * e8b00e29 metal : fix NSG1 > 1
| | | * 845876d0 metal : works with ne00 % 4 == 0
| | | * e68e3254 metal : opts
| | | * 92a0c174 metal : initial working version
| | |/  
| | | * 47c662b0 fix some spaces added by IDE in math op
| | | * 60687340 rename n_ctx to kv_size
| | | * ef96e8b1 server: document the --ctx-size deprecation in server README.md
| | | * 9a069567 server: rename legacy --ctx-size to --kv-size
| |_|/  
|/| |   
* | | b1de9682 ci : fix wikitext url + compile warnings (#5569)
* | | 7ad554f9 metal : fix unused warnings (#0)
* | | 5ee99c32 common, server : surface min_keep as its own parameter (#5567)
* | | c145f8a1 server : slots monitoring endpoint (#5550)
* | | 689a091b sampling : do not set min_keep to n_probs (#5564)
* | | f3f28c53 cmake : fix GGML_USE_SYCL typo (#5555)
* | | e75c6279 server : enhanced health endpoint (#5548)
* | | 36376abe server : --n-predict option document and cap to max value (#5549)
* | | 66c1968f server : graceful server shutdown (#5244)
* | | 1dcc3fde common : fix ub (#5530)
* | | 5d3de51f ggml, common, examples, tests : fixed type arguments in printf (#5528)
* | | fc0c8d28 llava : update surgery script to not remove tensors (#5536)
* | | bd2d4e39 1.5 bit quantization (#5453)
* | | c8e0d7ef flake.lock: Update
* | | 8f1be0d4 ggml : add ALiBi support for ggml_soft_max_ext (#5488)
* | | 6e4e973b ci : add an option to fail on compile warning (#3952)
* | | d250c9d6 gitignore : update for CLion IDE (#5544)
* | | 5bf2b94d cmake : fix VULKAN and ROCm builds (#5525)
* | | d2819d55 scripts : add helpers script for bench comparing commits (#5521)
| | | * 974e3cad ggml : try another fix
| | | * e9caab61 ggml : no cpu_set_t on Android
| |_|/  
|/| |   
* | | 4cb07276 llava : removed excess free(NULL) operation (#5531)
* | | 65085c71 llama : minor fixed return int value (#5529)
* | | 6dcc02d2 server : add "samplers" param to control the samplers order (#5494)
* | | 5f5808ca server : fix system prompt cli (#5516)
* | | f486f6e1 ggml : add numa options (#5377)
* | | 60ed04cf llava : fix clip-model-is-vision flag in README.md (#5509)
* | | 594845aa ci : fix BERT model download and convert
* | | 4524290e Use correct type of pooling for embedding models (#5500)
* | | c06e45d7 clip : fix wrong loop condition
* | | 9060a1e9 cuda : print message when initialization fails (#5512)
* | | 9350a1cf scripts : add hf.sh helper script (#5501)
* | | 73122473 fix(gguf-py): special tokens are no longer skipped when add_<token>_token is set to false (#5487)
* | | 0d417712 llava : fix memory management bug (#5491)
* | | 7930a8a6 llaba : hotfix for llava-1.6 image number (#5495)
| | | * e856bfed hf : add support for --repo and --file
| | | * e834aa1f hf : add error logs
| | | * 303da634 scripts : add hf.sh helper scripts
| |_|/  
|/| |   
* | | 704359e2 vulkan: Find optimal memory type but with fallback (#5381)
* | | 594fca3f readme : fix typo (#5490)
* | | ccbb277f llava : update README.md (#5489)
* | | 8084d554 cmake : ARM intrinsics detection for MSVC (#5401)
* | | aa234129 llava : support v1.6 (#5267)
* | | f5ca0548 Early return for zero size calls to get_tensor. (#5482)
* | | 6c00a066 gguf : add python reader example (#5216)
* | | ea9c8e11 llama : add support for Nomic Embed (#5468)
| | | * ccd757a1 convert : fix mistakes from refactoring
| | | * c2f407e3 cleanup convert-hf-to-gguf.py
| | | * b8ff85ef convert : pad vocab size to multiple of 64, not 8
| | | * 48a7ef6e Nomic BERT
| |_|/  
|/| |   
* | | c4e6dd59 llama : allow raw byte in SPM vocabs; don't crash on nl 404 (#5478)
* | | 037259be llama : make load error reporting more granular (#5477)
* | | 26397890 finetune : rename feed-forward tensors (w1/w2/w3) (#4839)
* | | cf45252a tests : multi-thread the tokenizer tests (#5474)
* | | 03bf161e llama : support batched embeddings (#5466)
* | | ad014bba make: add error message for bad CUDA version (#5444)
| | | * 5c977221 iq1_s: slightly faster dot product
| | | * f604a179 iq1_s: Tests
| | | * 425c6bbb iq1_s: Metal works, but quite slow
| | | * 020b548e iq1_s: Metal basics
| | | * 4be44b7c iq1_s: use IQ2_XXS for attn_output
| | | * 307c5f61 iq1_s: better grid
| | | * 77301492 iq1_s: ARM_NEON dot product. Works, but not very fast
| | | * 2ffb05ac iq1_s: AVX2 finally works
| | | * 67e7c423 Fix after merge with latest master
| | | * dc0b14be Fix shadow warnings
| | | * 5574533a Fix tests
| | | * 592b3b26 iq1_s: WIP AVX2 dot product - something is not right
| | | * d94139bf iq1_s: scalar CPU dot product
| | | * a9d48e97 iq1_s: CUDA is working
| | | * 80cd5bae iq1_s: WIP basics
| |_|/  
|/| |   
* | | 49cc1f7d bert : add tests + fix quantization (#5475)
* | | 99b8b43d tests : disable moe test (#5473)
* | | 895407f3 ggml-quants : fix compiler warnings (shadow variable) (#5472)
| |/  
|/|   
| | * 4246b71a Fix compiler warnings (shadow variable)
| |/  
|/|   
* | 099afc62 llama : fix quantization when tensors are missing (#5423)
* | df334a11 swift : package no longer use ggml dependency (#5465)
* | dbd8828e py : fix persimmon `n_rot` conversion (#5460)
* | 43fe07c1 ggml-sycl: Replace 3d ops with macro  (#5458)
* | 4a46d2b7 llava : remove prog parameter from ArgumentParser (#5457)
* | 3b169441 sync : ggml (#5452)
* | 3bdc4cd0 CUDA: mul_mat_vec_q tiling, refactor mul mat logic (#5434)
* | 2891c8aa Add support for BERT embedding models (#5423)
* | 97a33650 flake.lock: Update
* | c88c74f9 vulkan: only use M-sized matmul on Apple GPUs (#5412)
* | a803333a common : use enums for sampler types (#5418)
* | 68478014 server : allow to specify tokens as strings in logit_bias (#5003)
* | 85910c5b main : ctrl+C print timing in non-interactive mode (#3873)
* | 139b62a8 common : fix compile warning
* | 0f2411f1 ggml : fix compile warnings (unused vars) (#4966)
* | a07d0fee ggml : add mmla kernels for quantized GEMM (#4966)
* | e4640d8f lookup: add print for drafting performance (#5450)
* | 907e08c1 server : add llama2 chat template (#5425)
* | f026f812 metal : use autoreleasepool to avoid memory leaks (#5437)
* | cd9aea63 scripts : update sync scripts with new backends
* | 43b65f5e sync : ggml
* | 4633d93a ggml : add abort_callback for cpu backend (ggml/725)
* | 4b7b38be vulkan: Set limit for task concurrency (#5427)
* | e00d2a62 llava : add requirements.txt and update README.md (#5428)
* | 7c777fcd server : fix prompt caching for repeated prompts (#5420)
* | e5ca3937 llama : do not cap thread count when MoE on CPU (#5419)
* | e4124c24 readme : add JavaScript/Wasm repo (#5415)
* | b2f87cb6 ggml : fix `error C2078: too many initializers` for MSVC ARM64 (#5404)
* | 44fbe343 Fix Vulkan crash on APUs with very little device memory (#5424)
* | 8e6a9d2d CUDA: more warps for mmvq on NVIDIA (#5394)
* | 41f308f5 llama : do not print "offloading layers" message in CPU-only builds (#5416)
* | 6e99f2a0 Fix f16_sycl cpy call from Arc (#5411)
* | ff4ff05c llava : add missing .py, and fix paths in README.md (#5414)
* | b7b74cef fix trailing whitespace (#5407)
* | 4aa43fab llama : fix MiniCPM (#5392)
* | a6e514a8 llava: fix typo/formatting in README.md (#5405)
* | 26d4efd1 sampling: fix top_k <= 0 (#5388)
* | 8504d2d0 tests : .gitignore obj files
* | c4fbb671 CMAKE_OSX_ARCHITECTURES for MacOS cross compilation (#5393)
* | 8c933b70 fix typo in readme (#5399)
* | b906596b Add Ava in the list of llama.cpp UIs (#4362)
* | aa7ab99b CUDA: fixed mmvq kernel for bs 2,3,4 and -sm row (#5386)
* | 10afa6f1 [SYCL] update install make by w64devkit (#5297)
* | 0ef46da6 llava-cli : always tokenize special tokens (#5382)
* | ee1628bd Basic Vulkan Multi-GPU implementation (#5321)
* | ed0bf322 readme : modernize (#5379)
* | 9a697d84 readme : update ui list (#5354)
* | 316c7faf llama : add MiniCPM support (#5346)
* | f3e2b4fa server : update `/props` with "total_slots" value (#5373)
* | f68664ac convert : fix TypeError on GPT-2 vocab.json (#5288)
* | 213d1439 server : remove model.json endpoint (#5371)
* | 17c97fb0 CUDA: mul_mat_vec_q max. batch size 8 -> 4 (#5370)
* | b08f22c8 Update README.md (#5366)
| | * 7286b83d BERT WIP
| |/  
|/|   
* | f57fadc0 Slight quantization improvement for Q4_K and Q5_K (#5361)
* | 2e9c0bd6 readme : add phi, orion 14b, internlm2, and yi-VL to readme (#5362)
* | 2c516611 CUDA: mul_mat_vec_q for batch sizes > 1 (#5351)
* | 8a79c591 server : include total "num_slots" in props endpoint (#5349)
* | 31e79032 server : add `dynatemp_range` and `dynatemp_exponent` (#5352)
* | 4ffc7a17 server : various fixes for the prompt field in /completion (#5300)
* | 906cff55 py : handle byte tokens in `get_token_type` (#5341)
* | 098f6d73 make: Use ccache for faster compilation (#5318)
* | 78b00dda README: updated introduction (#5343)
* | c6b39553 ggml : make use of ggml-quants.h possible in C++ code (#5338)
| | * adcf16fd py : fix empty bytes arg
| | * ded2ad5b py : handle byte tokens in `get_token_type`
| |/  
|/|   
* | abb61944 ggml : avoid duplicating function calls using MIN/MAX macros (#5325)
* | 89503dcb iq3_xxs: quards for the no-imatrix situation (#5334)
* | 7e1ae372 py : fix internlm2-hf convert to gguf (#5305)
* | 6fdfa2ec iq2_xxs: tune quantization (#5320)
* | a2d60c91 server : allow to get default generation settings for completion (#5307)
| | * 91c453fb One cannot possibly be defining static_assert in a C++ compilation
| | * 44bf9492 Make use of ggml-quants.h possible in C++ code
| |/  
|/|   
* | e6f81775 common : add dynamic temperature parameters to main example cli (#5295)
* | 30679d43 scripts : fix typos, cleanup (#5303)
* | 4be04c89 scripts : add non-interactive server-llm.sh (#5303)
* | 5d55b0cd readme : add CodeShell models to the supported models list (#5330)
* | 4833ac20 [SYCL] Fix cpy with dims of 3 (#5289)
* | 9392ebd4 flake.lock: Update
* | 5ed26e1f Adding some imatrix tools (#5302)
* | 277fad30 cmake : use set() for LLAMA_WIN_VER (#5298)
* | 3c0d25c4 make: add nvcc info print (#5310)
* | 3cc5ed35 make: fix nvcc optimization flags for host code (#5309)
* | 60ecf099 add Vulkan support to Nix flake
* | e920ed39 Vulkan Intel Fixes, Optimizations and Debugging Flags (#5301)
* | 52bb63c7 refactor : switch to emplace_back to avoid extra object (#5291)
* | 1ec3332a YaRN : store rope scaling type as int32_t in memory (#5285)
* | 6a66c507 readme : add tenere in the ui tools list (#5284)
* | a305dba8 Fix im2col with 32fp (#5286)
* | 19122117 perplexity : fix KL divergence calculations on Windows (#5273)
* | e437b37f scripts : parse wtype in server-llm.sh (#5167)
* | 2d40085c py : add check for '.attn.masked_bias' layers to GPT2model (#5281)
* | b05102fe Tidy ggml-sycl (#5261)
* | 6b91b1e0 docker : add build for SYCL, Vulkan + update readme (#5228)
* | e805f0fa [SYCL] get MAX_MEM_ALLOC from device property (#5270)
* | af3ba5d9 [SYCL] update guide of SYCL backend (#5254)
* | e1e72109 llama : fix memory leak in llama_batch_free (#5252)
* | 128dcbd3 add --no-mmap in llama-bench (#5257)
* | 4d0924a8 Vulkan Phi Fix for AMD Proprietary Drivers (#5260)
| | * 49a483e0 wip
| |/  
| | * a647257b cuda : express strides with helper constants
| |/  
| * 1846e92a cuda : minor
| * ef68fac2 cuda : fix matrix names
| * cfd9732b cuda : simplify softmax
| * e04ff391 cuda : fix -INF block check
| * 5b263dd8 cuda : unroll Q*K^T loop
| * 3b1c4e76 cuda : speed-up reduce part of the kernel
| * a7b47156 cuda : switch to 1 warp for bs > 16
| * b958151e cuda : use half2 in softmax
| * c51f27c0 cuda : avoid __hisinf branches
| * 92472ea2 cuda : unroll some of the loops
| * 1f8a5924 cuda : make loops use the same loop values
| * 7c34655b cuda : use int instead of int64_t
| * b150abe8 cuda : avoid warp_reduce for smax
| * b68a1122 cuda : fix __hisinf() result check
| * 12eaa226 tests : update dims
| * db1f3c48 cuda : avoid zeroing fragments
| * c6769b94 tests : minor fix
| * cda5a60a metal : optimize softmax
| * 56e45a23 metal : optimize softmax for C > 32
| *   41d136b6 Merge branch 'master' into gg/flash-attn
| |\  
| |/  
|/|   
* | 8ca511ca cuda : fix LLAMA_CUDA_F16 (#5262)
* | d71ac909 make : generate .a library for static linking (#5205)
* | ce320601 llama : support InternLM2 (#5184)
* | 1cfb5372 Fix broken Vulkan Cmake (properly) (#5230)
| * 5a19a9f6 cuda : add flash_attn kernel (wip)
| | * b957b8f5 cuda : add flash_attn kernel (wip)
| |/  
| * 2e460137 cuda : fix soft_max to use correct mask size
| * 910b15bb ggml : fix ggml_soft_max mask requirement
| | * ac26f270 cuda : increase C to 128 for better performance
| | * 9a5c2a16 cuda : switch to F16 scalars + tune warps for RTX 2060
| | * 2c04beeb cuda : avoid extra QxQ matrix in shared memory
| | * 71b69aa7 cuda : fix flash_attn kernel to produce same results as CPU
| | * fd878f71 cuda: mask as fp16
| | *   3df0b8d4 Merge branch 'gg/flash-attn' of https://github.com/ggerganov/llama.cpp into flash-attn-cuda
| | |\  
| | |/  
| |/|   
| * | 8ad92dc1 ggml : switch to padded F16 mask for ggml_soft_max, ggml_flash_attn_ext
| | * 0afe47fa fix naive implementation
| | * b1479dfb fix kernel
| | * 3b0f74b4 latest kernel update, wrong values
| | *   7980178a Merge branch 'gg/flash-attn' of https://github.com/ggerganov/llama.cpp into flash-attn-cuda
| | |\  
| | * | a1d5a12b fix compiler error
| | * | 2455a8d6 update impl
| | * |   7cea9735 Merge branch 'gg/flash-attn' of https://github.com/ggerganov/llama.cpp into flash-attn-cuda
| | |\ \  
| | * | | 0a481fe1 integrate tensor cores
| | * | | 6e7cb0ee update implementation
| | * | |   78da3387 Merge branch 'gg/flash-attn' of https://github.com/ggerganov/llama.cpp into flash-attn-cuda
| | |\ \ \  
| | * | | | 0fc36d87 match to metal impl
| | * | | | 972c2adc use half2 instead half4
| | * | | | 64168214 fix equivalent fp16 math functions, compiler error 'undefined'
| | * | | | 6374bc57 cuda: port metal version flash_attn_ext
| | * | | |   a689b02a Merge branch 'gg/flash-attn' of https://github.com/ggerganov/llama.cpp into flash-attn-cuda
| | |\ \ \ \  
| | * | | | | fded2e6a apply suggestions
| | * | | | |   09db1a7c Merge branch 'gg/flash-attn' of https://github.com/ggerganov/llama.cpp into flash-attn-cuda
| | |\ \ \ \ \  
| | * | | | | | e53de286 fix compilation
| | * | | | | | f7bcfb05 cuda: add flash attention + test
| | | | | | | | * 1ad42b1f ggml : ggml_soft_max uses F16 mask
| | |_|_|_|_|_|/  
| |/| | | | | |   
| * | | | | | |   2ddc9bbe Merge branch 'master' into gg/flash-attn
| |\ \ \ \ \ \ \  
| |/ / / / / / /  
|/| | | | | | |   
* | | | | | | | d3bac7d5 llama : reorder build_orion() at correct place (#5118)
* | | | | | | | 5cb04dbc llama : remove LLAMA_MAX_DEVICES and LLAMA_SUPPORTS_GPU_OFFLOAD (#5240)
* | | | | | | | efb7bdbb metal : add im2col F32 dst support (#5132)
* | | | | | | | 15606309 llava : add MobileVLM support (#5132)
* | | | | | | | b2b9f025 format license text, restore apache license by legal suggestion (#5233)
* | | | | | | | dabcc5b4 ggml : limit n_threads to the max n_tasks (#5238)
* | | | | | | | f8e9140c Vulkan Fixes (#5223)
* | | | | | | | d62520eb Fix typos of IQ2_XXS and IQ3_XXS in llama.cpp (#5231)
* | | | | | | | 01684139 support SYCL backend windows build (#5208)
* | | | | | | | e8dc55d0 kompute : llama-bench support and ggml_cpu_has_kompute() (#5226)
| * | | | | | |   3d03bcb7 Merge branch 'master' into gg/flash-attn
| |\ \ \ \ \ \ \  
| |/ / / / / / /  
|/| | | | | | |   
* | | | | | | | e0085fdf Revert "server : change deps.sh xxd files to string literals (#5221)"
* | | | | | | | e6f291d1 server : fix context shift (#5195)
* | | | | | | | 4003be0e server : change deps.sh xxd files to string literals (#5221)
* | | | | | | | fea4fd4b ggml : fix IQ3_XXS on Metal (#5219)
| * | | | | | | 78df5527 tests : ifdef
| * | | | | | | d073e4f9 metal : fix array initialization
| | |_|_|_|_|/  
| |/| | | | |   
| * | | | | | 5fcb9c1c metal : faster inner loop for C == 32
| * | | | | | c6c1132e tests : more
| * | | | | | abeaf0d9 metal : disable buffer allocation logs
| * | | | | | 4794821a tests : add ATTN tests
| * | | | | | 1db22d70 metal : support Q > 8
| * | | | | | 134c81c7 metal : minor
| * | | | | |   0ad44baf Merge branch 'master' into gg/flash-attn
| |\ \ \ \ \ \  
| * | | | | | | 86128641 ggml : fix f16 mad
| * | | | | | | 3a428a10 metal : improve precision
| * | | | | | | ecc466a4 metal : add tests, fix scaling, support C > 32
| * | | | | | | 77f6976a metal : move output into local memory + optimize
| * | | | | | |   b3dd7d97 Merge branch 'master' into gg/flash-attn
| |\ \ \ \ \ \ \  
| | |_|_|_|_|_|/  
| |/| | | | | |   
| * | | | | | | 6fea843b metal : add parallel reduce version (disabled)
| * | | | | | | f9ca5dcb llama : avoid ggml_cast, use F32 query
| | |_|_|_|_|/  
| |/| | | | |   
| * | | | | | 40ea8cd1 metal : fix comment
| * | | | | | 432ad04f metal : scale and mask in matrix form
| * | | | | | d917746d metal : avoid redundant loads of the attention
| * | | | | | 1446a12b metal : efficient flash_attn_f16 implementation
| | |_|_|_|/  
| |/| | | |   
| | | | | | * 719a0871 iq3_xxs: forgotten update of the grid points
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 8f8ddfcf sync : ggml (#0)
* | | | | | 6fb50ebb gguf : fix comparison (ggml/715)
* | | | | | 625a699b `ggml_cuda_cpy` support for 4d tensors and float16->float32 upcasting (ggml/686)
* | | | | | a4b07c05 gguf : add input validation, prevent integer overflows (ggml/709)
* | | | | | 549a1e6c ci : fix yolo URLs + fix metal capture (ggml/712)
* | | | | | 5f14ee0b metal : add debug capture backend function (ggml/694)
* | | | | | 8e14e3dd Faster AVX2 dot product for IQ2_XS (#5187)
* | | | | | f4d7e549 SOTA 3-bit quants  (#5196)
* | | | | | 2256f36b Vulkan Windows APU Memory Handling (#5199)
* | | | | | 7359016c quantize : fix typo (#5211)
* | | | | | 81341699 main : allow empty --prompt-cache file (#5176)
* | | | | | 5589921e readme : minor (#5204)
* | | | | | 49f44b5c readme : update hot topics
* | | | | | 6685cc41 server : improve README (#5209)
* | | | | | ceebbb5b ggml alloc: Fix for null dereference on alloc failure (#5200)
* | | | | | 6daa69ee kompute : fix fallback to CPU (#5201)
* | | | | | fbf1ddec Nomic Vulkan backend (#4456)
* | | | | | 2aed77eb fix typo "RLIMIT_MLOCK" (#5175)
* | | | | | c82d18e8 server : embeddings compatibility for OpenAI (#5190)
* | | | | | 14fef85e py : fix except (#5194)
* | | | | | e76627bc py : improve BPE tokenizer support (#5189)
* | | | | | fbe7dfa5 ggml : add max buffer sizes to opencl and metal backends (#5181)
* | | | | | 172ac826 cmake : fix Vulkan build (#5182)
| |_|/ / /  
|/| | | |   
* | | | | d2f650cb metal : free metal objects (#5161)
* | | | | 35dec26c sync : ggml
* | | | | d460510c ggml : minor type fix (int64_t -> size_t)
* | | | | 2307523d ggml : add Vulkan backend (#2059)
* | | | | 0f648573 ggml : add unified SYCL backend for Intel GPUs (#2690)
* | | | | b764b8f1 flake.lock: Update (#5162)
* | | | | 9241c3a2 Apply min_p to unsorted tokens (#5115)
| |/ / /  
|/| | |   
* | | | b2b2bf98 Tests for min_p, sampling queue (#5147)
* | | | af4980bf readme : add link to rust bindings (#5148)
* | | | f2e69d28 llama : add support for Orion-14B (#5118)
* | | | 39baaf55 docker : add server-first container images (#5157)
* | | | 6db2b41a llava : support for Yi-VL and fix for mobileVLM (#5093)
* | | | 753eafed sync : ggml
* | | | e9764230 ggml : check ggml_add src1 type (ggml/708)
* | | | 35a2ee91 Remove unused data and add fixes (#5154)
* | | | ec903c03 server : add self-extend support (#5104)
* | | | a1d6df12 Add OpenCL add kernel (#5151)
* | | | bbe7c56c cmake : pass CPU architecture flags to nvcc (#5146)
* | | | 62fead3e cuda : fix tensor size calculation for non-split buffer (#5145)
* | | | 15b4538f ggml-alloc : add 10% margin to the buffer sizes (#5149)
* | | | 7032f4f6 ggml : update softmax n_task calculation (#5126)
* | | | 5f1925a8 scripts : move run-with-preset.py from root to scripts folder
* | | | 3b7c914d tests : gitignore test-c.o
* | | | 48c857aa server : refactored the task processing logic (#5065)
* | | | 413e7b05 ci : add model tests + script wrapper (#4586)
* | | | 6dd3c28c metal : remove unused `n_buffers` and `buffers` (#5129)
* | | | 38b431de gguf : fix "general.alignment" type in gguf_reader.py (#5136)
* | | | aad0b01d readme : update hot topics
* | | | 1182cf4d Another bucket sort (#5109)
* | | | fe54033b readme : add MobileVLM 1.7B/3B to the supported models list (#5107)
* | | | 5eaf9964 llama : dynamic temperature sampling (#4972)
* | | | d292f4f2 examples : make pydantic scripts pass mypy and support py3.8 (#5099)
* | | | 256d1bb0 android : use release cmake build type by default (#5123)
* | | | faa3526a Fix Q3_K_XS for MoE models (#5113)
* | | | ddc5a503 metal : show compile log messages
* | | | cd4fddb2 cuda : fix 2-bit quants on amd hip (#5105)
* | | | c9b316c7 nix-shell: use addToSearchPath
* | | | bf63d695 nix: add cc to devShell LD_LIBRARY_PATH
* | | | 1387ea21 llama : pre-allocate input tensors in a separate buffer (#5100)
* | | | 26d60760 metal : disable support for MUL_MAT F32 x F16
* | | | 44879ee8 Additional KL-divergence statistics (#5081)
* | | | 9ecdd12e CUDA: more info when no device code (#5088)
* | | | 89758723 minor : clean-up some warnings and style (#5094)
* | | | 2bed4aa3 devops : add intel oneapi dockerfile (#5068)
* | | | 125d03a5 llama.vim : added api key support (#5090)
* | | | 011e8ec5 llama : fix not enough space in buffer with Qwen (#5086)
* | | | 6f9939d1 KL-divergence (#5076)
* | | | 780e24a2 ggml : parallelize FP32 conversion when using BLAS (#5045)
* | | | 3ce7e8f8 llava : MobileVLM support (#4954)
* | | | b2d80e10 flake.nix: add a comment about flakes vs nix
* | | | 28603cd2 nix: add a comment on the many nixpkgs-with-cuda instances
* | | | 5e97ec91 nix: add a comment about makeScope
* | | | 72518707 nix: refactor the cleanSource rules
* | | | fe8b3c0d workflows: nix-ci: drop the redundant "paths" filter
* | | | f4dd0592 workflows: nix-build-aarch64: rate limit
* | | | f7276f75 workflows: nix-ci: rebuild on flake.lock updates
* | | | 15bceec2 imatrix : keep intermediate imatrix results (#5077)
* | | | d6bd4d46 llama : support StableLM 2 1.6B (#5052)
* | | | 152d9d05 finetune : print sample-start/include-sample-start (#5072)
* | | | 66d575c4 llama : add Q3_K_XS (#5060)
* | | | 57744932 ci : fix Windows CI by updating Intel SDE version (#5053)
* | | | 3466c6eb llama : add more qwen2 models (#5071)
* | | | 504dc37b Revert LLAMA_NATIVE to OFF in flake.nix (#5066)
* | | | 05490fad add safetensors support to convert-lora-to-ggml.py (#5062)
* | | | 6c5629d4 add `#include <string>` to unicode.h (#5051)
* | | | 7dcbe39d Add ability to evauate multiple choice tasks  (#5047)
* | | | 726c0fa9 Slightly faster imatrix (#5050)
* | | | 942c0107 flake.lock: Update (#5054)
* | | | b43ebde3 convert : partially revert PR #4818 (#5041)
* | | | 97c15498 perplexity : fix MSVC build after #5020 (#5043)
* | | | 6df465a9 llama : run all KQV ops on the CPU with no KV offload (#5049)
* | | | 77bc1bbd cmake : add support for ccache (#5002)
* | | | 48e2b133 Add a dart/flutter binding to README.md (#4882)
| | | | * 2bf91c53 metal : clean up
| | | | * f6416d44 wip : good version 8x32
| | | | * eb12e3c3 wip : disable skip
| | | | * 806382a3 wip : simdify ms, vs
| | | | * f2efa6cd wip : simd
| | | | * 6ccbd177 wip
| | | | | * da23b56f wip : no ic 8 step
| | | | |/  
| | | | * af3eda9c wip
| | | | * 5cbdba69 wip
| | | | * 035c4f01 wip
| | | | * 06c2d0d1 wip
| | |_|/  
| |/| |   
| * | | 17720fad metal : parallel reduce across heads
| * | | 77d08f32 metal : parallelize across KV size
| * | | a4b6341c wip : template for rows per warp
| * | | f31955f5 wip : 4 rows per simd group
| * | | 8cde449b wip : 8 rows per simd group
| * | | b9732580 metal : specialize for head size
| * | | 52ae0857 metal : reduce branches
| * | | 528da751 metal : f16 precision
| * | | 1173f49c metal : initial implementation
| * | | a9681feb ggml : online attention (CPU)
| * | |   c3cdfffa Merge branch 'master' into gg/flash-attn
| |\ \ \  
| |/ / /  
|/| | /   
| | |/    
| |/|     
* | | cca894f1 cuda : fix compile error in jetson platform (#4975)
| * | fa7ebcca ggml : fix GQA support in ggml_flash_attn_ext
| * | a1c004ef ggml : add ggml_flash_attn_ext API
| | | * 32a392fe try a differerent fix
| | | * e15c6163 perplexity : fix MSVC build after #5020
| |_|/  
|/| |   
* | | 381ee195 finetune : fix ggml_allocr lifetimes (tmp workaround) (#5033)
| | | * 4a3bc152 py : linting with mypy and isort
| | | * ffdd051a convert : update GGML script to use VocabFactory
| | | * cb4605fe convert : partially revert PR #4818
| |_|/  
|/| |   
* | | a5cacb22 imatrix : add README.md
* | | 9b75cb2b llama : support upcoming Qwen2 (#5037)
* | | de9a147d py : fix flake8 lint
* | | 7051aacf winogrande: evaluate log-probs in parallel (#5036)
* | | 2b3b999c llama : add CodeShell support (#5016)
* | | 993fba81 perplexity: avoid unnecessary alloocations and logit copies (#5035)
* | | 8b20858e perplexity : faster Winogrande via batching (#5024)
* | | 57e2a7a5 llama : fix falcon arch for tied output embeddings (#4978)
* | | 9b6ea426 cmake : add ggml public headers (#5011)
* | | 821f0a27 server : defer tasks when "slot unavailable" (#5018)
* | | 96d7f56d llama : fix mlock with no-mmap with Metal (#5025)
* | | 2d5419d0 imatrix : fix assert for src0 non-cont check
* | | d391ae9b perplexity : fix winogrande N tasks option
* | | e9240cdf scripts : add get-winogrande.sh
* | | b4675773 convert.py : fix llama/llama2 conversion due to vocab_size=-1 (#5019)
* | | 3e945cc1 HellaSwag: speed up by parallelizing log-prob evaluation (#5020)
|/ /  
| | * 14532151 kompute : fix ggml_add kernel
| | * 610394ff fix supported ops for kompute backend
| | * 7addf2b8 never try to evaluate an empty command buffer
| | * 16bc3c3b sync op_rope_f16 with recent op_rope_f32 changes
| | * 0f1a958a actually fix this assertion
| | * a97935e0 clean up old backend code
| | * 696faa86 kompute : fix rope_f32 and scale ops (#5008)
| | * 02b9bafe kompute : ignore exceptions in ggml_vk_available_devices (#12)
| | * de9b0bbb add sanity check and fix kompute teardown order
| | * 50579f27 attempt to get test-backend-ops working
| | * 8a99f698 fix assertion failure
| | * d5670d6e kompute : initial attempt at ggml-backend v2 support
| | * 1eb8804c PR #4766
| | *   3773e1af Merge branch 'master' of https://github.com/ggerganov/llama.cpp into ceb/nomic-vulkan
| | |\  
| | * \   ae6d6824 Merge commit 'd232aca5a73b290e218a2e48b91023d5e994203f' into ceb/nomic-vulkan
| | |\ \  
| | * | | 904c563d sync xxd commands with GPT4All llama.cpp.cmake
| | * | |   3959283e Merge commit '31f27758faf4a4bd08101a57c7ec3a473f771f86' into ceb/nomic-vulkan
| | |\ \ \  
| | * \ \ \   8b65f4c5 Merge commit 'bcc0eb4591bec5ec02fad3f2bdcb1b265052ea56' into ceb/nomic-vulkan
| | |\ \ \ \  
| | * | | | | 44b1a97a kompute : fix -Wunused-private-field warnings from clang
| | * | | | | 80727062 kompute : always destroy Manager via the destructor
| | * | | | | 2d2c76ac vulkan : fix free of stack addr in llama_buffer
| | * | | | | f58f581c refactor llama.cpp modifications
| | * | | | | c8fd4ba8 ggml : restore 'static' specifiers
| | * | | | | f7cb0a65 remove script with unclear purpose
| | * | | | | 9af7f58b move kompute to a submodule
| | * | | | | b906e126 kompute : fix compile warnings
| | * | | | |   747e1eaf Merge commit '81bc9214a389362010f7a57f4cbc30e5f83a2d28' into nomic-vulkan
| | |\ \ \ \ \  
| | * | | | | | 27631dbb separate shaders from kompute itself
| | * | | | | | 3e09e127 rename ggml-vulkan -> ggml-kompute
| | * | | | | | 56430c32 relicense Vulkan backend as MIT
| | * | | | | |   9ae88baf Merge remote-tracking branch 'upstream/master' into nomic-vulkan-redo
| | |\ \ \ \ \ \  
| | * | | | | | | a4bb9c5c vulkan : sync with "migrate to dynamic graphs"
| | * | | | | | |   23f6d51f Merge commit '4760e7cc0b68570d58f55e8dda469805d1759d0d' into nomic-vulkan
| | |\ \ \ \ \ \ \  
| | * | | | | | | | 208cd52f vulkan : implement YaRN RoPE scaling (#2268)
| | * | | | | | | |   1829f1d7 Merge commit '4760e7cc0b68570d58f55e8dda469805d1759d0d~' into nomic-vulkan
| | |\ \ \ \ \ \ \ \  
| | * | | | | | | | | 02c3309f merge fixup (e16b9fa4baa8a09c6619b116159830e898050942)
| | * | | | | | | | | 9c4dfd06 mention skipped change
| | * | | | | | | | |   fe26e6ad Merge commit 'e16b9fa4baa8a09c6619b116159830e898050942' into nomic-vulkan
| | |\ \ \ \ \ \ \ \ \  
| | * | | | | | | | | | 6474fc87 vulkan : handle ggml_scale for n%8 != 0
| | * | | | | | | | | |   2a41ba72 Merge commit '469c9addef75893e6be12edda852d12e840bf064' into nomic-vulkan
| | |\ \ \ \ \ \ \ \ \ \  
| | * | | | | | | | | | | a934b2cb vulkan : assert various kernel requirements
| | * | | | | | | | | | |   f194e1b6 Merge commit 'fcca0a700487999d52a525c96d6661e9f6a8703a' into nomic-vulkan
| | |\ \ \ \ \ \ \ \ \ \ \  
| | * | | | | | | | | | | | 39abedd1 vulkan : optimize workgroup sizes
| | * | | | | | | | | | | | 84f7fc45 vulkan : rope n_past is now KQ_pos, f16 rope kernel
| | * | | | | | | | | | | | 71565eb0 vulkan : replace ggml_diag_mask_inf with ggml_add (custom -inf mask)
| | * | | | | | | | | | | |   af00cca0 Merge commit 'ec893798b7a2a803466cc8f063051499ec3d96f7' into HEAD
| | |\ \ \ \ \ \ \ \ \ \ \ \  
| | * | | | | | | | | | | | | c438c168 fix build with external fmtlib (v10)
| | * | | | | | | | | | | | | a8cac532 kompute : fix issues with debug layers
| | * | | | | | | | | | | | | f88b1988 llama : fix Vulkan whitelist (#11)
| | * | | | | | | | | | | | | ffd0624b Remove this debug code.
| | * | | | | | | | | | | | | a5eb001e Revert the prompt processing on gpu for now.
| | * | | | | | | | | | | | | e006d377 Scale the workgroup count down to allow correct generation for falcon with AMD radeon cards with lower workgroup count limit
| | * | | | | | | | | | | | | 89b71278 llama : decide to disable Vulkan before loading tensors (#7)
| | * | | | | | | | | | | | | 1c170101 vulkan : fix missing break in matmul selection (#9)
| | * | | | | | | | | | | | | 74ddf0f1 Fix synchronization problem for AMD Radeon with amdvlk driver or windows drivers. Does not have any performance or fidelity effect on other gpu/driver combos I've tested.
| | * | | | | | | | | | | | | 8d9efbf9 Lower the workgroup count for some shaders by providing a loop that processes four floats at a time.
| | * | | | | | | | | | | | | 752f7ebd Remove unused push constant that was giving validation errors.
| | * | | | | | | | | | | | | 84000153 Don't try an allocation on a heap that is smaller than the size we require.
| | * | | | | | | | | | | | | cbc0d1af kompute : make scripts executable
| | * | | | | | | | | | | | | 21841d31 kompute : enable kp_logger and make it static (#8)
| | * | | | | | | | | | | | | cc05a602 use mat*vec shaders for mat*mat
| | * | | | | | | | | | | | | c1fd6454 attempted speedups 2
| | * | | | | | | | | | | | | 9bc52eba attempted speedups
| | * | | | | | | | | | | | | 8dc79ac3 clean up vulkan/cpu switch
| | * | | | | | | | | | | | | cd0257ed q4_1 mat*mat
| | * | | | | | | | | | | | | 4809890d rm commented dbg print
| | * | | | | | | | | | | | | b78a94bc q6k mm works
| | * | | | | | | | | | | | | d5741c07 use op param epsilon for norms
| | * | | | | | | | | | | | | 3327d84a perf: use bigger threadgroups in mm
| | * | | | | | | | | | | | | 46385ee0 misc vulkan cleanup
| | * | | | | | | | | | | | | f0cd38b9 add mat*mat ops
| | * | | | | | | | | | | | | 09d83f04 Delete TODO now that we have q8_0.
| | * | | | | | | | | | | | | 8564f790 falcon h2d + reenable vulkan
| | * | | | | | | | | | | | | 020b1745 vulkan: implement neox mode for rope
| | * | | | | | | | | | | | | ff4212d2 q8 mat*vec
| | * | | | | | | | | | | | | 9db90cbe f16 mv broadcasting fix (gqa fix)
| | * | | | | | | | | | | | | 3d850db7 kompute : remove Q6_K from list of supported quant types
| | * | | | | | | | | | | | | 24a4a595 kompute : only try to use Vulkan for LLaMA itself
| | * | | | | | | | | | | | | bc4b5ed1 Fixes for subgroup size to bring AMD and NVIDIA inline with eachother for all kernels.
| | * | | | | | | | | | | | | de589ced Change this back to be in agreement with metal and our previous softmax kernel.
| | * | | | | | | | | | | | | 6ac39752 Fixup the upstream CMakelists.txt so we can build just llama.cpp with our branch.
| | * | | | | | | | | | | | | 32289aa4 Fixes for norm.
| | * | | | | | | | | | | | | 06d4b215 Fix offset into the qh and now we have working vulkan accelerated for gguff'd llama.
| | * | | | | | | | | | | | | f1c9bc18 Add q6_k getrows and mul*vec kernel.
| | * | | | | | | | | | | | | 4b223ec4 Refactor getrows to use common code and get ready for q6_k.
| | * | | | | | | | | | | | | 5509f743 Minor cleanup.
| | * | | | | | | | | | | | | 601905e7 Move the subgroups and printf into common.
| | * | | | | | | | | | | | | 93306f16 Consolidate code for mat x vec kernels and use subgroups more extensively.
| | * | | | | | | | | | | | | 77135a3b Add a common boilerplate code via include and elim copy pasta
| | * | | | | | | | | | | | | 9e4f8b4a Upload immediately to device.
| | * | | | | | | | | | | | | 6b6c73a9 kompute : don't fail build because of -Warray-bounds
| | * | | | | | | | | | | | | 1b1416d7 Support for gguf.
| | * | | | | | | | | | | | | 2c24d67e Don't crash on available devices if we can't even create an instance.
| | * | | | | | | | | | | | | addac252 Set the singleton to nullptr here.
| | * | | | | | | | | | | | | 68aca6be Only use vulkan with known quant that work.
| | * | | | | | | | | | | | | 4ed25b2f Sync from device back to host at begin of new prompt.
| | * | | | | | | | | | | | | bd5f6399 Don't try and install kompute artifacts.
| | * | | | | | | | | | | | | 8bea7198 vulkan: disambiguate gpus with the same name
| | * | | | | | | | | | | | | 68cf1df6 Throw an exception when allocation fails for vulkan.
| | * | | | | | | | | | | | | beee5726 Make kompute actually include external SDK headers when requested
| | * | | | | | | | | | | | | b7e2e691 Completely revamp how we do object management with the vulkan backend and stop using so many static objects so we can tear down and bring up vulkan on new devices in the same runtime.
| | * | | | | | | | | | | | | 45c8778b Switch to a dynamic dispatch table instead of linking hard against libvulkan.
| | * | | | | | | | | | | | | 8563fa00 remove dynamic deps from kompute build
| | * | | | | | | | | | | | | 48a45ea4 Remove warning which fails on windows.
| | * | | | | | | | | | | | | ba15dfd0 Nomic vulkan backend licensed under the Software for Open Models License (SOM), version 1.0.
| | | | | | | | | | | | | | | * ccc78a20 hellaswag: speed up even more by parallelizing log-prob evaluation
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | ad19812c perplexity : faster HellaSwag via batching (#5017)
* | | | | | | | | | | | | | | 682986a0 Add Winogrande evaluation (#5015)
* | | | | | | | | | | | | | | dcad445d scritps : add helper script to get hellaswag data in txt format
* | | | | | | | | | | | | | | 1e605f41 metal : fix memory leak, dangling pointer and unused autorel (#5007)
* | | | | | | | | | | | | | | 6b6916b2 sync : ggml
* | | | | | | | | | | | | | | 38566680 ggml : add IQ2 to test-backend-ops + refactoring (#4990)
* | | | | | | | | | | | | | | ba69bbc8 imatrix : offload to GPU support (#4957)
| | | | | | | | | | | | | | | *   2917e6b5 Merge branch 'master' into gg/imatrix-gpu-4931
| | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 44a1a4a4 backend : add eval callback (#4935)
* | | | | | | | | | | | | | | | c918fe8d metal : create autorelease pool during library build (#4970)
* | | | | | | | | | | | | | | | 0f83e727 py : fix whitespace
|/ / / / / / / / / / / / / / /  
* | | | | | | | | | | | | | | 4f4bf35f py : fix missing added_tokens_dict for SPM and BPE vocabs (#4971)
* | | | | | | | | | | | | | | 2b3a665d llama : use Q4_K for attn_v for Q2_K_S when n_gqa >= 4 (#4996)
| | | | | | | | | | | | | | * 4fb52843 ci : rearrange output
| | | | | | | | | | | | | | * 10b25e03 ci : add imatrix test
| | | | | | | | | | | | | | * a722d05a imatrix : fix ggml_mul_mat_id hanlding
| | | | | | | | | | | | | | * 0b2fca9a imatrix : offload to GPU support
| | | | | | | | | | | | | | * e0493800 simple : fix
| | | | | | | | | | | | | | * e1b1db9f simple : do not perform tensor data copy if not needed
| | | | | | | | | | | | | | * 83f3d7a8 backend : clean-up the implementation
| | | | | | | | | | | | | | * 01b6f68a backend : group nodes in a single compute when user don't need them
| | | | | | | | | | | | | | * 65648b34 backend : add eval callback
| | | | | | | | | | | | | | | * 23742deb py : fix padded dummy tokens (I hope)
| | | | | | | | | | | | | | | * d92351e2 py : fix BPE vocab conversion
| | | | | | | | | | | | | | | * a1372737 py : pad with unknown tokens when data is missing
| | | | | | | | | | | | | | | * 9b464b4e py : fix missing added_tokens_dict for SPM vocab
| | | | | | | | | | | | | | | | * 9fd1e83f Use Q4_K for attn_v for Q2_K_S when n_gqa >= 4
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 75632936 metal : remove unnecessary nil check (#4986)
* | | | | | | | | | | | | | | | f46c0c1b llama : fix copy/paste error in llama_sampling_params comment (#4994)
* | | | | | | | | | | | | | | | 5c999609 py : remove unnecessary hasattr (#4903)
| | | | | | | | | | | | | | | | * 49bafe09 tests : avoid creating RNGs for each tensor
| | | | | | | | | | | | | | | | * 8eb8fd94 tests : avoid creating RNGs for each Q tensor
| | | | | | | | | | | | | | | | * b7ddc8bf cuda : fix out-of-bounds-access in `mul_mat_vec_q`
| | | | | | | | | | | | | | | | * 36feaeb4 ci : enable LLAMA_CUBLAS=1 for CUDA nodes
| | | | | | | | | | | | | | | | * e9a5d54b cuda : update supports_op for IQ2
| | | | | | | | | | | | | | | | * bc0bb300 ggml : add IQ2 to test-backend-ops + refactoring
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | bee938da nix: remove nixConfig from flake.nix (#4984)
* | | | | | | | | | | | | | | | cec8a484 finetune : add training data file to log message (#4979)
* | | | | | | | | | | | | | | | 334a835a ggml : importance matrix support for legacy quants (#4969)
* | | | | | | | | | | | | | | | 4feb4b33 examples : add complete parallel function calling example (#4974)
* | | | | | | | | | | | | | | | 959ef0c0 perplexity : fix kv cache handling for hellaswag (#4981)
* | | | | | | | | | | | | | | | c37b3474 flake.lock: update flake-parts, flake-parts/nixpkgs-lib, and nixpkgs (#4920)
* | | | | | | | | | | | | | | | 158f8c9e metal : localized logic in `ggml_metal_graph_compute` (#4924)
* | | | | | | | | | | | | | | | 862f5e41 android : introduce starter project example (#4926)
* | | | | | | | | | | | | | | | 3a48d558 metal : replace loop of dispatch_async with dispatch_apply (#4934)
* | | | | | | | | | | | | | | | 7c8d3abd metal : log `recommendedMaxWorkingSetSize` on iOS 16+ (#4936)
* | | | | | | | | | | | | | | | 122ed484 examples : fix and improv docs for the grammar generator (#4909)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | a0b3ac8c ggml : introduce GGML_CALL function annotation (#4850)
* | | | | | | | | | | | | | | d75c232e finetune : use LLAMA_FILE_MAGIC_GGLA (#4961)
* | | | | | | | | | | | | | | e0324285 speculative : threading options (#4959)
| | | | | | | | | | | | | | | * bb9abb5c imatrix: guard Q4_0/Q5_0 against ffn_down craziness
| | | | | | | | | | | | | | | * 6f9ec42a imatrix: adding support for legacy quants
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 3e5ca793 pass cpu-architecture arguments only to host code (C;C++) (#4943)
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 44833967 llama : apply classifier-free guidance to logits directly (#4951)
* | | | | | | | | | | | | | d9aa4ffa awq-py : fix typo in awq-py/README.md (#4947)
* | | | | | | | | | | | | | ddb008d8 cuda : fix dequantize kernel names (#4938)
* | | | | | | | | | | | | | 2faaef39 llama : check for 256 divisibility for IQ2_XS, IQ2_XXS (#4950)
* | | | | | | | | | | | | | 4a3156de CUDA: faster dequantize kernels for Q4_0 and Q4_1 (#4938)
* | | | | | | | | | | | | | a836c8f5 llama : fix missing quotes (#4937)
* | | | | | | | | | | | | | 467a882f Add ability to use importance matrix for all k-quants (#4930)
* | | | | | | | | | | | | | bb0c1392 llama : check LLAMA_TRACE env for extra logging (#4929)
* | | | | | | | | | | | | | 9408cfda scripts : sync-ggml-am.sh option to skip commits
* | | | | | | | | | | | | | 03c52674 llama : use LLAMA_LOG_ macros for logging
* | | | | | | | | | | | | | a128c38d Fix ffn_down quantization mix for MoE models (#4927)
* | | | | | | | | | | | | | 5f5fe1bd metal : correctly set SIMD support flags on iOS (#4923)
* | | | | | | | | | | | | | ac32902a llama : support WinXP build with MinGW 8.1.0 (#3419)
* | | | | | | | | | | | | | 147b17ac 2-bit quantizations (#4897)
* | | | | | | | | | | | | | 807179ec Make Q3_K_S be the same as olf Q3_K_L for Mixtral-8x7B (#4906)
* | | | | | | | | | | | | | 76484fbf sync : ggml
* | | | | | | | | | | | | | c71d608c ggml: cache sin/cos for RoPE (#4908)
* | | | | | | | | | | | | | 4be5ef55 metal : remove old API (#4919)
* | | | | | | | | | | | | | 0ea069b8 server : fix prompt caching with system prompt (#4914)
* | | | | | | | | | | | | | f172de03 llama : fix detokenization of non-special added-tokens (#4916)
* | | | | | | | | | | | | | 2d57de52 metal : disable log for loaded kernels (#4794)
* | | | | | | | | | | | | | df845cc9 llama : minimize size used for state save/load (#4820)
* | | | | | | | | | | | | | 6b48ed08 workflows: unbreak nix-build-aarch64, and split it out (#4915)
* | | | | | | | | | | | | | 722d33f3 main : add parameter --no-display-prompt (#4541)
* | | | | | | | | | | | | | c30b1ef3 gguf : fix potential infinite for-loop (#4600)
* | | | | | | | | | | | | | b38b5e93 metal : refactor kernel loading code (#4794)
* | | | | | | | | | | | | | 7dc78764 compare-llama-bench: tweak output format (#4910)
* | | | | | | | | | | | | | 356327fe server : fix deadlock that occurs in multi-prompt scenarios (#4905)
* | | | | | | | | | | | | | ee8243ad server : fix crash with multimodal models without BOS token (#4904)
| | | | | | | | | | | | | | * 9998ecd1 llama : add phixtral support (wip)
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 15ebe592 convert : update phi-2 to latest HF repo (#4903)
| | | | | | | | | | | | | | * 1fb563eb py : try to fix flake stuff
| | | | | | | | | | | | | | * fe252237 convert : update phi-2 to latest HF repo
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | de473f5f sync : ggml
* | | | | | | | | | | | | | f2384612 ggml : fix 32-bit ARM compat for IQ2_XS (whisper/1758)
* | | | | | | | | | | | | | fa5c1fb4 backend_sched : fix assignments
* | | | | | | | | | | | | | 52ee4540 examples : add pydantic models to GBNF grammar generator (#4883)
* | | | | | | | | | | | | | 3fe81781 CUDA: faster q8_0 -> f16 dequantization (#4895)
* | | | | | | | | | | | | | e7e4df03 llama : ggml-backend integration (#4766)
* | | | | | | | | | | | | | 584d674b llama : remove redundant assert for StableLM (#4901)
* | | | | | | | | | | | | | 930f907d export-lora : use LLAMA_FILE_MAGIC_GGLA (#4894)
* | | | | | | | | | | | | | e790eef2 llama.swiftui : update models layout (#4826)
* | | | | | | | | | | | | | 5537d9d3 gitignore : imatrix
* | | | | | | | | | | | | | 1b280c9f CUDA: fix softmax compile for old CUDA versions (#4862)
* | | | | | | | | | | | | | 3cabe806 llama : fix typo "imp_embd" -> "inp_embd"
* | | | | | | | | | | | | | 4315a943 common : streamline the formatting of help (#4890)
* | | | | | | | | | | | | | 2d00741e py : fix lint (#4889)
* | | | | | | | | | | | | | f445c0e6 llama : fix llm_build_k_shift to use correct n_rot (#4889)
* | | | | | | | | | | | | | 326b418b Importance Matrix calculation (#4861)
* | | | | | | | | | | | | | 1d118386 server : fix infill when prompt is empty (#4833)
* | | | | | | | | | | | | | 7edefbd7 main : better name for variable n_print (#4874)
* | | | | | | | | | | | | | 3ca63b45 main : disable token count by default (#4874)
* | | | | | | | | | | | | | b0377875 swift : track ggml release branch (#4867)
* | | | | | | | | | | | | | 469e75d0 llama : restore intended k-quants mixes for MoE models (#4872)
* | | | | | | | | | | | | | 49662cbe ggml : SOTA 2-bit quants (add IQ2_XS) (#4856)
* | | | | | | | | | | | | | 3ba5b8ca swift : pin ggml commit + remove ggml.h from spm-headers (#4878)
* | | | | | | | | | | | | | 4330bd83 server : implement credentialed CORS (#4514)
* | | | | | | | | | | | | | 27379455 server : support for multiple api keys (#4864)
* | | | | | | | | | | | | | eab67950 server : add `LOG_INFO` when model is successfully loaded (#4881)
* | | | | | | | | | | | | | d8d90aa3 ci: nix-flake-update: new token with pr permissions (#4879)
* | | | | | | | | | | | | | 43f76bf1 main : print total token count and tokens consumed so far (#4874)
* | | | | | | | | | | | | | 2f043328 server : fix typo in model name (#4876)
* | | | | | | | | | | | | | 2a7c94db metal : put encoder debug group behind a define (#4873)
* | | | | | | | | | | | | | 64802ec0 sync : ggml
* | | | | | | | | | | | | | 3267c2ab metal : fix deprecation warning (ggml/690)
* | | | | | | | | | | | | | f85a973a ggml : remove ggml_cpy_inplace and ggml_cont_inplace (ggml/693)
* | | | | | | | | | | | | | 5362e439 metal : wrap each operation in debug group (ggml/690)
* | | | | | | | | | | | | | e739de79 ggml : change GGML_MAX_NAME at compile time (ggml/682)
* | | | | | | | | | | | | | c910e3c2 Fix execlp call (ggml/689)
* | | | | | | | | | | | | | f34432ca fix : cuda order of synchronization when setting a buffer (ggml/679)
* | | | | | | | | | | | | | 7a9f75c3 server : update readme to document the new `/health` endpoint (#4866)
* | | | | | | | | | | | | | 5c1980d8 server : fix build + rename enums (#4870)
* | | | | | | | | | | | | | cd108e64 server : add a `/health` endpoint (#4860)
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 57d016ba llama : add additional suffixes for model params (#4834)
* | | | | | | | | | | | | 329ff615 llama : recognize 1B phi models (#4847)
* | | | | | | | | | | | | d34633d8 clip : support more quantization types (#4846)
* | | | | | | | | | | | | 4f56458d Python script to compare commits with llama-bench (#4844)
* | | | | | | | | | | | | 6efb8eb3 convert.py : fix vanilla LLaMA model conversion (#4818)
* | | | | | | | | | | | | 36e5a08b llava-cli : don't crash if --image flag is invalid (#4835)
* | | | | | | | | | | | | 4dccb38d metal : improve dequantize precision to match CPU (#4836)
* | | | | | | | | | | | | 9a818f7c scripts : improve get-pg.sh (#4838)
* | | | | | | | | | | | | 18adb4e9 readme : add 3rd party collama reference to UI list (#4840)
| | | | | | | | | | | | | * 9bfcb16f Add llama enum for IQ2_XS
| | | | | | | | | | | | | * a1610b05 iq2_xs: had forgotten to delete iq2-data.h
| | | | | | | | | | | | | * 8299b03a iq2_xs: faster AVX2 dit product
| | | | | | | | | | | | | * 3198e94f iq2_xs: AVX2 dot product - 19.5 t/s
| | | | | | | | | | | | | * 52ea3f79 iq2_xs: better ARM_NEON dot product
| | | | | | | | | | | | | * ff49d876 iq2_xs: working, but dog slow, ARM_NEON dot product
| | | | | | | | | | | | | * 55e2cae8 iq2_xs: Metal now works
| | | | | | | | | | | | | * 0aacd551 iq2_xs: WIP Metal
| | | | | | | | | | | | | * 9b6e38d8 iq2_xs: CUDA and scalar CPU works
| | | | | | | | | | | | | * 9f21b82e iq2_xs: this should have been in the basics
| | | | | | | | | | | | | * 3569fa3f iq2_xs: basics
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | d9653894 scripts : script to get Paul Graham essays in txt format (#4838)
* | | | | | | | | | | | | 128de358 server : update readme about token probs (#4777)
| | | | | | | | | | | | | * 24096933 server : try to fix infill when prompt is empty
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 8c583303 server : add api-key flag to documentation (#4832)
* | | | | | | | | | | | | 18c2e175 ggml : fix vld1q_s8_x4 32-bit compat (#4828)
* | | | | | | | | | | | | 8f900abf CUDA: faster softmax via shared memory + fp16 math (#4742)
| | | | | | | | | | | | | * 7216af5c ggml : fix 32-bit ARM compat (cont)
| | | | | | | | | | | | | * 27afe299 ggml : fix vld1q_s8_x4 32-bit compat
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 1fc2f265 common : fix the short form of `--grp-attn-w`, not `-gat` (#4825)
* | | | | | | | | | | | | a9a8c5de readme : add link to SOTA models
* | | | | | | | | | | | | dd5ae064 SOTA 2-bit quants (#4773)
* | | | | | | | | | | | | 668b31fc swift : exclude ggml-metal.metal from the package (#4822)
* | | | | | | | | | | | | 42ea63c5 llama.swiftui : update readme
* | | | | | | | | | | | | 52531fdf main : add self-extend support (#4815)
* | | | | | | | | | | | | b0034d93 examples : add passkey test (#3856)
* | | | | | | | | | | | | b7e79829 readme : add lgrammel/modelfusion JS/TS client for llama.cpp (#4814)
* | | | | | | | | | | | | 226460cc llama-bench : add no-kv-offload parameter (#4812)
* | | | | | | | | | | | | d5a410e8 CUDA: fixed redundant value dequantization (#4809)
| | | | | | | | | | | | | * d57cb9c2 passkey : add readme
| | | | | | | | | | | | | * 164d7a05 passkey : add "self-extend"-like context extension (#4810)
| | | | | | | | | | | | | * a42feb18 make : add passkey target
| | | | | | | | | | | | | * f2c9800d passkey : simplify n_past logic
| | | | | | | | | | | | | * bda3f2c8 passkey : select pass key pos from CLI
| | | | | | | | | | | | | * fbb999f5 passkey : better prints
| | | | | | | | | | | | | * 21196da1 examples : add passkey test
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 9dede37d llama : remove unused vars (#4796)
* | | | | | | | | | | | | 3c36213d llama : remove redundant GQA check (#4796)
* | | | | | | | | | | | | 72d8407b llama.swiftui : use llama.cpp as SPM package (#4804)
* | | | | | | | | | | | | d117d4dc llama : print tensor meta for debugging
* | | | | | | | | | | | | 3418c03e llama.swiftui : add visionOS target (#4805)
* | | | | | | | | | | | | 63ee677e ggml : use __builtin_amdgcn_sudot4 in __dp4a for gfx11 (#4787)
* | | | | | | | | | | | | 67984921 server : fix n_predict check (#4798)
* | | | | | | | | | | | | c75ca5d9 llama.swiftui : use correct pointer for llama_token_eos (#4797)
| | | | | | | | | | | | | * 7cfde781 llama : remove redundant GQA check
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 96e80dab examples : improve base-translate.sh script (#4783)
* | | | | | | | | | | | | eec22a1c cmake : check for openblas64 (#4134)
* | | | | | | | | | | | | be36bb94 flake.nix : fix typo (#4700)
* | | | | | | | | | | | | 91d38876 metal : switch back to default.metallib (ggml/681)
* | | | | | | | | | | | | d061bf94 ggml : fix q2_k bpw in comments (ggml/680)
* | | | | | | | | | | | | 1bf681f9 ggml : add error handling to graph_compute (whisper/1714)
* | | | | | | | | | | | | c1d7cb28 ggml : do not sched_yield when calling BLAS (#4761)
* | | | | | | | | | | | | 3681f224 examples : add few-shot translation example (#4783)
* | | | | | | | | | | | | b3a7c20b finetune : remove unused includes (#4756)
* | | | | | | | | | | | | 012cf349 server : send token probs for "stream == false" (#4714)
* | | | | | | | | | | | | a9192801 Print backend name on test-backend-ops failure (#4751)
* | | | | | | | | | | | | 3c0b5855 llama.swiftui : support loading custom model from file picker (#4767)
* | | | | | | | | | | | | e5804313 server : fix options in README.md (#4765)
* | | | | | | | | | | | | dc891b7f ggml : include stdlib.h before intrin.h (#4736)
* | | | | | | | | | | | | 46cea79e llama.swiftui : fix build of ggml.metallib (#4754)
* | | | | | | | | | | | | cb1e2818 train : fix typo in overlapping-samples help msg (#4758)
* | | | | | | | | | | | | ece9a45e swift : update Package.swift to use ggml as dependency (#4691)
* | | | | | | | | | | | | 7bed7eba cuda : simplify expression
* | | | | | | | | | | | | d55356d3 cuda : mark I16 and I32 ops as unsupported
* | | | | | | | | | | | | 75e3fd85 sync : ggml
* | | | | | | | | | | | | 28931371 metal : add kernel_get_rows_i32
* | | | | | | | | | | | | ab62fc3e scripts : fix sync order + metal sed
* | | | | | | | | | | | | 5f66ebca ggml : extend ggml_get_rows, ggml_repeat, ggml_concat (ggml/639)
* | | | | | | | | | | | | f2eb19bd server : throw an error when `slot unavailable` (#4741)
* | | | | | | | | | | | | f3f62f0d metal : optimize ggml_mul_mat_id (faster Mixtral PP) (#4725)
| | | | | | | | | | | | | * 9f51f3e6 metal : opt mul_mm_id
| | | | | | | | | | | | | *   21e100d6 Merge branch 'master' into gg/metal-opt-mul-mat-id
| | | | | | | | | | | | | |\  
| | | | | | | | | | | | | * | daf9b124 metal : minor fix
| | | | | | | | | | | | | * |   74460d00 Merge branch 'master' into gg/metal-opt-mul-mat-id
| | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | * \ \   c73e598d Merge branch 'master' into gg/metal-opt-mul-mat-id
| | | | | | | | | | | | | |\ \ \  
| | | | | | | | | | | | | * | | | 76f9d41d metal : optimizing ggml_mul_mat_id (wip)
| | | | | | | | | | | | | * | | | 5865b18e metal : fix mat-vec Q4_K kernel for QK_K == 64
| | | | | | | | | | | | | * | | | a8b9bb45 cmake : respect LLAMA_QKK_64 option
| | | | | | | | | | | | | * | | | 049a32ff metal : normalize mat-vec kernel signatures
| | | | | | | | | | | | | * | | | ad7cf37f metal : fix mat-vec Q8_0 kernel for BS > 1
| | | | | | | | | | | | | * | | | 6435a3de cmake : rename option to LLAMA_METAL_SHADER_DEBUG
| | | | | | | | | | | | | * | | | 4c054d98 metal : use uint64_t for strides
| | | | | | | | | | | | | * | | | b14b5a9e metal : fix compile warnings
| | | | | | | | | | | | | * | | | 1580805f metal : fix API debug warnings
| | | | | | | | | | | | | * | | | a184e105 cmake : add -fno-inline for Metal build (#4545)
| | | | | | | | | | | | | * | | | 515cfec4 metal : fix Metal API debug warnings
| | | | | | | | | | | | | * | | | 75c14f26 ggml : disable fast-math for Metal (cmake build only)
| | | | | | | | | | | | | | | | | * 4cc78d38 ggml : force F32 precision for ggml_mul_mat
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | 0ef3ca2a server : add token counts to html footer (#4738)
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 540938f8 llama : llama_model_desc print number of experts
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 0040d42e llama : replace all API facing `int`'s with `int32_t` (#4577)
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
| | | | | | | | | | | | | | * b5af7ad8 llama : refactor quantization to avoid <mutex> header
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 83e633c2 llama : differentiate the KV dims in the attention (#4657)
| | | | | | | | | | | | | | * 120a1a55 llama : auto download HF models if URL provided
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 32866c5e editorconfig : fix whitespace and indentation #4710
* | | | | | | | | | | | | | 5d7002d4 server : add --override-kv parameter (#4710)
* | | | | | | | | | | | | | 26f3071d py : re-enable mmap in convert hf (#4732)
* | | | | | | | | | | | | | 775ac871 finetune: fix typo in README.md (#4733)
* | | | | | | | | | | | | | 58ba655a metal : enable shader debugging (cmake option) (#4705)
* | | | | | | | | | | | | | edd1ab7b flake.lock: update
* | | | | | | | | | | | | | 198ed7eb flake.nix: suggest the binary caches
* | | | | | | | | | | | | | d8361747 workflows: nix-ci: add a qemu job for jetsons
* | | | | | | | | | | | | | 06f2a5d1 workflows: nix-flakestry: drop tag filters
* | | | | | | | | | | | | | c5239944 workflows: weekly `nix flake update`
* | | | | | | | | | | | | | 1e9ae54c workflows: nix-ci: add a job for eval
* | | | | | | | | | | | | | 7adedecb workflows: nix-ci: init; build flake outputs
* | | | | | | | | | | | | | 356ea17e flake.nix: expose checks
* | | | | | | | | | | | | | a5c088d8 flake.nix: rocm not yet supported on aarch64, so hide the output
* | | | | | | | | | | | | | 1e3900eb flake.nix: expose full scope in legacyPackages
* | | | | | | | | | | | | | e39106c0 ggml : add ggml_vdotq_s32 alias (#4715)
* | | | | | | | | | | | | | 9fbda719 clip : refactor + bug fixes (#4696)
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 39d8bc71 CUDA: fixed tensor cores not being used on RDNA3 (#4697)
| | | | | | | | | | | | | * f64e4f04 ggml : testing GPU FP precision via quantized CPY
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 24a447e2 ggml : add ggml_cpu_has_avx_vnni() (#4589)
* | | | | | | | | | | | | a20f3c74 CUDA: fix tensor core logic for Pascal and HIP (#4682)
* | | | | | | | | | | | | 0235b9b5 clip : use ggml_backend_buffer_is_host (#4205)
* | | | | | | | | | | | | ce18d727 clip : enable gpu backend (#4205)
* | | | | | | | | | | | | 91bb39ce cuda: fix vmm oom issue on NVIDIA AGX Orin (#4687)
* | | | | | | | | | | | | 04ac0607 python : add check-requirements.sh and GitHub workflow (#4585)
* | | | | | | | | | | | | 68eccbdc flake.nix : rewrite (#4605)
* | | | | | | | | | | | | 97bbca6e cmake : fix ld warning duplicate libraries libllama.a (#4671)
* | | | | | | | | | | | | 4af48015 llava-cli : refactor to use sampling library (#4669)
* | | | | | | | | | | | | db49ff8e server : replace sleep with condition variables (#4673)
* | | | | | | | | | | | | 60f55e88 server : fix OpenAI server sampling w.r.t. penalty. (#4675)
* | | | | | | | | | | | | b93edd22 server : allow to generate multimodal embeddings (#4681)
* | | | | | | | | | | | | 82d6eab2 main-cmake-pkg : fix build issue (#4665)
* | | | | | | | | | | | | afd997ab llama.swiftui : fix infinite loop, ouput timings, buff UI (#4674)
* | | | | | | | | | | | | c8255f8a scripts : print list of sync commits
* | | | | | | | | | | | | 441f51dc ci : build with CLBlast + ggml-opencl use GGML_API (whisper/1576)
* | | | | | | | | | | | | 38b3de46 sync : ggml
* | | | | | | | | | | | | afc8c192 ggml : fix some mul mat cases + add tests for src1 F16 (ggml/669)
* | | | | | | | | | | | | ca38b8d3 scripts : do not sync commits from this repo
* | | | | | | | | | | | | 65e5f6da Fix OpenAI server sampling w.r.t. temp and seed (#4668)
* | | | | | | | | | | | | ea5497df gpt2 : Add gpt2 architecture integration (#4555)
* | | | | | | | | | | | | f6793491 llama : add AWQ for llama, llama2, mpt, and mistral models (#4593)
* | | | | | | | | | | | | 879b690a finetune : fix output formatting in print_params (#4653)
* | | | | | | | | | | | | b47879b0 scripts : add sync-ggml-am.sh
* | | | | | | | | | | | | 951010fa ggml : fix dot product for ARM (#4630)
* | | | | | | | | | | | | f56d6077 Add byte token type when tokenizer.model is not exists (#4641)
* | | | | | | | | | | | | dc68f005 cuda : fix vmm pool with multi GPU (#4620)
* | | | | | | | | | | | | de8e4964 Update comment for AdamW implementation reference. (#4604)
* | | | | | | | | | | | | 77465dad Fix new CUDA10 compilation errors (#4635)
* | | | | | | | | | | | | a206137f Adding Emeltal reference to UI list (#4629)
| | | | | | | | | | | | | * f32f30bc test
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | b9f47952 simplify bug issue template (#4623)
* | | | | | | | | | | | | 753be377 llama : add PLaMo model (#3557)
* | | | | | | | | | | | | 5bf3953d cuda : improve cuda pool efficiency using virtual memory (#4606)
* | | | | | | | | | | | | 708e179e fallback to CPU buffer if host buffer alloc fails (#4610)
* | | | | | | | | | | | | 925e5584 ci(docker): fix tags in "Build and push docker image (tagged)" (#4603)
* | | | | | | | | | | | | 61239799 server : allow to specify custom prompt for penalty calculation (#3727)
* | | | | | | | | | | | | b9ec82d2 grammar : check the full vocab only if necessary (opt) (#4306)
* | | | | | | | | | | | | e0a40022 CUDA: fixed row rounding for 0 tensor splits (#4594)
* | | | | | | | | | | | | 7082d24c lookup : add prompt lookup decoding example (#4484)
* | | | | | | | | | | | | ba661751 sync : ggml (fix im2col) (#4591)
* | | | | | | | | | | | | a5587695 cuda : fix jetson compile error (#4560)
* | | | | | | | | | | | | 6724ef16 Fix CudaMemcpy direction (#4599)
* | | | | | | | | | | | | 48b7ff19 llama : fix platforms without mmap (#4578)
* | | | | | | | | | | | | 48b24b17 ggml : add comment about backward GGML_OP_DIAG_MASK_INF (#4203)
* | | | | | | | | | | | | 28cb35a0 make : add LLAMA_HIP_UMA option (#4587)
* | | | | | | | | | | | | f31b9848 ci : tag docker image with build number (#4584)
* | | | | | | | | | | | | 2bb98279 readme : add zig bindings (#4581)
* | | | | | | | | | | | | 0137ef88 ggml : extend `enum ggml_log_level` with `GGML_LOG_LEVEL_DEBUG` (#4579)
* | | | | | | | | | | | | c7e9701f llama : add ability to cancel model loading (#4462)
* | | | | | | | | | | | | afefa319 ggml : change ggml_scale to take a float instead of tensor (#4573)
* | | | | | | | | | | | | 769a7bc8 gguf-py : fix broken link
* | | | | | | | | | | | | 32259b2d gguf : simplify example dependencies
* | | | | | | | | | | | | 4a5f9d62 ci : add `jlumbroso/free-disk-space` to docker workflow (#4150)
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
| | | | | | | | | | | | *   ab1b7516 Merge branch 'master' into gg/ggml_scale
| | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | d232aca5 llama : initial ggml-backend integration (#4520)
| |_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 31f27758 llama : allow getting n_batch from llama_context in c api (#4540)
* | | | | | | | | | | | 56fa5081 metal : fix `ggml_metal_log` vargs (#4373)
* | | | | | | | | | | | 0f630fbc cuda : ROCm AMD Unified Memory Architecture (UMA) handling (#4449)
* | | | | | | | | | | | 562cf222 ggml-cuda: Fix HIP build by adding define for __trap (#4569)
| | | | | | | | | | | * b784f881 tests : fix test-grad0
| | | | | | | | | | | * 36c3f41f ggml : fix CPU implementation
| | | | | | | | | | | * 199f6bdc ggml : change ggml_scale to take a float instead of tensor
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 8fe03ffd common : remove incorrect --model-draft default (#4568)
* | | | | | | | | | | 91544948 CUDA: mul_mat_id always on GPU for batches >= 32 (#4553)
* | | | | | | | | | | c083718c readme : update coding guidelines
| | | | | | | | | | | * 7c87353e common : remove incorrect --model-draft default
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 880e3522 py : open merges file as 'utf-8' (#4566)
* | | | | | | | | | | 66f35a2f cuda : better error message for ggml_get_rows (#4561)
* | | | | | | | | | | 13988239 cuda : replace asserts in wrong architecture checks with __trap (#4556)
* | | | | | | | | | | d3223afd llama : disable per-tensor info prints on model load (#4562)
* | | | | | | | | | | 1d7a1912 Fix access violation in ggml_cuda_free_data if tensor->extra is NULL (#4554)
* | | | | | | | | | | 799fc226 CUDA: Faster Mixtral prompt processing (#4538)
* | | | | | | | | | | 328b83de ggml : fixed check for _MSC_VER (#4535)
| | | | | | | | | | | * a40f6110 ggml : force F32 precision for ggml_mul_mat
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | a7aee47b ggml-cuda: Fix HIP build (#4528)
* | | | | | | | | | | 0e18b2e7 llama.swiftui : add tinyllama 1.1B F16
* | | | | | | | | | | 6ff39b12 llama.swiftui : add more models
* | | | | | | | | | | b9e74f9b llama : add phi-2 + fix NeoX rope + ggml_mul_mat_set_prec (#4490)
* | | | | | | | | | | 3c04bf6d llama : fix try_override for bool_value which always return true (#4519)
| | | | | | | | | | | * 3c734f49 plamo : testing
| | | | | | | | | | | * 9339ffc9 update README
| | | | | | | | | | | * 907b9218 remove develop code
| | | | | | | | | | | * febc6359 update kqv code
| | | | | | | | | | | * ca8f6986 seems ok
| | | | | | | | | | | * f76fd392 use inp_pos
| | | | | | | | | | | * 86d5348f runnable
| | | | | | | | | | | * a22040a8 fix norm_rms_eps hparam
| | | | | | | | | | | * 4a3ef4f2 able to compile
| | | | | | | | | | | * 9d492365 update norm
| | | | | | | | | | | * b2330f57 plamo convert
| | | | | | | | | | | * 4c585b4c add tensor loading
| | | | | | | | | | | * feb0966a add plamo mock
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
| | | | | | | | | | | * a462159c cuda : ggml_cuda_op_mul_mat_cublas support F32 precision
| | | | | | | | | | | * 30338c56 Update ggml-cuda.cu
| | | | | | | | | | | * 3c8d6b16 Update ggml-cuda.cu
| | | | | | | | | | | * 18c67bdd ggml : add ggml_mul_mat_set_prec
| | | | | | | | | | | *   a8d2a6f3 Merge branch 'master' into HEAD
| | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 2994f0c5 decode : fix logits_valid for legacy API (#4516)
| | | | | | | | | | | * 42e95258 cuda : less diff in the rope_neox kernel
| | | | | | | | | | | * f703ca8a ggml : fix NeoX rope to rotate just first n_dims
| | | | | | | | | | | | * 1b058171 decode : fix logits_valid for old API
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | b1306c43 readme : update hot topics
* | | | | | | | | | | | 800a489e llama.swiftui : add bench functionality (#4483)
* | | | | | | | | | | | f7f468a9 gguf-py : fail fast on nonsensical special token IDs (#4489)
* | | | | | | | | | | | 919c4066 build : Check the ROCm installation location (#4485)
* | | | | | | | | | | | 45668633 finetune : keep allocs alive until all allocations are done (#4486)
* | | | | | | | | | | | 0ffc92d2 server : disable llm logs if SERVER_VERBOSE is off (#3792)
* | | | | | | | | | | | 8edd2b40 server : fix grammar being ignored (#4494)
* | | | | | | | | | | | eb16dae7 server : fix possible ambiguity in content type charset (#4501)
* | | | | | | | | | | | 62bd52b7 server : allow requests larger than 8K (#4500)
* | | | | | | | | | | | 5daa5f54 Link to cublas dynamically on Windows even with LLAMA_STATIC (#4506)
| | | | | | | | | | | | * 86506662 llama.swiftui : improve bench
| | | | | | | | | | | | * 5c5bdba6 llama : remove "mostly" from model infos
| | | | | | | | | | | | * 262fd466 llama.swiftui : remove model from project
| | | | | | | | | | | | * 4ed98b90 llama.swiftui : avoid data copy via "downloadTask"
| | | | | | | | | | | | * 96294487 llama.swiftui : UX improvements
| | | | | | | | | | | | * d36ca171 gitignore : xcode stuff
| | | | | | | | | | | | * da44d452 comment #Preview & fix editorconfig check
| | | | | | | | | | | | * a520e87e update project.pbxproj
| | | | | | | | | | | | * ce1df812 add download buttons & expose llamaState.loadModel
| | | | | | | | | | | | * ff87313d force to use n_gpu_layers on simulator
| | | | | | | | | | | | * 6a868020 llama.swiftui : initial bench functionality
| | | | | | | | | | | | * afd336f7 llama.swiftui : add bench button
| | | | | | | | | | | | | * f86b9d15 lookup : minor
| | | | | | | | | | | | | * 5b279754 lookup : fix token positions in the draft batch
| | | | | | | | | | | | | * 1b26d715 Added colors to distinguish drafted tokens (--color). Updated README
| | | | | | | | | | | | | *   45b8032b Merge branch 'prompt-lookup' of github.com:LeonEricsson/llama.cpp into prompt-lookup
| | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | *   34048416 Merge branch 'ggerganov:master' into prompt-lookup
| | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | * | | 21431197 kv_cache management
| | | | | | | | | | | | | |/ /  
| | | | | | | | | | | | | * | 1665ad8b BUG: generates gibberish/repeating tokens after a while
| | | | | | | | | | | | | * | 0ec5fdb5 main loop finished, starting to debug
| | | | | | | | | | | | | * | cae8f50b initial commit, going through initializations
| | | | | | | | | | | | | | | *   d2f1e0da Merge branch 'cuda-cublas-opts' into gg/phi-2
| | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | | *   e75889a9 Merge branch 'master' into cuda-cublas-opts
| | | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | | c6c4fc08 lora : add support for non-llama models (#3333)
* | | | | | | | | | | | | | | | | 8a5be3bd llama : sanity checks for access to logits (#4274)
| |_|_|_|_|_|_|_|_|_|_|_|_|/ / /  
|/| | | | | | | | | | | | | | |   
| | | | | | | | | | | | | | | *   66a8dd35 Merge branch 'master' into cuda-cublas-opts
| | | | | | | | | | | | | | | |\  
| | | | | | | | | | | | | | | * \   c830a053 Merge branch 'master' into cuda-cublas-opts
| | | | | | | | | | | | | | | |\ \  
| | | | | | | | | | | | | | | * | | e3742272 Revert "cuda : use CUBLAS_COMPUTE_16F for non-attention ops"
| | | | | | | | | | | | | | | * | | 0f2498f2 cuda : use CUBLAS_COMPUTE_16F for non-attention ops
| | | | | | | | | | | | | | | * | | 3b9ea655 cuda : use CUBLAS_COMPUTE_32F to speed-up and avoid dst cpy
| | | | | | | | | | | | | | * | | | b672c169 ggml : fix NeoX rope to rotate just first n_dims
| | | | | | | | | | | | |_|/ / / /  
| | | | | | | | | | | |/| | | | |   
| | | | | | | | | | | * | | | | | 0644c3be phi-2 : scale Q instead of KQ for better precision
| | | | | | | | | | | * | | | | | 0b6ffa58 convert : revert "added_tokens_decoder" change
| | | | | | | | | | | * | | | | | a878be4c convert : phi don't add BOS token
| | | | | | | | | | | * | | | | | 5469d82d llama : fix meta KV override bug
| | | | | | | | | | | * | | | | | 7500fa2f py : whitespaces
| | | | | | | | | | | * | | | | | aa5c881a phi-2 : use layer norm eps
| | | | | | | | | | | * | | | | | a2a3d2c8 phi-2 : various fixes
| | | | | | | | | | | * | | | | | e2076553 fix breaking change
| | | | | | | | | | | * | | | | | 12cc80cb phi2 implementation
| | | | | | | | | | | |/ / / / /  
| | | | | | | | | | | | | | | | * b0547d21 gguf-py : fail fast on nonsensical special token IDs
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 88ae8952 server : add optional API Key Authentication example (#4441)
* | | | | | | | | | | | | | | | ee4725a6 ggml : group mul_mat_id rows by matrix (cpu only) (#4480)
| |_|_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 6744dbe9 ggml : use ggml_row_size where possible (#4472)
* | | | | | | | | | | | | | | cafcd4f8 ggml : remove n_dims from ggml_tensor (#4469)
* | | | | | | | | | | | | | | c50e4001 py : add protobuf dependency (#4466)
* | | | | | | | | | | | | | | 20a68a70 ggml : add ggml_row_size() (fixes llama out of space) (#4461)
* | | | | | | | | | | | | | | 55e87c37 ggml : fix OpenCL broadcast requirement for ggml_mul (close #4453)
* | | | | | | | | | | | | | | 873637af convert : support loading vocab from fast tokenizer config (#3633)
* | | | | | | | | | | | | | | 0353a184 readme : update supported model list (#4457)
* | | | | | | | | | | | | | | 948ff137 server : fix handling of characters that span multiple tokens when streaming (#4446)
* | | | | | | | | | | | | | | 4d98d9a6 sync : ggml (SD ops, tests, kernels) (#4444)
* | | | | | | | | | | | | | | 70f806b8 build : detect host compiler and cuda compiler separately (#4414)
| | | | | | | | | | | | | | | *   c8554b80 Merge branch 'master' of https://github.com/ggerganov/llama.cpp into ceb/fix-cuda-warning-flags
| | | | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | | 9fb13f95 common : add `--version` option to show build info in CLI (#4433)
* | | | | | | | | | | | | | | | 113f9942 readme : update hot topics
* | | | | | | | | | | | | | | | 799a1cb1 llama : add Mixtral support (#4406)
* | | | | | | | | | | | | | | | fecac456 server : tweak default sampling parameters (#4367)
* | | | | | | | | | | | | | | | 9494d7c4 english : use `typos` to fix comments and logs (#4354)
* | | | | | | | | | | | | | | | 6138963f build : target Windows 8 for standard mingw-w64 (#4405)
* | | | | | | | | | | | | | | | 6391817c llama : document logits_all deprecation (#4418)
* | | | | | | | | | | | | | | | d9d4cfef server : fix local model name in server (#4420)
* | | | | | | | | | | | | | | | 41a11aaf ggml : increased GGML_MAX_PARAMS to allow finetuning of 70b models (#4424)
| | | | | | | | | | | | | | | * d870a9fd get_flags.mk -> get-flags.mk
| | | | | | | | | | | | | | | * cacac251 cmake : fix improper joining in generator expression
| | | | | | | | | | | | | | | * cdf3cc3c cmake : make CUDA warning stuff properly conditional
| | | | | | | | | | | | | | | * e30a8ad1 cmake : capitalize variables
| | | | | | | | | | | | | | | * b5b2cdff cmake : fix incorrect variable reference
| | | | | | | | | | | | | | | * a81a34ad cmake : detect host compiler and cuda compiler separately
| | | | | | | | | | | | | | | * abacb278 cmake : silence linker check stdout
| | | | | | | | | | | | | | | * 88781479 make : honor NVCC, LLAMA_CUDA_CCBIN, NVCCFLAGS
| | | | | | | | | | | | | | | * 93ca80fa make editorconfig checker happy
| | | | | | | | | | | | | | | * 91df2623 make : detect host compiler and cuda compiler separately
| | | | | | | | | | | | | | | * 9b28f341 make : simplify nvcc flags
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | 8a7b2fa5 Update README.md (#4388)
* | | | | | | | | | | | | | | e18f7345 grammar : revert the replacement of llama_token_to_piece with id_to_token (#4396)
| | | | | | | | | | | | | | | * e1241d9b metal : switch to execution barriers + fix one of the barriers
| | | | | | | | | | | | | | | * 109e7aa8 metal : limit kernels to not use more than the allowed threads
| | | | | | | | | | | | | | | * ab558ac2 metal : fix soft_max kernels
| | | | | | | | | | | | | | | * 82e4f645 convert-hf : support for mixtral-instruct (#4428)
| | | | | | | | | | | | | | | * 90c12e6b ggml : do not use BLAS with ggml_mul_mat_id
| | | | | | | | | | | | | | | * ea4402bb test-backend-ops : add one more sum_rows test
| | | | | | | | | | | | | | | * a51bc0c1 metal : fix binary ops for ne10 % 4 != 0
| | | | | | | | | | | | | | | * 08eb9917 metal : add cpy f16 -> f32 kernel
| | | | | | | | | | | | | | | * a742d9f9 gguf-py : bump version
| | | | | | | | | | | | | | | * 6a419f4d convert : support safetensors format
| | | | | | | | | | | | | | | * f1cbfabd convert : fix style
| | | | | | | | | | | | | | | * 7dc75e39 convert : use 1e6 rope_freq_base for mixtral
| | | | | | | | | | | | | | | * 296c945d cuda : fix mul_mat_id with multi gpu
| | | | | | | | | | | | | | | * 33e50f1b test-backend-ops : disable MOE test with thread sanitizer
| | | | | | | | | | | | | | | * ffda94c8 test-backend-ops : simplify and disable slow tests to avoid CI timeout
| | | | | | | | | | | | | | | * 8cbaed1d llama : fix hard-coded number of experts
| | | | | | | | | | | | | | | * b0029815 test-backend-ops : fix dequantize block offset
| | | | | | | | | | | | | | | * f1380d78 test-backend-ops : add cpy from f32 -> all types test
| | | | | | | | | | | | | | | * 54d254bb test-backend-ops : cleanup, add moe test for batches
| | | | | | | | | | | | | | | * 54ba2634 test-backend-ops : make experts more evenly probable (test_moe)
| | | | | | | | | | | | | | | * b0b83dd9 metal : fix ggml_mul_mat_id for F32
| | | | | | | | | | | | | | | * 65923a8e convert : determine n_ctx correctly
| | | | | | | | | | | | | | | * 8614aa73 cuda : fix get_rows when ncols is odd
| | | | | | | | | | | | | | | * cefebb36 test-backend-ops : add moe test
| | | | | | | | | | | | | | | * e640cbe0 llama : add n_expert and n_expert_used to hparams + change quants
| | | | | | | | | | | | | | | * d1259b7b llama : do not quantize expert gating tensors
| | | | | | | | | | | | | | | * 6cfb31f9 metal : add indirect mat-vec kernels for all quantization types
| | | | | | | | | | | | | | | * 016f9bb5 metal : fix ggml_get_rows to work with non-cont src1
| | | | | | | | | | | | | | | * 0710b0f7 llama : offload missing ffn_moe_silu
| | | | | | | | | | | | | | | * 62b95f93 cuda : support non-contiguous src1 in get_rows
| | | | | | | | | | | | | | | * 2e4db482 ggml : update get_rows f16 and q
| | | | | | | | | | | | | | | * ac3f7d8e ggml : get_rows : support non-contiguos tensors with gaps, generalize up to 3D
| | | | | | | | | | | | | | | * 8c5b66ee metal : reduce the kernel launches for ggml_mul_mat_id
| | | | | | | | | | | | | | | * 7e2006b0 metal : add/mul/div use general kernel when src1 not cont
| | | | | | | | | | | | | | | * 06dfde3e llama : add basic support for offloading moe with CUDA
| | | | | | | | | | | | | | | * 2cbcba82 metal : add more general support for ggml_get_rows + tests
| | | | | | | | | | | | | | | * 9064b1ca ggml : fix ggml_get_rows to take into account ne02 / ne11
| | | | | | | | | | | | | | | * ee8fb399 ggml : add n_as argument to ggml_mul_mat_id
| | | | | | | | | | | | | | | * 7372b622 ggml : ggml_get_rows support 2D indexing [n_tokens, n_experts] (cpu only)
| | | | | | | | | | | | | | | * 8b185b70 llama : fix expert weighting in the FFN
| | | | | | | | | | | | | | | * 7ea36953 llama : first working version
| | | | | | | | | | | | | | | * af1a096b llama : fix cur -> cur_expert
| | | | | | | | | | | | | | | * aedfad12 llama : update graph to support MoE
| | | | | | | | | | | | | | | * 861cd678 ggml : sync latest ggml_mul_mat_id
| | | | | | | | | | | | | | | * a3eefe95 llama : model loading
| | | | | | | | | | | | | | | * d38e41ee convert : fix n_ff typo
| | | | | | | | | | | | | | | * dff8cbeb convert : support Mixtral as LLAMA arch
| |_|_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | | |   
* | | | | | | | | | | | | | | fe680e3d sync : ggml (new ops, tests, backend, etc.) (#4359)
| |_|_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | bcc0eb45 llama : per-layer KV cache + quantum K cache (#4309)
| |_|_|_|_|_|_|_|/ / / / /  
|/| | | | | | | | | | | |   
| | | | | | | | | | | | | * fc5f3346 readme : add API change notice
| | | | | | | | | | | | | *   680a99e7 Merge branch 'master' into gg/per-layer-kv
| | | | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | | |   
* | | | | | | | | | | | | | 81bc9214 train : fix #4227 (double free in examples/train-text-from-scratch/train-text-from-scratch.cpp) (#4351)
* | | | | | | | | | | | | | 05cd6e50 server : recognize cache_prompt parameter in OAI API (#4347)
* | | | | | | | | | | | | | caa92492 common : fix compile warning
* | | | | | | | | | | | | | da5eaef1 speculative : support `--color` (#4343)
* | | | | | | | | | | | | | 5f6e0c0d grammar : pre-computed pieces + reserve mem + less string copies (#4330)
| |_|_|_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 5aa365d8 llama : allow overriding GGUF metadata when loading model (#4092)
* | | | | | | | | | | | | 52c8bc3c sampling : custom samplers order (#4285)
* | | | | | | | | | | | | e4b76bbe swift : revert compiler checks for swift package (#4332)
| |_|_|_|_|_|_|_|/ / / /  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 23b5e12e simple : update error message for KV cache check (#4324)
* | | | | | | | | | | | d208995c swift : fix concatenation method to avoid invalid UTF8 stringfication (#4325)
* | | | | | | | | | | | 5c9f90cb swift : fix prompt tokenization logic (#4321)
* | | | | | | | | | | | 4fa44e84 grammar-parser : fix typo (#4318)
| | | | | | | | | | | * 1a1a1c38 llama : support quantum K cache (#4312)
| | | | | | | | | | | | * af99c6fb llama : remove memory_f16 and kv_f16 flags
| | | | | | | | | | | | * 4adb1d69 cuda : add comment
| | | | | | | | | | | | * dd86df82 metal : use mm kernel only for quantum KV cache
| | | | | | | | | | | | * 903167a7 llama-bench : support type_k/type_v
| | | | | | | | | | | | * b2acedeb cuda : add F32 -> Q4_0 and F32 -> Q4_1 copy kernels
| | | | | | | | | | | | * e8457c90 cuda : wip
| | | | | | | | | | | | * 6b58ae98 metal : add F32 -> Q4_1 copy kernel
| | | | | | | | | | | | * 9d69ecc0 metal : add F32 -> Q4_0 copy kernel
| | | | | | | | | | | | * 7864a2cd llama : fix build
| | | | | | | | | | | | * 3ce30e07 llama : pass KV cache type through API
| | | | | | | | | | | | * b881f630 cuda : use mmv kernel for quantum cache ops
| | | | | | | | | | | | * a1bf6c09 cuda : add F32 -> Q8_0 copy kernel
| | | | | | | | | | | | * bcfebf24 metal : add F32 -> Q8_0 copy kernel
| | | | | | | | | | | | * d04ee928 llama : support quantum K cache (wip)
| | | | | | | | | | | |/  
| | | | | | | | | | | * 66aaac98 llama : update session save/load
| | | | | | | | | | | * e262947d common : add command-line arg to disable KV cache offloading
| | | | | | | | | | | * c80b8a2b llama : remove mirrors, perform Device -> Host when partial offload
| | | | | | | | | | | * c44bc1ee llama : keep the KV related layers on the device
| | | | | | | | | | | * 1fa91a48 llama : enable offload debug temporarily
| | | | | | | | | | | * 3d3e6bd0 llama : offload for rest of the model arches
| | | | | | | | | | | * f3dbfb9f llama : offload K shift tensors
| | | | | | | | | | | * 986b3da7 llama : offload KV cache per-layer
| | | | | | | | | | | *   c294c78e Merge branch 'master' into per-layer-kv
| | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | fbbc4282 ggml : reuse ggml_get_n_tasks() in ggml_graph_plan() (#4308)
* | | | | | | | | | | | adf3de4f ggml : fix soft max out-of-bounds access (#4307)
* | | | | | | | | | | | 33e171d1 server : fix OpenAI API `stop` field to be optional (#4299)
* | | | | | | | | | | | 6949b50d py : add grammar to oai like api (#4294)
* | | | | | | | | | | | d7b800b8 llama : pad KV cache size (#4280)
| | | | | | | | | | | * f4f9367f less code duplication, offload k and v separately
| | | | | | | | | | | * 55f2f2fb remove unnecessary copies
| | | | | | | | | | | * e9bcf66a per-layer KV
| | | | | | | | | | | | * 3cb1c348 metal : try to improve batched decoding
| | | | | | | | | | | | * 3e68df86 llama : pad KV cache size to 32
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 5a7d3125 llama : avoid using "optional" keyword (#4283)
* | | | | | | | | | | | d5a1cbde llama : support optional tensors (#4283)
* | | | | | | | | | | | b220222a swift : fix token_to_piece implementation (#4278)
* | | | | | | | | | | | 511f52c3 build : enable libstdc++ assertions for debug builds (#4275)
* | | | | | | | | | | | 03562f3a llama : support attention bias on LLaMA architecture (#4283)
* | | | | | | | | | | | 37c746d6 llama : add Qwen support (#4281)
* | | | | | | | | | | | 880f5797 llama : fix integer overflow during quantization (#4284)
* | | | | | | | | | | | 8d6d9f03 py : add requirements file for convert-hf-to-gguf.py (#4277)
* | | | | | | | | | | | ef47ec18 ggml : add ggml_soft_max_ext (#4256)
| | | | | | | | | | | | * eb594c0f alloc : fix build with debug
| | | | | | | | | | | | * d9c8fa3b metal : simplify soft max kernel
| | | | | | | | | | | | * c4db5923 metal : warp-based reduce for rms_norm
| | | | | | | | | | | | * 55717c98 metal : warp-based reduction for soft max kernel
| | | | | | | | | | | | * 68e02c0d cuda : fix warp reduction initialization of shared mem
| | | | | | | | | | | | * 6b86bcff cuda : increase max block size to 1024
| | | | | | | | | | | | * 62532c05 cuda : do warp-based block reduce
| | | | | | | | | | | | * c7c8dabc ggml : update soft max cpu
| | | | | | | | | | | | * ebd062bc cuda : use 512 threads for soft_max instead of 32
| | | | | | | | | | | | * 580fe206 metal : simplify soft_max encoding
| | | | | | | | | | | | * 390a4459 batched-bench : print threads
| | | | | | | | | | | | * 6a66f69f ggml : implement soft_max_ext (CPU)
| | | | | | | | | | | | * 88519fbf cuda : implement soft_max_ext
| | | | | | | | | | | | * e89597c0 metal : implement soft_max_ext
| | | | | | | | | | | | | * 5b74310e build : enable libstdc++ assertions for debug builds
| |_|_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | | |   
* | | | | | | | | | | | | 1d144112 server : add --log-disable to disable logging to file (#4260)
* | | | | | | | | | | | | f43f0936 server : add single-client multi-prompt support (#4232)
* | | | | | | | | | | | | d2809a3b make : fix Apple clang determination bug (#4272)
* | | | | | | | | | | | | 15f5d960 build : fix build info generation and cleanup Makefile (#3920)
* | | | | | | | | | | | | 33c9892a llava : ShareGPT4V compatibility (vision encoder only loading) (#4172)
* | | | | | | | | | | | | 8efa0f6e main : pass LOG_TEE callback to llama.cpp log (#4033)
* | | | | | | | | | | | | 524907aa readme : fix (#4135)
* | | | | | | | | | | | | 3bd2c7ce docker : add finetune option (#4211)
* | | | | | | | | | | | | bde629bb batched.swift : update README.md (#4214)
* | | | | | | | | | | | | f7f9e062 cmake : fix the metal file foder path (#4217)
* | | | | | | | | | | | | 74daabae readme : fix typo (#4253)
* | | | | | | | | | | | | b18c66ca llama : fix alignment of general.name in print meta (#4254)
* | | | | | | | | | | | | f4d973ce convert.py : fix llama/llama2 conversion due to vocab_size=-1 (#4258)
* | | | | | | | | | | | | 954e2285 llama : fix typical sampling (#4261)
* | | | | | | | | | | | | e2bd725f py : fix oai proxy (#3972)
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 1f5cd832 examples : add readme files
* | | | | | | | | | | | 4fea3420 readme : add FreeChat (#4248)
* | | | | | | | | | | | 64e64aa2 ggml : restore abort() in GGML_ASSERT (#4242)
* | | | | | | | | | | | 8406b092 ggml : re-enable BLAS for CPU when src0 != F32 + remove redundant full offload checks in llama.cpp (#4240)
| | | | | | | | | | | | * bb39b879 ggml : restore abort() in GGML_ASSERT
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | b38a16df cmake : fix issue with version info not getting baked into LlamaConfig.cmake (#3970)
| | | | | | | | | | | | * 87f4102a llama : revert n_threads_batch logic
| | | | | | | | | | | | * e9b7a5cb llama : use n_threads_batch only when n_tokens >= 32
| | | | | | | | | | | | * f815fe43 ggml : use blas even if src0 is not F32
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | 0dab8cd7 readme : add Amica to UI list (#4230)
* | | | | | | | | | | | bb03290c examples : iOS example with swift ui (#4159)
| |_|_|_|_|_|_|_|_|/ /  
|/| | | | | | | | | |   
* | | | | | | | | | | f3b26981 ggml : fix -Warray-bounds warning with gcc (#4231)
| | | | | | | | | | | * 6272b676 use stride=128 if built for tensor cores
| | | | | | | | | | | * dd71a35c make MUL_MAT_SRC1_COL_STRIDE conditional on runtime mmq
| | | | | | | | | | | * 12fb1c58 cuda : tweak mm stride to double perf on P40 + GTX 970
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 3e73d31d lookahead : support `-n -1` infinite generation
* | | | | | | | | | | 9656026b readme : update hot topics
* | | | | | | | | | | 922754a8 lookahead : add example for lookahead decoding (#4207)
* | | | | | | | | | | 22da0553 metal : fix yarn (#4220)
* | | | | | | | | | | 1ddb52ec scripts : Use mmap in torch load (#4202)
* | | | | | | | | | | f837c3a9 llama : grammar `reserve` space in `decode_utf8` (#4210)
* | | | | | | | | | | 3014b541 Update docs for yarn_ext_factor <0.0 as unspecified instead of NaN (#4189)
* | | | | | | | | | | 04814e71 readme : update hot topics
* | | | | | | | | | | af19d357 server : OAI API compatibility (#4198)
* | | | | | | | | | | e9c13ff7 llama : set metal log callback correctly (#4204)
* | | | | | | | | | | 8a052c13 ggml-cuda : support stablelm rope (#4156)
* | | | | | | | | | | 189d6844 convert : fix tensors using grad in some models (#4173)
| | | | | | | | | | | * 8d8b76d4 lookahead : add comments
| | | | | | | | | | | * 1a07a339 lookahead : fix a bug in the seq_id of the lookahead tokens
| | | | | | | | | | | * 7d50de2d lookahead : add to Makefile
| | | | | | | | | | | * 7bd1cd7e lookahead : use deterministic init
| | | | | | | | | | | * 6eb5166e lookahead : filter repeating n-grams
| | | | | | | | | | | * 61d03972 lookahead : initial working implementation
| | | | | | | | | | | * 1b2e0bc3 lookahead : use loop instead recursion to generate n-grams
| | | | | | | | | | | * eb03b9ad lookahead : generate and store n-grams
| | | | | | | | | | | * 7c517e17 lookahead : init
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 2568a4bf main.swift : fix eos checking (#4197)
| | | | | | | | | | | * 21b70bab straightforward /v1/models endpoint
| | | | | | | | | | | * b6163142 server : change random string generator
| | | | | | | | | | | * b3e88bf4 server : minor code style
| | | | | | | | | | | * c544faed server : enable special tokens during tokenization by default
| | | | | | | | | | | * b94b1091 server : indentation
| | | | | | | | | | | *   80724eb0 Merge branch 'master' into server-oai-compat
| | | | | | | | | | | |\  
| |_|_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | | |   
* | | | | | | | | | | | b35f3d0d readme : use PATH for Windows ROCm (#4195)
* | | | | | | | | | | | 55978ce0 Fix incorrect format strings and uninitialized variables. (#4133)
| |_|_|_|_|_|_|/ / / /  
|/| | | | | | | | | |   
* | | | | | | | | | | 6b0a7420 llama : KV cache view API + better KV cache management (#4170)
* | | | | | | | | | | d103d935 readme : update hot topics
* | | | | | | | | | | 9d5949f0 examples : fix typo in parallel example doc comment (#4181)
* | | | | | | | | | | ff8238f7 docs : add llama-star arch idea
| | | | | | | | | | * f25308be server : some style changes
| | | | | | | | | | * e1516709 Fix server.cpp code style according to review
| | | | | | | | | | * 9ad4d273 Improve server README.md
| | | | | | | | | | * af4d68b2 Update server README.md
| | | | | | | | | | * 2f84f5dc fix code style
| | | | | | | | | | * a0a08eed Add openai-compatible POST /v1/chat/completions API endpoint to server example
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
| | | | | | | | | | * f8e9f114 common : add -dkvc arg for enabling kv cache dumps
| | | | | | | | | | * 5df7d06c llama : allow exporting a view of the KV cache (#4180)
| | | | | | | | | | * 671f639c llama : zero KV cache used upon clear
| | | | | | | | | | * 79cb8f00 llama : keep track of used KV cells + better KV cache management
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 8e672efe stablelm : simplify + speedup generation (#4153)
* | | | | | | | | | 0b871f1a finetune - update readme to mention llama support only (#4148)
* | | | | | | | | | dfc7cd48 readme : update ROCm Windows instructions (#4122)
* | | | | | | | | | 881800d1 main : Add ChatML functionality to main example (#4046)
* | | | | | | | | | f23c0359 ci : add flake8 to github actions (python linting) (#4129)
* | | | | | | | | | 40a34fe8 speculative : fix prompt tokenization in speculative example (#4025)
* | | | | | | | | | dae06c06 Revert "finetune : add --n-gpu-layers flag info to --help (#4128)"
* | | | | | | | | | 05e8301e finetune : add --n-gpu-layers flag info to --help (#4128)
* | | | | | | | | | 936c79b2 server : relay error messages (#4131)
* | | | | | | | | | 262005ad common : comma should be semicolon (#4137)
* | | | | | | | | | 35985acf gitignore : tokenize
* | | | | | | | | | e9370664 gguf-py : export chat templates (#4125)
* | | | | | | | | | 28a2e6e7 tokenize example: Respect normal add BOS token behavior (#4126)
* | | | | | | | | | 0b5c3b04 scripts : Remove missed baichuan convert script (#4127)
* | | | | | | | | | 2923f17f Clean up ggml-cuda.cu warnings when compiling with clang (for ROCM) (#4124)
* | | | | | | | | | bbecf3f4 llama : increase max nodes (#4115)
* | | | | | | | | | 8e936108 build : support ppc64le build for make and CMake (#3963)
* | | | | | | | | | 5ad387e9 tokenize : fix trailing whitespace
* | | | | | | | | | 2fa02b4b examples : add tokenize (#4039)
* | | | | | | | | | 2ab0707a convert : use 'model' value if it exists. This allows karpathy/tinyllamas to load (#4089)
* | | | | | | | | | 11173c92 py : Falcon HF compatibility (#4104)
* | | | | | | | | | 9e87ef60 common : improve yaml log escaping (#4080)
* | | | | | | | | | c7cce124 llava : fix compilation warning that fread return value is not used (#4069)
* | | | | | | | | | f7d5e975 py : remove superfluous import statements (#4076)
* | | | | | | | | | ba4cf5c0 train : move number of gpu layers argument parsing to common/train.cpp (#4074)
* | | | | | | | | | e85bb1a8 llama : add functions to get the model's metadata (#4013)
* | | | | | | | | | 3e916a07 finetune : speed-up ggml_compute_forward_out_prod_f32 via BLAS (#4079)
* | | | | | | | | | 947f64f1 finetune : zero the loraB initial vectors (#4082)
* | | | | | | | | | b83e149e cuda : get_row_rounding F32 (#4095)
* | | | | | | | | | 4f447a48 llama : fix data units (#4101)
* | | | | | | | | | 91f64993 Respect tokenizer.ggml.add_bos_token value when tokenizing (#4040)
* | | | | | | | | | 8da46278 gguf : fix potential infinite loops while parsing (#4100)
| | | | | | | | | | * f8249026 YaRN : correction to GPT-NeoX implementation
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | a6fc554e llama : restore prefix space in llama tokenizer (#4081)
* | | | | | | | | | 1cf2850d ggml-cuda : increase max graph size (#4084)
* | | | | | | | | | 6bb4908a Fix MacOS Sonoma model quantization (#4052)
* | | | | | | | | | 36eed0c4 stablelm : StableLM support (#3586)
* | | | | | | | | | b46d12f8 convert.py: also look for plain model.safetensors  (#4043)
* | | | | | | | | | bd90eca2 llava : fix regression for square images in #3613 (#4056)
* | | | | | | | | | 3d68f364 ggml : sync (im2col, GPU conv, 32-bit arm compat) (#4060)
* | | | | | | | | | c049b37d readme : update hot topics
| |_|_|_|_|_|/ / /  
|/| | | | | | | |   
* | | | | | | | | 4760e7cc sync : ggml (backend v2) (#3912)
| |_|_|_|_|/ / /  
|/| | | | | | |   
* | | | | | | | bb50a792 Add ReLU and SQR CUDA ops to (partially) fix Persimmon offloading (#4041)
* | | | | | | | 21fd874c gguf-py: gguf_writer: Use bytearray to build metadata (#4051)
* | | | | | | | 532dd74e Fix some documentation typos/grammar mistakes (#4032)
* | | | | | | | e86fc56f Fix gguf-convert-endian script (#4037)
* | | | | | | | d96ca7de server : fix crash when prompt exceeds context size (#3996)
* | | | | | | | 34b0a082 gguf-py: Refactor and allow reading/modifying existing GGUF files (#3981)
* | | | | | | | 4a4fd3ee server : allow continue edit on completion mode (#3950)
* | | | | | | | df9d1293 Unbreak persimmon after #3837 (#4010)
| | | | | | | | * d0445a2e better documentation
| | | | | | | | * bfcbb5bc format -> std::to_string
| | | | | | | | * 07352f49 llama : add functions to get the model's metadata
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | a75fa576 scripts: Generalize convert scripts (#3838)
* | | | | | | | 57ad015d server : add min_p param (#3877)
* | | | | | | | 875fb428 ggml-alloc : fix backend assignments of views (#3982)
* | | | | | | | 0a7c980b gguf : track writer state, free unneeded tensors, cleanup (#3871)
* | | | | | | | 413503d4 make : do not add linker flags when compiling static llava lib (#3977)
* | | | | | | | e9c1cecb ggml : fix backward rope after YaRN (#3974)
* | | | | | | | 54b4df88 Use params when loading models in llava-cli (#3976)
* | | | | | | | 46876d2a cuda : supports running on CPU for GGML_USE_CUBLAS=ON build (#3946)
* | | | | | | | 381efbf4 llava : expose as a shared library for downstream projects (#3613)
* | | | | | | | 2833a6f6 ggml-cuda : fix f16 mul mat (#3961)
* | | | | | | | d9ccce2e Allow common process_escapes to handle \x sequences (#3928)
* | | | | | | | bb60fd0b server : fix typo for --alias shortcut from -m to -a (#3958)
* | | | | | | | 132d25b8 cuda : fix disabling device with --tensor-split 1,0 (#3951)
* | | | | | | | 3d48f42e llama : mark LLM_ARCH_STARCODER as full offload supported (#3945)
| | | | | | | | * 47d604fa fix issues
| | | | | | | | *   73c0010e Merge remote-tracking branch 'origin/master' into fix-tensor-split-zero
| | | | | | | | |\  
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | c41ea36e cmake : MSVC instruction detection (fixed up #809) (#3923)
* | | | | | | | | a7fac013 ci : use intel sde when ci cpu doesn't support avx512 (#3949)
* | | | | | | | | 48ade945 cuda : revert CUDA pool stuff (#3944)
| | | | | | | | * 05c51f96 cuda : fix disabling device with --tensor-split 1,0
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | f28af0d8 gguf-py: Support 01.AI Yi models (#3943)
| | | | | | | | * 3ef358ff Revert "cuda : use CUDA memory pool with async memory allocation/deallocation when available (#3903)"
| | | | | | | | * 6b10aa9f Revert "cuda : add ROCM aliases for CUDA pool stuff (#3918)"
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | d9b33fe9 metal : round up to 16 to fix MTLDebugComputeCommandEncoder assertion (#3938)
* | | | | | | | 5ba37461 ggml-metal: fix yarn rope (#3937)
* | | | | | | | abb77e73 ggml-cuda : move row numbers to x grid dim in mmv kernels (#3921)
* | | | | | | | 8f961abd speculative : change default p_accept to 0.5 + CLI args (#3919)
* | | | | | | | 05816027 common : YAYF (yet another YARN fix) (#3925)
* | | | | | | | 3fdbe6b6 llama : change yarn_ext_factor placeholder to -1 (#3922)
* | | | | | | | 629f917c cuda : add ROCM aliases for CUDA pool stuff (#3918)
* | | | | | | | 51b2fc11 cmake : fix relative path to git submodule index (#3915)
* | | | | | | | 224e7d5b readme : add notice about #3912
* | | | | | | | c7743fe1 cuda : fix const ptrs warning causing ROCm build issues (#3913)
* | | | | | | | d6069051 cuda : use CUDA memory pool with async memory allocation/deallocation when available (#3903)
* | | | | | | | 4ff1046d gguf : print error for GGUFv1 files (#3908)
* | | | | | | | 21958bb3 cmake : disable LLAMA_NATIVE by default (#3906)
* | | | | | | | 2756c4fb gguf : remove special-case code for GGUFv1 (#3901)
* | | | | | | | 1efae9b7 llm : prevent from 1-D tensors being GPU split (#3697)
* | | | | | | | b12fa0d1 build : link against build info instead of compiling against it (#3879)
* | | | | | | | 4d719a6d cuda : check if this fixes Pascal card regression (#3882)
* | | | | | | | 183b3fac metal : fix build errors and kernel sig after #2268 (#3898)
* | | | | | | | 2fffa0d6 cuda : fix RoPE after #2268 (#3897)
* | | | | | | | 0eb332a1 llama : fix llama_context_default_params after #2268 (#3893)
* | | | | | | | d02e98cd ggml-cuda : compute ptrs for cublasGemmBatchedEx in a kernel (#3891)
* | | | | | | | 898aeca9 llama : implement YaRN RoPE scaling (#2268)
* | | | | | | | c43c2da8 llm : fix llm_build_kqv taking unused tensor (benign, #3837)
* | | | | | | | 523e49b1 llm : fix falcon norm after refactoring (#3837)
| |_|_|_|/ / /  
|/| | | | | |   
* | | | | | | e16b9fa4 metal : multi-simd softmax (#3710)
| | | | | | | * 46868a49 metal : multi-simd softmax
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | ff8f9a88 common : minor (#3715)
* | | | | | | 50337961 llm : add llm_build_context (#3881)
* | | | | | | 0e40806c common : allow caller to handle help/argument exceptions (#3715)
* | | | | | | a2758d08 log : make generating separate log files optional (#3787)
* | | | | | | e75dfdd3 sampling : null grammar field after reset (#3885)
* | | | | | | 9a3b4f6c ggml : fix UNUSED macro (#3762)
* | | | | | | 73bdcb39 finetune : add -ngl parameter (#3762)
* | | | | | | f0e20932 scripts : add server-llm.sh (#3868)
* | | | | | | ca190bca server : re-enable completion and embedded at the same time (#3876)
| | | | | | | * a8796f96 llm : cleanup + comments
| | | | | | | * 78186f40 llm : restore the non-graph llm_build_ functional API
| | | | | | | * 995ee091 llm : deduce norm eps based on type + explict max_alibi_bias, clamp_kqv
| | | | | | | * 9284aa6a llm : add llm_build_context
| |_|_|_|_|_|/  
|/| | | | | |   
| | | | | | | * 7420bef8 wip wip wip
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 71e3718a llama : refactor graph build code (#3837)
* | | | | | | 238657db samplers : Min-P sampler implementation [alternative to Top P/Top K] (#3841)
| | | | | | | *   afb39292 Merge branch 'master' into llama-refactor
| | | | | | | |\  
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 07178c98 flake.nix: fix for rocm 5.7 (#3853)
| | | | | | | * 5baefef4 llama : add llm_build helper functions (#3848)
| | | | | | | | * 29fe5169 wip
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | * dab42893 scripts : working curl pipe
| | | | | | | | * f3947e1e scripts : rename to server-llm.sh
| | | | | | | | * 2f719c87 scripts : add deploy-server.sh
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 207b5190 ggml : move FP16 <-> FP32 code to ggml-impl.h (#3861)
| | | | | | | | * 7923b70c llama : add llm_build_inp_embd helper
| | | | | | | | * 2073347e llama : remove extra ; + deduplicate gate_b logic
| | | | | | | | * fc5a26aa llama : enable warning about not offloaded tensors
| | | | | | | | * 0bfdcdd0 llama : normalize tensor names
| | | | | | | | * 6669cd83 llama : update offload functions for KQ tensors
| | | | | | | | * 2926ef63 llama : fix input allocation logic
| | | | | | | | * a3f80013 llama : add LLAMA_OFFLOAD_DEBUG + fix starcoder offloading
| | | | | | | | * 792d1a1b llama : minor
| | | | | | | | * f39e6075 llama : add llm_build_kqv helper
| | | | | | | | * c9121fdd llama : remove obsolete comments in build graphs
| | | | | | | | * a104abea llama : simplify falcon Q, K, V computation
| | | | | | | | * 31a12f3d llama : fix llm_build_k_shift to use n_head_kv instead of n_head
| | | | | | | | * 59908619 llama : remove obsolete offload names
| | | | | | | | * 3e046259 llama : add llm_build_kv_store helper
| | | | | | | | * 909d6447 llama : fix offloading after recent changes
| | | | | | | | * 38728a0b llama : add llm_build_k_shift helper
| | | | | | | | * dbf836bb llama : add llm_build_ffn helper function (#3849)
| | | | | | | | * 7db9c96d llama : add llm_build_norm helper function
| | | | | | | |/  
| | | | | | | * 210e6e5d llama : remove obsolete map for layer counting
| | | | | | | * 79ad7344 llama : comment
| | | | | | | * 76108793 llama : add functional header
| | | | | | | * 8925cf9e llama : add layer index to all tensor names
| | | | | | | * 1e9c5443 llama : refactor tensor offloading as callback
| | | | | | | | * 4b3cb98d ggml-impl : move extern "C" to start of file
| | | | | | | | * d70917f4 ggml : prefix lookup tables with ggml_
| | | | | | | | * 1039a16c ggml : remove duplicate static assert macros
| | | | | | | | * 223696c9 ggml : add math.h to ggml-impl.h
| | | | | | | | * 334984e4 ggml : explicitly initialize deprecated type traits
| | | | | | | | * a1c3ff68 tests : fix ARM build
| | | | | | | | * d3e2cedb ggml : move FP16 <-> FP32 stuff to ggml-impl.h
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | * bc28aaa8 make : use -lfto=auto to avoid warnings and maintain perf
| | | | | | | | * 57c4296c ci : fix focal build
| | | | | | | | * a6aba2c8 ci : try to fix code coverage build
| | | | | | | | * 6f6b0db6 build : disable lto for C++ (make) and enable existing LTO flag (cmake)
| | | | | | | | * 1206b5f3 build : enable link-time optimizations
| |_|_|_|_|_|_|/  
|/| | | | | | |   
* | | | | | | | 6e08281e Extend llama_kv_cache_seq_rm to allow matching any sequence (#3843)
* | | | | | | | 2046eb43 make : remove unnecessary dependency on build-info.h (#3842)
* | | | | | | | 71a09da3 llama : fix kv shift bug (#3835)
* | | | | | | | d69d777c ggml : quantization refactoring (#3833)
| | | | | | | | * 15267192 llama : refactor tensor offloading as callback
| | | | | | | |/  
| | | | | | | * da936188 llama : move refact in correct place + optimize graph input
| | | | | | | * 739b85c9 llama : try to fix build
| | | | | | | * 25cfbf67 llama : fix non-CUDA build
| | | | | | | * b4ad03b3 llama : try to optimize offloading code
| | | | | | | * 79617902 llama : fix res_norm offloading
| | | | | | | * e14aa461 llama : do tensor offload only with CUDA
| | | | | | | * 0dc05b84 llama : factor graph input into a function
| | | | | | | * 4e98897e llama : support offloading result_norm + comments
| | | | | | | * 51c4f9ee llama : comments
| | | | | | | * 3af87713 llama : update offload log messages to print node index
| | | | | | | * 83d2c437 llama : offload rest of the models
| | | | | | | * 38aca9e1 llama : factor out tensor offloading outside the build call (wip)
| | | | | | | * 5946d98f metal : disable kernel load log
| | | | | | | * 8b2420d2 llama : factor out ggml-alloc from graph graph build functions
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | ff3bad83 flake : update flake.lock for newer transformers version + provide extra dev shell (#3797)
| | | | | | | * 8a86b95e quantize : --pure option for disabling k-quant mixtures
| | | | | | | * ee37e35d ggml-quants : fix Zig and Swift builds + quantize tool
| | | | | | | * 3412be72 ggml : factor all quantization code in ggml-quants
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 82a6646e metal : try cwd for ggml-metal.metal if bundle lookup fails (#3793)
* | | | | | | ba231e8a issues : change label from bug to bug-unconfirmed (#3748)
* | | | | | | 8a2f2fea convert : ignore tokens if their IDs are within [0, vocab_size) (#3831)
| | | | | | | * de7e0912 convert : ignore tokens if their IDs are within [0, vocab_size)
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | bd6d9e20 llama : allow quantizing k-quants to fall back when tensor size incompatible (#3747)
* | | | | | | ee1a0ec9 llama : add option for greedy sampling with probs (#3813)
* | | | | | | 17746110 common : print that one line of the syntax help *also* to standard output (#3823)
* | | | | | | fdee152e starcoder : add GPU offloading (#3827)
* | | | | | | 41aee4df speculative : ensure draft and target model vocab matches (#3812)
* | | | | | | 6d459cbf llama : correctly report GGUFv3 format (#3818)
| |_|_|_|/ /  
|/| | | | |   
* | | | | | c8d6a1f3 simple : fix batch handling (#3803)
| | | | | | * bbfc62ac sampling : temp == 0.0 -> no probs, temp < 0.0 -> probs
| | | | | | * c86cca80 llama : add comment about llama_sample_token_greedy() missing probs
| | | | | | * 4aa1fb0d llama : add option for greedy sampling with probs
| | | | | | | * cd3e20fb cuda : fix multi-gpu with tensor cores
| | | | | | | * 706ff4c2 cuda : try to fix main device write
| | | | | | | * 1a0843c4 cuda : utilize tensor cores with multiple GPU devices
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 2f9ec7e2 cuda : improve text-generation and batched decoding performance (#3776)
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 34b2a5e1 server : do not release slot on image input (#3798)
| | | | | | * 49af767f build : add compile option to force use of MMQ kernels
| | | | | | * a4e15a36 cuda : add CUDA_USE_TENSOR_CORES and GGML_CUDA_FORCE_MMQ macros
| | | | | | * 4c6744b5 cuda : remove duplicated cuBLAS GEMM code
| | | | | | * a3c28439 cuda : fine-tune >= VOLTA params + use MMQ only for small batches
| | | | | | * 16b60dd7 cuda : add F32 sgemm branch
| | | | | | * 52af7826 cuda : new cublas gemm branch for multi-batch quantized src0
| | | | | | * 59d1232e cuda : prints wip
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 6961c4bd batched-bench : print params at start
* | | | | | cc448774 log : disable pid in log filenames
* | | | | | ad939626 server : add parameter -tb N, --threads-batch N (#3584) (#3768)
* | | | | | 1717521c server : do not block system prompt update (#3767)
* | | | | | b2f7e04b sync : ggml (conv ops + cuda MSVC fixes) (#3765)
* | | | | | abd21fc9 cmake : add missed dependencies (#3763)
* | | | | | 2b4ea35e cuda : add batched cuBLAS GEMM for faster attention (#3749)
* | | | | | daab3d7f Add more tokenizer tests (#3742)
| |_|_|/ /  
|/| | | |   
* | | | | 469c9add metal : handle ggml_scale for n%4 != 0 (close #3754)
* | | | | e3932593 Revert "make : add optional CUDA_NATIVE_ARCH (#2482)"
* | | | | 9d029564 issues : separate bug and enhancement template + no default title (#3748)
* | | | | 69a67350 Update special token handling in conversion scripts for gpt2 derived tokenizers (#3746)
* | | | | 5be6c803 llama : remove token functions with `context` args in favor of `model` (#3720)
* | | | | 6336701c Fix baichuan convert script not detecing model (#3739)
| | | | | * d798a17c cuda : add TODO for calling cublas from kernel + using mem pool
| | | | | * 27c34c01 cuda : reduce mallocs in cublasGemmBatchedEx branch
| | | | | * 3d297c1a cuda : add cublasGemmStridedBatchedEx for non-broadcasted cases
| | | | | | * 69664749 cuda : play with faster Q4_0 dequantization
| | | | | |/  
| | | | | * d4156690 cuda : add ROCm / hipBLAS cublasGemmBatchedEx define
| | | | | * 878aa4f2 Apply suggestions from code review
| | | | | * c13fcfbf cuda : batched cuBLAS GEMMs for src0 F16 and src1 F32 (attention ops)
| | | | | * 84d4ca0e cuda : minor indentation
| | | | | * 8d8d54f8 ggml : skip nops in compute_forward
| | | | | * 6a30bf3e batched : add NGL arg
| | | | | * 8fb1be64 cmake : add helper for faster CUDA builds
| |_|_|_|/  
|/| | | |   
| | | | | * b9bb4cbe Separate bug and enhancement template + no default title
| |_|_|_|/  
|/| | | |   
* | | | | 96981f37 make : add optional CUDA_NATIVE_ARCH (#2482)
* | | | | 438c2ca8 server : parallel decoding and multimodal (#3677)
* | | | | 9e70cc03 Add test for MPT tokenization (#3728)
* | | | | 5a42a5f8 readme : remove unsupported node.js library (#3703)
* | | | | a5e7dbd6 llama : validate special token ids are in range when loading GGUF model (#3635)
* | | | | d3956aea main : escape prompt for cfg_negative_prompt and consecutive inputs in main with interactive (#3623)
| | | | | * c0f4d548 server : add comment about changing slot_state to bool
| | | | | * 83e14901 server : fix slot reuse
| | | | | * 8fe7ca48 server : apply fix from #3722
| | | | | * 00ae55b3 server : hide ctx_sampling->prev behind API (#3696)
| | | | | * 3d6a687f Update readme to document multimodal in server
| | | | | * dd1af2ed server : minor style
| | | | | *   a4d69d8b Merge branch 'server-rev' of https://github.com//ggerganov/llama.cpp into server-rev
| | | | | |\  
| | | | | | * a8063171 server : completion requests remember slot_id
| | | | | * | 2679c432 Update readme to document multimodal in server
| | | | | |/  
| | | | | * f305d643 editorconfig : new line in index.html
| | | | | * 5359fb92 Do not save/load image_data to localStorage
| | | | | * f67d9713 server : bug fix for prompt caching
| | | | | * 569ebf11 server : refactor ctx_sampling init + n_ctx + names
| | | | | * ef18f4d5 server : fix crash in Debug on macOS (I have no idea why this fixes it!?)
| | | | | * 197a0a9e server : fix switch fallthrough
| | | | | * 715f384a clip : link to ggml, not to llama
| | | | | * 4b4ab722 make : silence stb warnings
| | | | | *   176993c8 Merge branch 'master' into server-rev
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
| | | | | * 2eb4c11e fix image load + view image in chat
| | | | | * 17b23eb9 server : fix multibyte handle in partial response (#3706)
| | | | | * 778c070d server : logs + minor code style
| | | | | * 5d540e80 server : no need for atomic int - already using mutex
| | | | | * 113dd600 server : bach has to be allocated for n_parallel sequences
| | | | | * 6b2437e3 added thread safe pipeline
| | | | | * 325d1793 server : minor sync
| | | | | * 9740824b server : snake case
| | | | | * e3a2c3fe server : use refs + use llama_batch_clear()
| | | | | * 3d5929e8 server : bug fix in ingest_images
| | | | | * a8c981b7 server : remove beam-search functionality
| | | | | * 654e0a1f server : coding-style normalization (part 2)
| | | | | * e44ed601 server : coding-style normalization
| | | | | * ab2fc002 latest changes of sampling API
| | | | | *   8540568c Merge branch 'master' of https://github.com/ggerganov/llama.cpp
| | | | | |\  
| | | | | * | 7196c4e0 new sampling API
| | | | | * |   84b8f2b0 Merge branch 'ggerganov:master' into master
| | | | | |\ \  
| | | | | * | | 35fd3743 fix zig build
| | | | | * | | c02c52ef fix multiple clients
| | | | | * | | d2b1fac6 fix make bui;d errors
| | | | | * | | ed0c11cb multimodal support enabled by default
| | | | | * | | 6c277eaa update api like OpenAI
| | | | | * | | 58f8ae9b readme change
| | | | | * | |   fa0f22f1 Merge remote-tracking branch 'upstream/master'
| | | | | |\ \ \  
| | | | | * | | | aa2268f4 sync README.md changes
| | | | | * | | | 4d180433 fix llava implementation
| | | | | * | | | d7eca255 context shift fixed
| | | | | * | | | 2d9f11db fixed premature end due stop word
| | | | | * | | | fd64f04f fix long prompt than ctx proposed in #3639
| | | | | * | | | b727e022 fix ci make build undefined ref errors
| | | | | * | | | ce961a30 some ci fixes
| | | | | * | | |   9035978a Merge pull request #6 from damian0815/fssrepo_mac_fixes
| | | | | |\ \ \ \  
| | | | | | * | | | 299f6b54 fix compilation errors with llvm
| | | | | * | | | |   f47fd17b Merge branch 'ggerganov:master' into master
| | | | | |\ \ \ \ \  
| | | | | * | | | | | 4e5c5c45 notify the user from server ui that multimodality is unavialable
| | | | | | |/ / / /  
| | | | | |/| | | |   
| | | | | * | | | | 7e64bfe0 refactor code + remove unused comments + improved README.md
| | | | | * | | | | 9f72b446 add multimodal input - alfa
| | | | | * | | | | de35b479 fixed tokens probs
| | | | | * | | | | 9d98cdda llava multimodal integration
| | | | | * | | | | eb082012 add changes to README.md
| | | | | * | | | | a2c2d98c add context swap
| | | | | * | | | | b6d9e212 fixed timings per slot
| | | | | * | | | | a410a9e3 unused change reverted
| | | | | * | | | | 6358ae5f server ui now support multiple clients
| | | | | * | | | | 4ba5a501 chat.mjs support cached prompt + some fixes
| | | | | * | | | | 500ac712 cached prompt support
| | | | | * | | | | 83c2b355 grammar + no stream completion
| | | | | * | | | | 5b8e29de multiple client support
| | | | | * | | | | 81484805 completion endpoint working
| | | | | * | | | | 29c8cdd6 refactored sampling function
| | | | | * | | | |   b716eeb7 Merge branch 'master' of https://github.com/ggerganov/llama.cpp
| | | | | |\ \ \ \ \  
| | | | | * | | | | | 78504218 save dev progress
| | | | | * | | | | | 47123020 crash fixed
| | | | | * | | | | | 63f99b1e implementing parallel decoding in server example
| | | | | | | | | | | * cb79f8a2 llama : add SKIP_KQ_KQV option
| | | | | | | | | | | * ed9fde7a ggml : skip nops
| | | | | | | | | | | * 2471d56a llama : profiling the attention compute
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 22c69a27 batched : add len CLI argument
* | | | | | | | | | | 465219b9 CLBlast: Add outer loops over src0 for broadcasting in mulmat
* | | | | | | | | | | d1031cf4 sampling : refactor init to use llama_sampling_params (#3696)
| | | | | | | | | | | * 56ba00b9 sampling : hide prev behind API and apply #3661
| | | | | | | | | | | * 7e2b5fb1 sampling : add llama_sampling_print helper
| | | | | | | | | | | * b5265615 sampling : rename penalty params + reduce size of "prev" vector
| | | | | | | | | | | * 84ed48b4 examples : remove embd-input and gptneox-wip
| | | | | | | | | | | * 6e658765 llama : combine repetition, frequency and presence penalties in 1 call
| | | | | | | | | | | * cd1e9378 sampling : refactor init to use llama_sampling_params
| |_|_|_|_|_|_|_|_|_|/  
|/| | | | | | | | | |   
* | | | | | | | | | | 8cf19d60 gguf : support big endian platform (#3552)
* | | | | | | | | | | a0edf73b server : fix uninitialized sampling context (close #3685)
* | | | | | | | | | | f439e506 ggml : fix rope + llama minor optimizations (#3560)
* | | | | | | | | | | e78f3ef2 convert : restore compat with old Falcon models (#3680)
* | | | | | | | | | | f3b25e40 multimodal : add BakLLaVA conversion support (#3682)
* | | | | | | | | | | 60abea97 llava : avoid segfault in case of non-existent mmproj file (#3674)
| |_|_|_|_|_|_|_|_|/  
|/| | | | | | | | |   
* | | | | | | | | | 004797f6 readme : update hot topics
* | | | | | | | | | 4e82b2ea speculative : bug fixes
* | | | | | | | | | 0e89203b speculative : add tree-based sampling example (#3624)
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | c67fe68e metal : implement q5_0 and q5_1 kernels (#3648)
* | | | | | | | | 1117d066 opencl : fix element-wise multiplication (#3656)
| |_|_|_|_|_|_|/  
|/| | | | | | |   
| | | | | | | | *   ad2727d0 Merge branch 'master' into speculative-tree
| | | | | | | | |\  
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | cb33f43a fix embeddings when using CUDA (#3657)
* | | | | | | | | e1675d13 llama : avoid fprintf in favor of LLAMA_LOG (#3538)
* | | | | | | | | 8402566a readme : update hot-topics & models, detail windows release in usage (#3615)
* | | | | | | | | 40e5ce05 CLBlast: Fix temporary buffer size for f16 conversion (wsize)
* | | | | | | | | a5e8c1d8 train-text-from-scratch : fix assert failure in ggml-alloc (#3618)
* | | | | | | | | e74c705e editorconfig : remove trailing spaces
* | | | | | | | | 3ad1e3f1 server : documentation of JSON return value of /completion endpoint (#3632)
| | | | | | | | *   bd9451ca Merge branch 'master' into speculative-tree
| | | | | | | | |\  
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 1142013d save-load-state : fix example + add ci test (#3655)
* | | | | | | | | 5fe268a4 readme : add Aquila2 links (#3610)
* | | | | | | | | 1a159553 tokenizer : special token handling (#3538)
| | | | | | | | *   010c52ec Merge branch 'master' into speculative-tree
| | | | | | | | |\  
| |_|_|_|_|_|_|_|/  
|/| | | | | | | |   
* | | | | | | | | 281ef73c k-quants : fix quantization ranges (#3646)
* | | | | | | | | 940efa95 llava : fix tokenization to not add bos between image embeddings and user prompt (#3645)
| |_|_|_|_|_|/ /  
|/| | | | | | |   
* | | | | | | | 11bff290 MPT : support GQA for replit-code-v1.5 (#3627)
* | | | | | | | 11dc1091 Honor -ngl option for Cuda offloading in llava (#3621)
| | | | | | | * e6dd81f0 speculative : fix the n_drafted fix + p constants
| | | | | | | * f07cd35d speculative : fix off-by-one for n_drafted
| | | | | | | * 373d782d minor : comments + rename
| | | | | | | * 1c626e2f speculative : minor refactor
| | | | | | | * 360a3331 common : add llama_batch_add() and llama_batch_clear() helpers
| | | | | | | * 00594910 prompts : add assistant.txt
| | | | | | | * 5b34bfa2 swift : try to fix build
| | | | | | | * b8acb6c9 swift : fix build
| | | | | | | * b5554b9e sampling : fix malloc
| | | | | | | * 0d96efab batched : fix n_seq_id
| | | | | | | * 7e48e21b examples : fix build after sampling refactoring
| | | | | | | * 4a7f43f2 speculative : refactor sampling
| | | | | | | * 32a67cbd speculative : reuse the n_parallel CLI param
| | | | | | | * 4de5a2d4 speculative : add tree-based sampling support
| | | | | | | * 5261aee8 sampling : one sequence per sampling context
| | | | | | |/  
| | | | | | | * 932589c0 Honor -ngl option for Cuda offloading in llava
| |_|_|_|_|_|/  
|/| | | | | |   
* | | | | | | 2a4bcbac llama : remove n_threads from llama_decode_internal (#3614)
* | | | | | | 424b6381 ggml : add context enumeration functions (#3605)
* | | | | | | 1e0e873c CLBlast: Fix matrix-vector multiplication (#3544)
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 370359e5 examples: support LLaVA v1.5 (multimodal model) (#3436)
* | | | | | 9e24cc6e docs : fix typo GOMP_CPU_AFFINITY (#3597)
* | | | | | d28e572c cmake : fix add_compile_options on macOS
* | | | | | f3040bea typo : it is `--n-gpu-layers` not `--gpu-layers` (#3592)
* | | | | | 1a8c8795 ci : check if there is enough VRAM (#3596)
* | | | | | b016596d server : add completion mode (no chat) (#3582)
* | | | | | 6b3ae4da prompts : add mnemonics.txt
* | | | | | 57dd55e2 server : fix kv cache management (#3588)
* | | | | | b8fe4b5c main : fix session loading bug (#3400)
* | | | | | a8bdd655 server : add parameter -tb N, --threads-batch N (#3584)
* | | | | | 70c29da1 common : fix mirostat state when using multiple sequences (#3543)
* | | | | | 8c70a5ff batched : add bench tool (#3545)
| |_|_|_|/  
|/| | | |   
| | | | | * 2fcdf869 batched-bench : add mmq CLI arg
| | | | | * daeb834d batched-bench : pass custom set of PP, TG and PL
| | | | | * c062ffd1 batched-bench : init warm-up batch
| | | | | *   76e17f8d Merge branch 'master' into batched-bench
| | | | | |\  
| |_|_|_|_|/  
|/| | | | |   
* | | | | | 24ba3d82 examples : add batched.swift + improve CI for swift (#3562)
* | | | | | 9f6ede19 Add MPT model to supported models in README.md (#3574)
* | | | | | 233fc1c6 Minor improvements in GPT2 tokenizer (#3567)
* | | | | | c5b49360 readme : add bloom (#3570)
* | | | | | 02d2875d llm : add bloom models (#3553)
* | | | | | 0aa6595a swift : improvements and fixes (#3564)
* | | | | | f5f9121d llm : add MPT support (#3417)
* | | | | | 11ea5c7d infill. : fix tokenization (#3508)
* | | | | | 95bd60a0 ggml-alloc : fix assert in debug builds (#3555)
| |_|/ / /  
|/| | | |   
| | | | * 026bb1b1 batched-bench : add readme + n_kv_max is now configurable
| | | | * 7438728d batched : minor fix table
| | | | * bf06d654 batched : add bench tool
| | | | | * ee745692 ggml-alloc : fix assert in debug builds
| |_|_|_|/  
|/| | | |   
* | | | | fcca0a70 refact : fix convert script + zero out KV cache to avoid nans (#3523)
* | | | | dcc09d25 metal : do not use mul_mm kernels when ne00 < 64 (#3542)
* | | | | db3abcc1 sync : ggml (ggml-backend) (#3548)
* | | | | eee42c67 ci : add Zig CI/CD and fix build (#2996)
| |_|_|/  
|/| | |   
* | | | 8e6716a1 api_like_OAI.py : compat with Microsoft Guidance (#2746)
* | | | 9c38d181 api_like_OAI.py : simplify function (#2796)
* | | | a1202a31 k-quants : fix comments about block sizing (#3499)
| | | | * ee268b54 llama : no longer perform uninitialized access to the KV cache
| | | | *   acead654 Merge branch 'master' into fix-refact
| | | | |\  
| |_|_|_|/  
|/| | | |   
* | | | | 94e502df ci : enable on obj-c changes + fix metal build (#3540)
* | | | | 7d8b2493 zig : fix build by introducing train.cpp (#3539)
* | | | | b0ec5218 metal : support MTLGPUFamily < Apple7, formatting, style (#3524)
* | | | | 63d3b06a llama : fix missing break in Persimmon arch case statements (#3535)
* | | | | a16e89ce Fix trying to strip newline from empty prompt and cfg prompt file content (#3534)
* | | | | 4d038332 gguf.py : fix CI for publishing GGUF package (#3532)
| | | | * 0f8df395 metal : assert various kernel requirements
| | | | * 42833bc7 ggml : silu(-inf) should never happen
| | | | * bdbe1171 refact : fix convert script + zero out KV cache to avoid nans
| | | | | * 6b9554a7 metal : print more GPU info + disable mul_mm for MTLGPUFamiliy < Apple7
| | | | | * 545b0349 minor
| | | | | * 8f6ad684 metal : indentations
| | | | | * c6002248 metal : rename kernels mul_mat_ to mul_mv_
| | | | | * 99ed03a2 metal : improve decoding speed for batches of 2-16
| | | | | | * ba44776d bump version
| | | | | | * 5ad84f0b bump version
| | | | | | * 6dd3e8ea bump version
| | | | | | * 0e1010b6 fix
| | | | | | * 9ccbb277 Bump version
| | | | | | * 68017ef4 Fix CI for publishing GGUF package
| |_|_|_|_|/  
|/| | | | |   
* | | | | | c47066d8 py : change version of numpy requirement to 1.24.4 (#3515)
| |_|_|_|/  
|/| | | |   
* | | | | f1782c68 quantize : fail fast on write errors (#3521)
* | | | | c26765a0 metal : support default.metallib load & reuse code for swift package (#3522)
| |_|_|/  
|/| | |   
* | | | 0e797c2f llm : support Adept Persimmon 8B (#3410)
* | | | 3a716b4d Fix for #3454 (#3455)
* | | | 1faaae8c readme : update models, cuda + ppl instructions (#3510)
* | | | cb13d73a server : docs fix default values and add n_probs (#3506)
* | | | 9ca79d5c kv cache slot search improvements (#3493)
* | | | 0c731ca4 prompts : fix editorconfig checks after #3416
* | | | a8777ad8 parallel : add option to load external prompt file (#3416)
* | | | 97af49fa server : reuse llama_sample_token common util (#3494)
* | | | 16820a5a llama : correct hparams comparison (#3446)
* | | | 04b2f438 ci : fix xcodebuild destinations (#3491)
| | | | * 5ab6c213 server-parallel : add "--reverse-prompt" + compiler warning fixes
| | | | * afc09db5 fix json format README
| | | | * eb75395b remove trail whitespace
| | | | * a7a6ceb7 server handling multiple clients with cam
| |_|_|/  
|/| | |   
* | | | 48edda30 convert : update Falcon script for new HF config (#3448)
* | | | 45eba936 build : use std::make_tuple() for compatibility with older GCC versions (#3488)
* | | | acec9eaa common : process escape sequences in reverse prompts (#3461)
* | | | e2583cbc CLBlast: Fix handling of on-device tensor data
* | | | e8b8d32e server : fix incorrect num_tokens_predicted (#3480)
* | | | 8f3a642e swift : disable ACCELERATE_NEW_LAPACK (#3481)
* | | | 07453844 ci : add swift build via xcodebuild (#3482)
* | | | 019ba1dc convert : fix Baichuan2 models by using vocab size in config.json (#3299)
* | | | beabc8cf readme : add project status link
* | | | 0d152b37 ggml : fix build after #3329
* | | | f8c90cdb llm : add Refact model (#3329)
* | | | f93af024 sync : ggml (conv 1d + 2d updates, UB fixes) (#3468)
* | | | f72f8f22 finetune : readme fix typo (#3465)
* | | | 79f34abd ggml : add RISC-V Vector Support for K-Quants and improved the existing intrinsics (#3453)
* | | | 8186242b main : consistent prefix/suffix coloring (#3425)
* | | | ac2219fe llama : fix session saving/loading (#3400)
* | | | 48be797f llama : expose model's rope_freq_scale in the API (#3418)
* | | | f56e1bae metal : alibi for arbitrary number of heads (#3426)
* | | | 017efe89 cmake : make LLAMA_NATIVE flag actually use the instructions supported by the processor (#3273)
| |_|/  
|/| |   
* | | ff5a3f0c Work on the BPE tokenizer (#3252)
* | | 1c84003c convert : fix vocab size when not defined in hparams (#3421)
* | | e78f0b0d cmake : increase minimum version for add_link_options (#3444)
* | | 665018c7 CLBlast: Add broadcast support for matrix multiplication (#3402)
* | | 29a404a9 gguf : add BERT, MPT, and GPT-J arch info (#3408)
* | | 0fe32103 gguf : general usability improvements (#3409)
| | | * 5418932b llama : fix comments for llama_kv_cache API
| | | * 337120cc llama : fix handling of "future" tokens when loading sessions
| | | * 0f332a91 llama : temp fix for clearing "future" tokens from the KV cache
| | | *   6a9fe3df Merge branch 'master' into fix-sessions
| | | |\  
| |_|_|/  
|/| | |   
* | | | 9476b012 cmake : make CUDA flags more similar to the Makefile (#3420)
* | | | a03ce384 finetune : fix #3404 (#3437)
* | | | a8476769 metal : set log callback before initializing (#3427)
* | | | 095231df cmake : fix transient definitions in find pkg (#3411)
* | | | ea55295a docker : ignore Git files (#3314)
* | | | c97f01c3 infill : add new example + extend server API (#3296)
* | | | f5ef5cfb ggml-cuda : perform cublas mat mul of quantized types as f16 (#3412)
* | | | 40e07a60 llama.cpp : add documentation about rope_freq_base and scale values (#3401)
* | | | bc34dd4f train : fix KQ_pos allocation (#3392)
* | | | 2777a84b llama : quantize up to 31% faster on Linux and Windows with mmap (#3206)
* | | | 0a4a4a09 readme : update hot topics + model links (#3399)
* | | | 569550df readme : add link to grammars app (#3388)
| | | * b0670db3 llama : fix session saving/loading
| |_|/  
|/| |   
* | | c71bf2c4 swift : fix build on xcode 15 (#3387)
* | | bc39553c build : enable more non-default compiler warnings (#3200)
* | | 0ccfc62a ggml_tensor: update the structure comments. (#3283)
* | | 7f1a0fe7 ggml : release the requested thread pool resource (#3292)
* | | 16bc66d9 llama.cpp : split llama_context_params into model and context params (#3301)
* | | 0512d666 ci : multithreaded builds (#3311)
* | | 0e76a899 train : finetune LORA (#2632)
* | | 2db94d98 gguf : basic type checking in gguf_get_* (#3346)
* | | ecf90b1a gguf : make token scores and types optional (#3347)
* | | 2619109a ci : disable freeBSD builds due to lack of VMs (#3381)
| |/  
|/|   
* | ec893798 llama : custom attention mask + parallel decoding + no context swaps (#3228)
|/  
* 45855b3f docs : mark code as Bash (#3375)
| * c5650ed4 server : avoid context swaps by shifting the KV cache
| * ce2d995a server : clear the KV cache beyond n_past before llama_decode
| * 2b8830af examples : do not eval prompt 2 times (close #3348)
| * a2075615 examples : add example for batched decoding
| * d008733e examples : utilize new llama_get_logits_ith()
| * 4c72ab13 metal : use mm kernels for batch size > 2
| * e9463792 llama : simplify returns if/else branches
| * 4ad06769 parallel : fix crash when `-n -1`
| *   25856900 Merge branch 'master' into custom-attention-mask
| |\  
| |/  
|/|   
* | 4aea3b84 readme : add Mistral AI release 0.1 (#3362)
* | da040034 ggml-cuda : perform cublas fp16 matrix multiplication as fp16 (#3370)
* | e5196210 convert : remove bug in convert.py permute function (#3364)
* | ac435761 make-ggml.py : compatibility with more models and GGUF (#3290)
* | 20c7e1e8 gguf : fix a few general keys (#3341)
* | dc689740 metal : reusing llama.cpp logging (#3152)
* | 527e57cf build : add ACCELERATE_NEW_LAPACK to fix warning on macOS Sonoma (#3342)
* | ffe88a36 readme : add some recent perplexity and bpw measurements to READMES, link for k-quants (#3340)
* | 99115f3f cmake : fix build-info.h on MSVC (#3309)
* | 1726f962 docs: Fix typo CLBlast_DIR var. (#3330)
* | a98b1633 nix : add cuda, use a symlinked toolkit for cmake (#3202)
* | c091cdfb llama-bench : add README (#3317)
* | 51a7cf5c examples : fix RoPE defaults to match PR #3240 (#3315)
* | bedb92b6 scripts : use `/usr/bin/env` in shebang (#3313)
* | bc9d3e39 Update README.md (#3289)
* | 36b904e2 ggml-opencl.cpp: Make private functions static (#3300)
* | 324f3403 zig : fix for updated c lib (#3259)
* | f56c418a embedding : update README.md (#3224)
* | 8185710a CUDA: use only 1 thread if fully offloaded (#2915)
* | 7eb41179 readme : update hot topics
* | a5661d7e llama : allow gguf RoPE keys to be overridden with defaults (#3240)
* | 65c2c1c5 benchmark-matmult : do not use integer abs() on a float (#3277)
* | 80834dae flake : Restore default package's buildInputs (#3262)
* | a40f2b65 CI: FreeBSD fix (#3258)
* | d119c04c examples : fix benchmark-matmult (#1554)
* | 8781013e make : restore build-info.h dependency for several targets (#3205)
| * c1596f63 llama : fix kv cache heuristic when context is less than 32
| | * 72e7ef4e simple : fixes
| |/  
| * 88451600 simple : add README.md
| * 5a3369d8 llama : llama.h formatting + comments
| * b2debf65 parallel : add disabled experimental batch chunking in powers of two
| * ded9b43c parallel : fix cases where the input prompts can overflow the batch
| * ee1d670c parallel : fix bug (extra BOS) + smaller token_prev array
| * 1be2b8c1 ggml : revert change to ggml_cpy, add ggml_cont_Nd instead (#3275)
| * 2f3a46fc train : make KQ_pos memory buffer permanent via dummy scale op
| * 54206962 llama : disable MPI for now
| * e04dc519 ggml-cuda : add rope f16, restore performance with parallel decoding (#3272)
| * db0fc2da simple : improve comments + free batch
| * b377bf22 simple : add parallel decoding support
| * addae65f llama : improve llama_batch API + simplify parallel example
| * a1327c71 parallel : rename hot-plug to continuous-batching
| * e1067efb llama : fix n_kv to never become 0
| * 7b7472ee parallel : minor
| * 6028879f parallel : print misses on each request
| * eed3fd42 parallel : count cache misses
| * 8a9aca37 parallel : remove question with short answers
| * 4b5f3cd6 parallel : process system prompt once + configurable paramters + llama API
| * 82e20e9b parallel : remove new line from prompt
| * d37081ae llama : silence errors KV cache errors
| * 16090a5d parallel : fix sequence termination criteria
| * 806d397c parallel : try smaller batches when the KV cache is fragmented
| * ddad2277 llama : fix cell_max logic + rename functions
| * 36714e16 parallel : various improvements
| * 467e3079 simple : fix token counting
| * 25bd2540 make : add parallel to build + fix static functions in llama.cpp
| * 7e2b9974 ggml-cuda : update rope implementation for parallel decoding (#3254)
| * daf4c6d3 llama : fix worst case graph build
| * fa0e6778 llama : extend batch API to select which logits to output
| * 897caccd fixes : speculative KV cache + llama worst-case graph
| * 466b5138 parallel : disable hot-plug to avoid cache fragmentation
| * 0161372b parallel : example for serving multiple users in parallel
| * 1f17ea63 speculative : fix KV cache management
| * 7c1bdd0e llama : apply K-cache roping for Falcon and Baichuan
| * 0cbf3bfe llama : add llama_kv_cache_shift_seq + no more context swaps
| * 86c90e34 metal : disable concurrency optimization
| * f015b266 llama : more robust cell_max heuristic + wip shift
| * 4d76d762 llama : extend llama_kv_cache API
| * 6952a460 llama : add cell_max heuristic for more efficient kv_cache
| * 9f42e754 llama : add new llama_decode() API that works with llama_batch
| *   58bb5110 Merge branch 'master' into custom-attention-mask
| |\  
| |/  
|/|   
* | 7ddf1855 ci : switch cudatoolkit install on windows to networked (#3236)
* | ee66942d CUDA: fix peer access logic (#3231)
* | 111163e2 CUDA: enable peer access between devices (#2470)
* | 8b428c9b llama.cpp : show model size and BPW on load (#3223)
* | 578d8c8f CUDA: fix scratch malloced on non-main device (#3220)
* | b541b4f0 Enable BUILD_SHARED_LIBS=ON on all Windows builds (#3215)
* | 5dbc2b32 Enable build with CUDA 11.0 (make) (#3132)
* | b08e75ba Fixing the last deviations from sentencepiece indicated by test-tokenizer-1 (#3170)
* | e6616cf0 examples : add compiler version and target to build info (#2998)
* | 3aefaab9 check C++ code with -Wmissing-declarations (#3184)
* | 69eb67e2 fix build numbers by setting fetch-depth=0 (#3197)
* | 4fe09dfe llama : add support for StarCoder model architectures (#3187)
| * d29e7693 llama : unified KV cache + batch inference API
| | * 99c5c9a0 Upload immediately to device.
| | * 0631ea36 Don't crash on available devices if we can't even create an instance.
| | * 703ef9c1 Set the singleton to nullptr here.
| | * 7ff671e1 Only use vulkan with known quant that work.
| | * 8616ce08 Sync from device back to host at begin of new prompt.
| | * 80da9b89 Don't try and install kompute artifacts.
| | * e5ab32aa vulkan: disambiguate gpus with the same name
| | * 2f7732b6 Throw an exception when allocation fails for vulkan.
| | * 9bee309a Make kompute actually include external SDK headers when requested
| | * 0412ec28 Completely revamp how we do object management with the vulkan backend and stop using so many static objects so we can tear down and bring up vulkan on new devices in the same runtime.
| | * 5b2d8236 Switch to a dynamic dispatch table instead of linking hard against libvulkan.
| | * e308fb04 remove dynamic deps from kompute build
| | * ced23198 Remove warning which fails on windows.
| | * 4cdaa3c9 Nomic vulkan backend licensed under the Software for Open Models License (SOM), version 1.0.
| | | * 784d14ed llama : store non-RoPEd K cache (WIP)
| | |/  
| |/|   
| * | fad56936 metal : add rope_f16 kernel + optimize cpy kernels
| * | 1fb033fd ggml : ggml_rope now takes a vector with positions instead of n_past
| * | 3b4bab6a llama : replace ggml_diag_mask_inf with ggml_add (custom -inf mask)
| * | c5df72e8 tests : verify that RoPE is "additive"
|/ /  
* | 80291a1d common : do not use GNU zero-length __VA_ARGS__ extension (#3195)
* | c6f1491d metal : fix bug in soft_max kernels (out-of-bounds access) (#3194)
* | e3d87a6c convert : make ftype optional in simple scripts (#3185)
* | 8c00b7a6 sync : ggml (Metal F32 support + reduce ggml-alloc size) (#3192)
* | 7e50d34b cmake : fix building shared libs for clang (rocm) on windows (#3176)
* | 235f7c19 flake : use pkg-config instead of pkgconfig (#3188)
* | a51b6876 metal : relax conditions on fast matrix multiplication kernel (#3168)
* | 76164fe2 cmake : fix llama.h location when built outside of root directory (#3179)
* | c2ab6fe6 ci : Cloud-V for RISC-V builds (#3160)
* | 2d770505 llama : remove mtest (#3177)
| | * 92a4f868 llama : make starcoder graph build more consistent with others
| | * f82328ab metal : fix out-of-bounds access in soft_max kernels
| | * 6c353dc7 cleanup useless code
| | * a1cf66ea working in cpu, metal buggy
| | * 101c5787 add TBD
| | * 8bc76a22 add input embeddings handling
| | * ab13d071 store mqa directly
| | * 4420cff6 fix vram calculation for starcoder
| | * dac31da4 fix comments
| | * 0be15e16 fix head count kv
| | * 77c7ec17 properly load all starcoder params
| | * 26836119 set n_positions to max_positioin_embeddings
| | * a17ef397 add max_position_embeddings
| | * 57f064d7 load starcoder weight
| | * 166a259f set head_count_kv = 1
| | * 7298c37e add LLM_ARCH_STARCODER to llama.cpp
| | * 7e0a843b fix ffn_down name
| | * 76d32cca convert MQA to MHA
| | * eb7f0eba support convert starcoder weights to gguf
| | * 0c5d4d87 add placeholder of starcoder in gguf / llama.cpp
| |/  
|/|   
* | 98311c42 llama : make quantize example up to 2.7x faster (#3115)
* | feea179e flake : allow $out/include to already exist (#3175)
* | 769266a5 cmake : compile ggml-rocm with -fpic when building shared library (#3158)
* | cf8238e7 flake : include llama.h in nix output (#3159)
* | 4b8560e7 make : fix clang++ detection, move some definitions to CPPFLAGS (#3155)
* | 83a53b75 CI: add FreeBSD & simplify CUDA windows (#3053)
* | 5c872dbc falcon : use stated vocab size (#2914)
* | 990a5e22 cmake : add relocatable Llama package (#2960)
* | 980ab41a docker : add gpu image CI builds (#3103)
* | e3940841 gguf-py : support identity operation in TensorNameMap (#3095)
* | 4c8643dd feature : support Baichuan serial models (#3009)
* | 35f73049 speculative : add heuristic algorithm (#3006)
| | * e7e7b114 llama : remove experimental stuff
| | * e343b8b4 metal : revert the concurrnecy change because it was wrong
| | * 336afbcb metal : relax conditions on fast matrix multiplication kernel
| |/  
|/|   
* | 71ca2fad whisper : tokenizer fix + re-enable tokenizer test for LLaMa (#3096)
* | 1b6c650d cmake : add a compiler flag check for FP16 format (#3086)
* | 0a5eebb4 CUDA: mul_mat_q RDNA2 tunings (#2910)
* | 84e72365 speculative: add --n-gpu-layers-draft option (#3063)
* | b52b29ab arm64 support for windows (#3007)
* | 4f7cd6ba CUDA: fix LoRAs (#3130)
* | 89e89599 CUDA: fix mul_mat_q not used for output tensor (#3127)
* | d54a4027 CUDA: lower GPU latency + fix Windows performance (#3110)
* | 1b0d0925 cmake : support build for iOS/tvOS (#3116)
* | 8a4ca9af CUDA: add device number to error messages (#3112)
* | f31b6f4e metal : PP speedup (#3084)
* | 6eeb4d90 convert: remove most of the n_mult usage in convert.py (#3098)
* | 21ac3a15 metal : support for Swift (#3078)
* | 4fd54779 metal : support build for iOS/tvOS (#3089)
* | ec2a24fe flake : add train-text-from-scratch to flake.nix (#3042)
* | 7d99aca7 readme : fix typo (#3043)
* | ba7ffbb2 metal : Q3_K speedup (#2995)
* | e64f5b55 examples : make n_ctx warning work again (#3066)
* | 94f10b91 readme : update hot tpoics
* | b3e9852e sync : ggml (CUDA GLM RoPE + POSIX) (#3082)
* | cb6c44c5 build : do not use _GNU_SOURCE gratuitously (#2035)
* | a21baeb1 docker : add git to full-cuda.Dockerfile main-cuda.Dockerfile (#3044)
* | 6ff712a6 Update deprecated GGML TheBloke links to GGUF (#3079)
* | ebc96086 ggml-alloc : correctly check mmap return value for errors (#3075)
* | 7f412dab enable CPU HBM (#2603)
* | 6336d834 convert : fix F32 ftype not being saved (#3048)
* | 00d62adb fix some warnings from gcc and clang-tidy (#3038)
* | 4fa2cc17 make : improve test target (#3031)
* | 5ffab089 make : fix CPPFLAGS (#3035)
* | 15b67a66 llama-bench : use two tokens in the warmup run for prompt evals (#3059)
* | be8c9c24 metal : parallel RoPE on Metal (#3024)
* | be6beeb8 metal : correct fix of kernel_norm (#3060)
* | c4f49664 metal : fix kernel_norm (fixes Falcon on Metal) (#3057)
| | * 2f689dee metal : minor
| | * efac2d46 common : don't do warm-up with more than n_batch tokens (close #3058)
| | * 78337967 metal : restore original F16 mat-vec multiplication
| | * ed92c3d4 metal : put warning in kernel_norm to not combine the loops
| | * 5e1c4089 metal : fix kernel_norm
| |/  
|/|   
* | fec2fb19 ggml : posixify madvise and pagesize (#3037)
* | 178b1850 k-quants : fix zero-weight guard in Q6_K (ref #3040)
* | ea2c85d5 convert-llama-ggml-to-gguf: Try to handle files older than GGJTv3 (#3023)
* | 9912b9ef build : add LLAMA_METAL_NDEBUG flag (#3033)
* | 9e202315 make : use new flag variables for recent changes (#3019)
* | de2fe892 examples : replace fprintf to stdout with printf (#3017)
* | c9c3220c convert: fix convert.py not working with int filename_stem (#3028)
* | d59bd970 Guard against all weights in a super-block being zero (#3010)
* | 35938ee3 llama : update logic for number of threads when using BLAS
* | 92177210 speculative : add grammar support (#2991)
* | 2ba85c86 py : minor
* | e36ecdcc build : on Mac OS enable Metal by default (#2901)
| | * 30ac7a41 gitignore : metal
| | * 28eea84a make : fix merge conflict remnants
| | *   65520729 Merge branch 'master' into build-metal-default
| | |\  
| |_|/  
|/| |   
| | * ac4038aa readme : update Metal instructions
| | * 23360b15 common : better `n_gpu_layers` assignment
| | * 323a9d3b llama : fix vocab_only logic when GPU is enabled
| | * 99161230 llama : enable GPU inference by default with Metal
| | * 15f1790a make : fix target clean
| | * b59beebd make : move targets back to the top
| | *   4de22829 Merge branch 'master' into build-metal-default
| | |\  
| | * | bcf62ba7 make : try to fix build on Linux
| | * | e966ae05 build : on Mac OS enable Metal by default
| | | | * f3a84b2e llama : better express the KV cache dependencies in the graph
| | | | * 60c2ef6d metal : utilize view_src to see of tensor is a view
| | | | * ebd3467c metal : more readable kernel
| | | | * 7704db25 ggml : just in case
| | | | * ad80e5a4 llama : add ggml_cont to trigger bug with Metal
| |_|_|/  
|/| | |   
* | | | bd33e5ab ggml-opencl : store GPU buffer in ggml_tensor::extra (#2994)
| | | | * c79d130f make : fix speculative build
| | | | * 2db2471c speculative : avoid grammar_mem
| | | | * e7dc5b08 speculative : reuse grammar parser + better logs and comments
| | | | * 6c150d76 speculative : print draft token pieces
| | | | * ebe41d49 common : warm-up with 2 tokens - seems to work better
| | | | * 01345788 grammar : remove one nested level
| | | | * 2d89da4f grammar : add comments to new grammar file
| | | | * e0a8658e grammars : add json_arr.gbnf
| | | | * 69f2fafe speculative : add grammar support
| |_|_|/  
|/| | |   
* | | | 31035681 llama-bench : make cpp file non-executable (#2999)
* | | | 5b8530d8 make : add speculative example (#3003)
* | | | e4386f41 server : add a subtle loading animation to the edit box (#2466)
* | | | 35195689 2x faster (rms) norm cuda kernels (3.7% e2e improvement) (#2985)
* | | | cf9b0848 ggml-alloc : use virtual memory for measurement (#2973)
* | | | 47068e51 speculative : PoC for speeding-up inference via speculative sampling (#2926)
| | | | * 847896ab speculative : add --draft CLI arg
| | | | * a15ca746 speculative : print encoding speed
| | | | * c82c808d speculative : initial example
| |_|_|/  
|/| | |   
* | | | 8f429fa5 perplexity : fix ETA by warming up the model with an empty run
* | | | 6519e9c9 gguf(python): Fix special vocab handling when id < 0 (#2984)
* | | | b7f2aa9e metal : restore 363f0bf and fix reduce in F16_F32 kernels (#2986)
* | | | 73a12a63 cov : disable comment in PRs (#2989)
* | | | 37301347 llama : fix bpe tokenize from byte (#2889)
* | | | d9151e6f metal : revert 6af0bab until we fix it
* | | | afc43d5f cov : add Code Coverage and codecov.io integration (#2928)
* | | | 6460f758 opencl : fix a bug in ggml_cl_pool_malloc() for ggml_cl_mul_mat_f32() (#2955)
* | | | ca82cf7b metal : more optimizations (#2959)
| |_|/  
|/| |   
* | | 6a31a3bd swift : add support for k-quants (#2983)
* | | cff7b0bf convert.py : BPE fixes (#2938)
* | | 340af42f docs : add `catai` to `README.md` (#2967)
* | | c42f0ec6 examples : fix gpt-neox (#2943)
* | | 2753415a swift : add missing c file to Package.swift (#2978)
* | | bc054af9 make : support overriding CFLAGS/CXXFLAGS/CPPFLAGS/LDFLAGS (#2886)
* | | 3358c381 logging: Fix creating empty file even when disabled (#2966)
* | | 52315a42 readme : update clblast instructions (#2903)
* | | 8b56b4f2 metal : show all Metal device instances in the system (#2952)
* | | 21f3d1be k-quants : fix build on armv7 (android only) (#2920)
* | | 571083f5 server : avoid aniprompt in probabilities of final response (#2849)
* | | f04d0028 cuda : vsubss4 for older versions of ROCm/clang (#2942)
| |/  
|/|   
* | 69fdbb9a readme : quick start command fix (#2908)
* | 5d6f19f1 Allow quantize to only copy tensors, some other improvements (#2931)
* | 0d589366 llama2c : rename function
* | 6c9c2342 make : use unaligned vector moves on MinGW (#2945)
* | ee8654bc minor : add const qualifiers (#2853)
* | 49bb9cbe docs : add java-llama.cpp to README.md (#2935)
* | ef156499 build : fix most gcc and clang warnings (#2861)
* | d8d6977f examples : add C grammar (#2357)
* | 5aec2cfa ggml : add RISC-V vector intrinsics support (#2929)
* | 13268c53 metal : slight speed-up for add and mul kernels (#2917)
* | 4dcd47d7 logs : fix mingw-like builds (fixes #2898) (#2911)
* | 18705a30 llama2c : fix segfault and alloc-dealloc-mismatch (#2913)
* | e8d91589 metal: somewhat faster f16 x f32 matrix multiply kernel (#2951)
* | bce1fef3 convert : fix another python 3.8 issue (#2949)
* | 528134dd remove convert-llama-7b-pth-to-gguf.py and convert-llama-hf-to-gguf.py (#2906)
* | aeefac4f scripts: Use local gguf package when running from repo (#2927)
* | e8422de3 @vxiiduu's fix for PrefetchVirtualMemory (#2930)
* | 92d0b751 convert : fix python 3.8 support, modernize type annotations (#2916)
* | 8afe2280 CUDA: mul_mat_q=true llama_context_params default (#2912)
* | 71d69755 [Docker] fix tools.sh argument passing. (#2884)
| | * 8c2b8812 cuda : poc for norm quants (only -b 1 works)
| | * df54d2f1 ggml : use less ggml_mul tasks when src0 rows are few
| | * 253eab8a ggml : poc for normalizing weights for better quantization (metal)
| |/  
|/|   
| | * b4e70822 metal : add poc for normalized Q4_0 and Q4_1
| | *   9ffe54ed Merge branch 'master' into norm-quants
| | |\  
| |_|/  
|/| |   
* | | b532a69b convert.py : use dir name to name the llama
* | | c90d135e examples : fix underscore in beam-search + .gitignore (close #2900)
* | | 0d1c7061 gguf : add workflow for Pypi publishing (#2896)
* | | 95092944 make : add test and update CI (#2897)
* | | 35092fb5 docs : add `node-llama-cpp` to `README.md` (#2885)
| | * dead8f4b Fix misaligned memory access in Q4_1 kernel
| | * 72af2599 Fix misaligned memory access in Q4_1 kernel
| | * e5d23f2e ggml : fix ARM build + speed-up ggml_mul
| | * a4d1eb72 ggml : add q4_1 normalized quants
| | * 67542556 ggml : poc for normalizing weights for better quantization
| | | *   488e0320 Merge branch 'master' into gguf-publish-ci
| | | |\  
| |_|_|/  
|/| | |   
* | | | dc07dc49 convert : various script cleanups/fixes + merges and special token handling (#2842)
| | | * 3303f38f fix trailing whitespace
| | | * 4d277cb5 gguf : add workflow for Pypi publishing
| | | * b8e572f6 gguf : add workflow for Pypi publishing
| |_|/  
|/| |   
* | | ad9ddcff llm.vim : stop generation at multiple linebreaks, bind to <F2> (#2879)
* | | 8341a259 main : log file (#2748)
* | | 84940895 tests : add a C compliance test (#2848)
* | | 06abf8ee ggml : add view_src and view_offs to ggml_tensor for views (#2874)
* | | c03a243a remove outdated references to -eps and -gqa from README (#2881)
* | | fa3582f5 Tell users attmepting to run perplexity with too few tokens to use more (#2882)
* | | e37e69dc 10X faster BPE tokenizer (#2876)
* | | 53885d72 py : fix "usage" messages (#2873)
* | | bcce96ba convert.py : fix baichuan7B support (#2870)
* | | 74e0caeb readme : add react-native binding (#2869)
* | | d4b5e16c make : fix clang tests build, add missing examples (#2859)
* | | 3a007648 metal : add option to disable debug logs (close #2764)
* | | 611363ac scripts : add pipefail
* | | 95b6e521 added `struct` to llama_dump_timing_info_yaml's `llama_context` (#2857)
* | | 44c117f4 train : mem usage and other improvements  (#2439)
* | | 43033b7b llama-bench : set locale to utf8 (#2832)
* | | 6b73ef12 YAML result logging + preset script (#2657)
* | | 75fafcbc make : fix tests build (#2855)
* | | be475f60 llama.cpp : fix wrong vsnprintf call in MS compiler (#2856)
* | | 3af6b863 ggml : tiny ggml_vec_dot_q4_K_q8_K AVX2 improvement (#2819)
| | | * cec628e7 temporarily disable broken 512 build
| | | * 0e1730a9 ci: add linux binaries to release build
| |_|/  
|/| |   
* | | 35feac65 ggml : sync (mem align to header + conv_transpose_2d fixes + ggml_alloc) (#2852)
* | | 92b1bbd2 CUDA: fix RoPE asserts, block sizes (#2833)
* | | dd0dc366 llama.h : add missing struct keyword for C compat in callback type (#2847)
* | | f55538c3 metal : fix memory leak (#2762)
* | | ebcee207 quantize : make output filename optional again (#2823)
* | | 3e8ff47a devops : added systemd units and set versioning to use date. (#2835)
* | | 103cfafc gguf : fix strings to not be null-terminated (#2839)
* | | c10704d0 llama : fix MPI threads (close #2827)
* | | 230d46c7 examples : update llama2.c converter to read vocab and write models in GGUF format (#2751)
* | | 463173a6 llama : speedup tokenization (#2831)
* | | eaa13a48 falcon : fix CUDA inference by making K and Q contiguous (#2830)
* | | da7455d0 readme : fix headings
* | | 25423e91 scripts : helper convert script
* | | a6d1189f k_quants tuning for Falcon-7b (#2816)
* | | c48c5bb0 readme : update hot topics
* | | d0cee0d3 gguf : add 64-bit support (GGUF v2) (#2821)
* | | edd4c148 llama : more tokenizer fixes (#2810)
* | | 1591e2e5 ggml : detect SSSE3 (#2825)
* | | 789c8c94 ci : add LoRA test to CI (#2650)
* | | c1ac54b7 server : add `/detokenize` endpoint (#2802)
* | | 730d9c68 convert.py : advanced option (#2753)
| | | * 33a5517d llama.cpp : print gguf version
| | | * b61b1700 gguf : fix typo
| | | * 09b6da74 gguf.py : string len uint64_t and n_dims uint32_t
| | | * 6d369a15 gguf : update all counts to 64-bit
| | | * bc3eaf26 gguf.py : string lengths uint32_t
| | | * be726c57 gguf.py : uint64_t on all lengths, sizes and counts, enums still uint32_t
| | | * ba335ff5 gguf.py : bump GGUF version
| | | * 3656b3ce gguf : v1 backwards comp
| | | * 4f0547e4 gguf : add support for 64-bit (no backwards comp yet)
| | | * 5f1fffd2 gguf : bump version to 2
| |_|/  
|/| |   
* | | c7d92e6d llama : use Unicode Escape Sequence to replace encoded characters (#2814)
* | | 61d1a289 flake.nix : add rocm support and cleanup (#2808)
* | | 741ca7dd llama : move #includes out of _GNU_SOURCE conditional (#2817)
* | | 72f895c9 main : fix bug (penalize_nl=false doesn't work) + suppress warning on mingw (#1528)
* | | 50526f37 llama : use std::abs in llama_sample_tail_free (#2800)
* | | 04f4b1eb k-quants : remove unnecessary tensor shape restrictions (#2811)
* | | 75923754 Better perplexity for 2- and 3-bit quantization for LLaMA-v2-70B (#2807)
* | | 771551a7 Fix HellaSwag (#2805)
* | | f305bad1 flake : build llama.cpp on Intel with nix (#2795)
* | | a2ca4e9d Handle null rope scaling value (#2793)
* | | 2ba83c86 Fix spm whitespaces (#2806)
* | | bae5c5f6 examples : skip unnecessary external lib in server README.md how-to (#2804)
| | | * d34472c1 Fix HellaSwag
| | | | * 5562e3e6 temporarily disable broken 512 build
| | | | * 20f7f4c8 ci: add linux binaries to release build
| |_|_|/  
|/| | |   
* | | | 232caf3c llama : fix struct decl (#2790)
| |_|/  
|/| |   
* | | d046dcee Faster perplexity computation (#2786)
* | | c82742ac llama : add llama_beam_search() (#2267)
* | | 28b2c996 convert.py : Get rope scale from HuggingFace models (#2772)
* | | 154725c5 llama-bench : add model sizes (#2771)
* | | 12e2e33a convert.py : export rope freq_base when converting CodeLlama from an HF model (#2773)
* | | 29674ab4 server : display token probabilities in the UI (#2489)
* | | 5439a0ab ci : pip install gguf in editable mode (#2782)
* | | 8194cd87 gguf : export objects to user code (#2780)
* | | 6bbc598a ROCm Port (#1087)
* | | 3f460a2b cuda : add RoPE kernel for mode == 2 (NeoX) (#2760)
* | | 87e3733f gguf : make gguf pip-installable
* | | b91ad7f4 ggml-alloc : enlarge size of parse_seq (#2776)
| | | * 0248ca81 gguf : add notes for tests
| | | * 2897926d gguf : update readme with build notes
| | | * 8798aea2 gguf : update readme with build notes
| | | *   3e98cbe7 Merge branch 'master' into gguf-pip
| | | |\  
| |_|_|/  
|/| | |   
* | | | 2e5f70a2 Added `enum` to `llama_token_get_type` return type (#2774)
* | | | d0f77b13 convert.py : try to determine n_ctx automatically for CodeLlama (#2770)
* | | | 0d3094f0 gguf : add rope_freq_base parameter for CodeLlama (#2769)
* | | | 01f22246 falcon : write file type
* | | | 38b16dfc metal : bug-fix when enable ggml-alloc (#2757)
* | | | 8f8c28e8 convert : auto-determine model name based on dir + scripts update
* | | | 7694adda Fix for main example getting stuck when -n -2 and --interactive (#2767)
* | | | fea95c68 fix convert.py for codellama, add llama 34B to the list of recognized models (#2768)
* | | | ef955fbd Tag release with build number (#2732)
* | | | d67777c2 metal : add Q8_0 support (#2763)
| | | * 87338093 requirements : add gguf
| | | *   b8a777e7 Merge branch 'master' into gguf-pip
| | | |\  
| |_|_|/  
|/| | |   
* | | | c3e53b42 llama : escape all U+2581 in a string (#2750)
| | | * 0288361b gguf : fix line endings
| | | * 344f6e37 gguf: prepare as Pip package
| | | * 5dd87057 gguf: prepare as Pip package
| | | * 050046fa gitignore : add dist and rm pyproject.toml
| |_|/  
|/| |   
* | | 6e91a1b0 llama : fix grammar sometimes generating null char (#2756)
* | | 44d5462b readme : fix link
* | | c7868b07 minor : fix trailing whitespace
* | | 79da24b5 readme : update hot topics
* | | cf658adc llm : add Falcon support (#2717)
| | | *   977629a3 Merge branch 'master' into fix-eos
| | | |\  
| |_|_|/  
|/| | |   
* | | | a192860c minor : fix trailing whitespace
* | | | 95385241 examples : restore the functionality to import llama2.c models (#2685)
* | | | 335acd2f fix convert-lora-to-ggml.py (#2738)
* | | | 5290c38e main : insert bos if no tokens (#2727)
* | | | cc34dbda gitignore : fix for windows (#2729)
* | | | 7c2227a1 chmod : make scripts executable (#2675)
* | | | f19dca04 devops : RPM Specs (#2723)
* | | | 8207214b Fix values shown in the quantize tool help (#2735)
* | | | 62959e74 Strided perplexity (#2714)
* | | | 7f7ddd50 Fix ggml to gguf conversion on Windows (#2733)
* | | | b8ad1b66 server : allow json array in prompt or content for direct token input (#2306)
* | | | f5fe98d1 docs : add grammar docs (#2701)
* | | | 777f42ba Improve handling of special tokens in GGML to GGUF converter (#2725)
* | | | 46ef5b5f llama : fix whitespace escaping in tokenizer (#2724)
* | | | c63bb1d1 CUDA: use mul_mat_q kernels by default (#2683)
* | | | 3b6cfe7c convert.py : clarifying error message (#2718)
* | | | 800c9635 Fix CUDA softmax by subtracting max value before exp (#2665)
* | | | deb7dfca gguf : add ftype meta info to the model (#2710)
* | | | bac66994 Quantization imrovements for k_quants (#2707)
* | | | 519c981f embedding : evaluate prompt in batches (#2713)
* | | | 1123f7fb ggml-cuda : use graph allocator (#2684)
* | | | ef3f333d ggml : sync latest (SAM + SD operators, CUDA alibi) (#2709)
* | | | 8e4364f2 llama-bench : minor fixes (#2695)
* | | | 1e3bc523 ggml : support CUDA's half type for aarch64(#1455) (#2670)
* | | | 14b1d7e6 metal : add missing barriers for mul-mat (#2699)
* | | | 226255b4 server : fallback to default if client param is null (#2688)
* | | | 930523c8 Fix convert-llama-ggmlv3-to-gguf.py vocab conversion (#2698)
* | | | c8dba409 py : remove obsolete script
* | | | 6381d4e1 gguf : new file format with flexible meta data (beta) (#2398)
| | | * d3f5fbef main : flush stdout
| | | * e3da126f main : inject reverse prompt after EOS + update examples/chat.sh
| | | * 8af1991e main : restore old EOS behavior in interactive mode
| |_|/  
|/| |   
| | | * 66a66a05 readme : add notice about new file format
| | | * 811f653f py : cosmetics
| | | * 49c25cce tests : use new tokenizer type API (#2692)
| | | * 0b53b8b0 llama : add API for token type
| | | * 8d177edd llama : improve token type support (#2668)
| | | * e06cbcee gguf : add Python script to convert GGMLv3 LLaMA models to GGUF (#2682)
| | | * 6490ff71 py : fix whitespace
| | | *   1e7a0092 Merge branch 'master' into gguf
| | | |\  
| |_|_|/  
|/| | |   
* | | | dadbed99 metal : fix synchronization in new matrix multiplication kernel (#2686)
* | | | cb1c0727 HellaSwag: split token evaluation into batches if needed (#2681)
* | | | 9e232f02 ggml : move all type info to ggml_type_traits (#2663)
* | | | 5e9ff54a More efficient Hellaswag implementation (#2677)
* | | | 1f0bccb2 server : better default prompt (#2646)
* | | | f63564ad server : update xxd usage for older versions compatibility (#2649)
* | | | 2d8b76a1 Add link to clojure bindings to Readme. (#2659)
* | | | 7af633ae readme : incoming BREAKING CHANGE
| | | * 7a7d1ba6 convert-llama-hf-to-gguf.py : rope scale fix
| | | * 9070e330 convert-llama-7b-pth-to-gguf.py : rope scale fix
| | | * c082b9fa llama.cpp : use rope scale kv
| | | * dc1f0510 convert-llama-7b-pth-to-gguf.py : rope scale and added tokens
| | | * 5f6ff387 convert-llama-hf-to-gguf.py : rope scale and added tokens
| | | * 6a69a693 gguf.py : fix rope scale kv
| | | * c818c405 convert-llama-hf-to-gguf.py : fix attn_q permute
| | | * 58bde5c5 Delete convert-permute-debug.py
| | | * 287db510 Delete convert-permute-debug-master.py
| | | * d5c8fcfd convert.py : 70b model working (change attn_q permute)
| | | * 7de7cb4b convert-permute-debug.py : change permute type of attn_q
| | | * 4f92488d convert-permute-debug-master.py : permute debug for master
| | | * 5a02b962 convert-permute-debug.py : permute debug print
| | | * f838faa8 convert-llama-7b-pth-to-gguf.py : special tokens
| | | * 76b46627 convert-llama-hf-to-gguf.py : special tokens
| | | * 28b8c265 cmpnct_gpt2bpe.hpp : cleanup
| | | * c0a1269b Update examples/server/README.md
| | | * 6a2e5200 cmpnct_gpt2bpe.hpp : remove non-general stuff
| | | * 8945d47f gptneox-main.cpp : fixes
| | | * 781bf248 falcon-main.cpp : fixes
| | | * dadf098b cmpnct_gpt2bpe.hpp : fixes
| | | * b3a7a2b4 convert-falcon-hf-to-gguf.py : add tensor data layout
| | | * 2c8055b6 convert-falcon-hf-to-gguf.py : update ref
| | | * 1d80eea5 falcon-main.cpp : fix for falcon 40b
| | | * bd5a5790 gguf.py : fix for falcon 40b
| | | * 281d6d11 convert-llama-hf-to-gguf.py : remove extra kv
| | | * 593b04fd convert-llama-7b-pth-to-gguf.py : remove extra kv
| | | * c0e4ca63 convert-gptneox-hf-to-gguf.py : remove extra kv
| | | * 16ab9ba3 convert-falcon-hf-to-gguf.py : remove extra kv
| | | * d5e976c1 falcon-main.cpp : falcon inference example
| | | * fb7c883c convert-falcon-hf-to-gguf.py : falcon HF --> gguf conversion, not tested
| | | * 25b8a892 llama : introduce enum llama_vocab_type + remove hardcoded string constants
| | | * a4ad2bf3 llama : fix MPI build
| | | * 5d2656d6 llama : avoid hardcoded special tokens
| | | * 035d5114 llama : minor API updates
| | | * 2d6c2c75 llama : remove C++ API + reorganize common source in /common dir
| | | *   38016ed9 Merge branch 'master' into gguf
| | | |\  
| |_|_|/  
|/| | |   
* | | | 097e121e llama : add benchmark example (#2626)
* | | | eaf98c26 readme : add link to Rust bindings (#2656)
* | | | e9b12c33 perplexity : more meaningful ETA number - 2 decimal points
| | | * 660ca9bb llama : re-order functions
| | | * dea5be61 editorconfig : fix whitespaces
| | | * e35f8c74 tests : update vocab file with new magic
| | | *   856afff7 Merge branch 'master' into gguf
| | | |\  
| |_|_|/  
|/| | |   
* | | | 604b8bdf Fix unicode in grammars (fixes #2501) (#2553)
* | | | 10151bee server : support for saving templates in browser LocalStorage (#2486)
* | | | 0992a7b8 README: fix LLAMA_CUDA_MMV_Y documentation (#2647)
* | | | 6ddeefad [Zig] Fixing Zig build and improvements (#2554)
| | | * aa3efe87 llama : print number of tensors per type + print arch + style
| | | * b275de74 llama.cpp : get special token kv and linefeed token id
| | | * 306070c8 llama.cpp : print kv general.name
| | | * d9e6890a test-tokenizer-0.cpp : fix warning
| | | * 147a99bd gguf.py : reverse GGUF_MAGIC
| | | * c20ae49b ggml.h : reverse GGUF_MAGIC
| | | * 3c1b7217 convert-llama-7b-pth-to-gguf.py : fixes
| | | * 9e2d4dd4 convert-llama-hf-to-gguf.py : fixes
| | | * 640ddc42 gguf.py : gptneox mapping
| | | * b668cd32 convert-gptneox-hf-to-gguf.py : fixes
| | | * fc3a5232 gguf.py : write tensors in a single pass (#2644)
| | | | * 6a9e6375 gguf.py : indentation
| | | | *   307e09cd Merge branch 'gguf' into gguf-write-single-pass
| | | | |\  
| | | | |/  
| | | |/|   
| | | * | 5484737d llama : fix tensor name grepping during quantization
| | | * | 57eaadb8 llama : throw error if gguf fails to init from file
| | | * | b3cc1829 llama.cpp : typo
| | | * | acaa9823 convert.py : fix HF tensor permuting / unpacking
| | | * | 78e1e578 quantize-stats.cpp : .bin --> .gguf
| | | * | fb11dd3f common.h : .bin --> .gguf
| | | * | e72c8c21 ggml : fix bug in gguf_set_kv
| | | * | 899f9a53 llama : fix lambda capture
| | | * | 93f285bd gptneox : move as a WIP example
| | | * | 81a2c2a6 llama : fix llama_model_loader memory leak
| | | * | dd9e2fc9 ci : update ".bin" to ".gguf" extension
| | | * | c3b73937 editorconfig : ignore models folder
| | | * |   6d66ef96 Merge branch 'master' into gguf
| | | |\ \  
| |_|_|/ /  
|/| | | |   
* | | | | 8dae7ce6 Add --cfg-negative-prompt-file option for examples (#2591)
* | | | | a73ccf1a llama : replace (permute + reshape + view_1d) with (view_3d) (#2538)
* | | | | 7cf54e1f tests : adds simple llama grammar tests (#2618)
* | | | | a872a2b2 ggml-alloc : fix discrepency between measure&eval (#2639)
* | | | | 0919a0f7 cmake : install ggml-meta.metal if LLAMA_METAL (#2449)
* | | | | ed53db86 metal : print error of load pipeline state (#2564)
* | | | | fc8ef549 metal : enable ggml-alloc (#2627)
* | | | | bf83bff6 metal : matrix-matrix multiplication kernel (#2615)
* | | | | b5ffb284 scripts : add helper script to get wikitext
* | | | | 3ebb0093 server : add missing /json-schema-to-grammar.mjs (#2616)
| | | * | 11bf4366 llama : sync with recent PRs on master
| | | * | 8ace03ad convert.py : better always have n_head_kv and default it to n_head
| | | | * e426b3cf gguf.py : fix vertical alignment
| | | | * 4dbce7d0 gguf : rm file_type key and method
| | | | * 1d93d04c gguf : refactor pth to gguf conversion script
| | | | * f7170417 gguf : rename h5 to hf (for HuggingFace)
| | | | * 9f02694c gguf : refactor gptneox conversion script
| | | | * 22c61c5b gguf : style fixes in simple conversion script
| | | | *   2f8fc92d gguf : fix conflicts
| | | | |\  
| | | | |/  
| | | |/|   
| | | * | d646c4ef convert.py : n_head_kv optional and .gguf file extension
| | | * | dd016cc2 Revert "ci : disable CI temporary to not waste energy"
| | | * | 2ddd9681 convert.py : update to support GGUF output
| | | * | e0429d38 convert-new.py : output gguf (#2635)
| | | * | d6fd53af llama.cpp : use ggml_elements()
| | | * | 5a0a2c56 llama.cpp : print actual model size
| | | | * 5f97a48f gguf : single pass for writing tensors + refactoring writer
| | | | * dce07c31 gguf : single pass for writing tensors + refactoring writer
| | | | * f31e9230 gguf : single pass for writing tensors + refactoring writer
| | | |/  
| | | * 42f8fe19 examples/gguf : no need to keep q option for quantization any more
| | | * 5ec18934 convert-new.py : pick #2427 for HF 70B support
| | | * c8ee87f1 gguf.py : merge all files in gguf.py
| | | * 88b57694 gguf : deduplicate (#2629)
| | | * 758ff1bb llama : refactor model loading code (#2620)
| | | * ea5615a0 convert-llama-h5-to-gguf.py : clarify the reverse permute
| | | * 4a1741aa gptneox-main.cpp : add tensor data layout
| | | * 2ae0e985 convert-llama-7b-pth-to-gguf.py : add tensor data layout
| | | * 66756c82 convert-llama-h5-to-gguf.py : add tensor data layout
| | | * b6056c3d gguf.py : add tensor data layout
| | | * 2dd5d2c9 convert-llama-h5-to-gguf.py : add 70b gqa support
| | | * ca475829 gguf-llama.cpp :  fix n_head_kv
| | | * ab2cbd03 convert-llama-7b-pth-to-gguf.py : add token types
| | | * cedb4870 gguf.py : add token types
| | | * 5d518d42 constants.py : add token types
| | | * 7ec125b1 convert-llama-h5-to-gguf.py : add token types
| | | * 6c63550f llama : update tokenizer style
| | | * 7494c784 llama : sync gguf-llama with llama (#2613)
| | | * afc4ca28 convert : update convert-new.py with tokenizer fixes (#2614)
| | | * ec1b1007 llama : tokenizer fixes (#2549)
| | | *   8af3a99f Merge branch 'master' into gguf
| | | |\  
| |_|_|/  
|/| | |   
* | | | d783f798 metal : return null instead of exit(1) (#2573)
* | | | d75561df server : add --numa support (#2524)
* | | | 348acf18 llama : add missing enum keyword in function signatures (#2610)
* | | | 1cd06fa2 CUDA: launch_bounds, small q4_K, q5_K mmq refactor (#2596)
* | | | 2feb8934 server : fix default grammar by use empty string in the UI (#2604)
* | | | 5517d6e6 server : implement json-schema-to-grammar.mjs & add grammar param in the UI (#2588)
| | | * 6f148548 gitignore : add gptneox-main
| | | * f00780b2 llama : sync gguf-llama.cpp with latest llama.cpp (#2608)
| | | * 6f64b6c0 Create convert-llama-7b-pth-to-gguf.py
| | | * 62490f13 gguf : use UNIX line ending
| | | * 0c19ae70 simple : minor style changes
| | | * 5c5a95ba gguf.py : dont add empty strings
| | | * a7d226f8 convert-llama-h5-to-gguf.py : fixes
| | | * d753dfbc gptneox-main.cpp : tensor name map changes
| | | * 806a1574 Delete gguf_tensor_map.py
| | | * 51939d7d Create gguf_namemap.py : tensor name map changes
| | | * 5d22a9db convert-gptneox-h5-to-gguf.py : tensor name map changes
| | | *   56a1f320 Merge branch 'master' into gguf
| | | |\  
| |_|_|/  
|/| | |   
* | | | f31b5397 Enhance Windows 7 and below compatibility. (#2592)
* | | | ee77efea test : add simple grammar parsing tests (#2594)
* | | | f64d44a9 CUDA: Fixed OpenLLaMA 3b mmq, reduced compile time (#2590)
* | | | b19edd54 Adding support for llama2.c models (#2559)
* | | | 53dc3994 server: fixed wrong variable name in timing json (#2579)
| | | * 196b50fe gguf : add todos and comments
| | | * 24f48833 fix conflicts
| | | * 6beebf3f gptneox-main.cpp : add file_type key
| | | * 2827b840 convert-gptneox-h5-to-gguf.py : add file_type key
| | | * bf2dad31 convert : rm quantization version
| | | *   1d60468e fix conflicts
| | | |\  
| | | | * 17800cd8 convert-llama-h5-to-gguf.py : load model in parts to save memory
| | | | * e3d1f07e convert-gptneox-h5-to-gguf.py : load model in parts to save memory
| | | * | 91d4bfd5 convert : write more metadata for LLaMA
| | | |/  
| | | * 9bf5a7ef Update gguf_tensor_map.py
| | | * c7bd8c14 gptneox-main.cpp : n_layer --> n_block
| | | * e91a2224 convert-llama-h5-to-gguf.py : n_layer --> n_block
| | | * 489616e1 convert-gptneox-h5-to-gguf.py : n_layer --> n_block
| | | * d2ce9cfe gguf.py : n_layer --> n_block
| | | * 8b5f0c50 constants.py : n_layer --> n_block
| | | * 5e58ffa1 gptneox-main.cpp : n_layer --> n_block
| | | * e606ffea convert-llama-h5-to-gguf.py : simplify nbytes
| | | * f8218477 convert-gptneox-h5-to-gguf.py : simplify nbytes
| | | * 4cef57c8 convert-llama-h5-to-gguf.py : no need to convert tensors twice
| | | * 8f09157e convert-gptneox-h5-to-gguf.py : no need to convert tensors twice
| | | * 5d81a715 gguf.py : no need to convert tensors twice
| | | * 60d54083 gguf : roper closing of file
| | | * 202eab04 gguf : quantization is working
| | | * 1fc3d30b gguf : start implementing quantization (WIP)
| | | * fa7c3954 gguf : start implementing quantization (WIP)
| | | * b2571af2 gguf : start implementing quantization (WIP)
| | | * c4f02b4f gguf : start implementing quantization (WIP)
| | | * 0e1a3c7e gguf : start implementing quantization (WIP)
| | | * 4fa017a1 gguf : start implementing quantization (WIP)
| | | *   186c496f Merge branch 'gguf' of https://github.com//ggerganov/llama.cpp into gguf
| | | |\  
| | | | * e76c59d5 Update gptneox-main.cpp
| | | | * 2a5ac7af Update gguf_tensor_map.py
| | | * | 2f52008b gguf : rm references to old file magics
| | | |/  
| | | * e7324232 gguf : get rid of n_mult, read n_ff from file
| | | * f44bbd3d gguf : rm redundant method
| | | * 7009cf58 gguf : shorter name for member variable
| | | * 61919c1a gguf : rm references to old file formats
| | | * d09fd107 gguf : write metadata in gguf_file_saver
| | | * 781b9ec3 gguf : write metadata in gguf_file_saver (WIP)
| | | * 28abfc90 gguf : write metadata in gguf_file_saver (WIP)
| | | * e3a49609 gguf : add gguf_get_kv_type
| | | * eb8ca699 gguf : add gguf_get_kv_type
| | | * b2440f19 gguf : start implementing gguf_file_saver (WIP)
| | | * a356b0e2 gguf : start implementing gguf_file_saver (WIP)
| | | * e7d346c3 gguf : start implementing gguf_file_saver (WIP)
| | | * f316b94c gguf : rm deprecated function
| | | * cfb8e35b gguf :  inference with 7B model working (WIP)
| | | * 42cc04d1 gguf : calculate n_mult
| | | * 22de6c5c upd .gitignore
| | | * 4c0f64e3 rm binary commited by mistake
| | | * 4f865181 gguf : start implementing libllama in GGUF (WIP)
| | | * 1c4d8bf9 gguf : start implementing libllama in GGUF (WIP)
| | | * 0246d0dd gptneox-main.cpp : map tensor names
| | | * 7d5f4522 convert-llama-h5-to-gguf.py : map tensor names
| | | * f4d137d9 convert-gptneox-h5-to-gguf.py : map tensor names
| | | * ece4fc18 map tensor names
| | | * 65559a23 Update gptneox-main.cpp
| | | * 8083ae34 gguf : minor stuff
| | | *   1da82c55 Merge branch 'master' into gguf
| | | |\  
| | | * | 4357e692 gguf.py : use custom alignment if present
| | | * | db5618ad cmpnct_gpt2bpe.hpp : comments
| | | * | 278ada95 gguf.py : bytesarray for gpt2bpe tokenizer
| | | * | fb0b2437 Makefile : remove gptneox-common
| | | * | 5d98989c gpt2 bpe tokenizer (handles merges and unicode)
| | | * | e6f19ba2 gptneox-main.cpp : gpt2 bpe tokenizer
| | | * | 2922280a convert-gptneox-h5-to-gguf.py : gpt2bpe tokenizer
| | | * | 6691aa87 Delete gptneox-common.h
| | | * | 23abbe8e Delete gptneox-common.cpp
| | | * | c5ba5efd convert-llama-h5-to-gguf.py : special tokens
| | | * | e1e9b285 convert-llama-h5-to-gguf.py : accumulate kv / ti + special tokens
| | | * | c3a65c4b gguf-util.h : update note
| | | * | cf365fbc gguf : gguf counterpart of llama-util.h
| | | * | 1b4f9c8e convert-gptneox-h5-to-gguf.py : accumulate kv and ti + special tokens
| | | * | 49380a23 gguf.py : accumulate kv and tensor info data + special tokens
| | | * | ff1cb023 constants.py : special tokens
| | | * | 36a36c32 Update gptneox-main.cpp
| | | * | c77fabb1 gptneox-main.cpp : special tokens
| | | * | e7a74169 convert-gptneox-h5-to-gguf.py : Special tokens
| | | * | da4900e8 Update convert-llama-h5-to-gguf.py
| | | * | f3de876a fix : update convert-llama-h5-to-gguf.py
| | | * | bb42aefa gguf : mmap tensor data example
| | | * | b26f5b2e gguf : fix typo in function call
| | | * | 7aa0a0e7 gguf : support custom alignment value
| | | * | 6b3a7b9f Update convert-llama-h5-to-gguf.py
| | | * | 4f5b6224 Update convert-gptneox-h5-to-gguf.py
| | | * | 2a091467 Update convert-gptneox-h5-to-gguf.py
| | | * | 068a8e0f Update convert-llama-h5-to-gguf.py
| | | * | 30c4ea47 add gptneox gguf example
| | | * | 2fabc176 Update convert-llama-h5-to-gguf.py
| | | * | f175b058 Makefile : add gptneox gguf example
| | | * | e9192b01 add gptneox gguf example
| | | * | 4ed98bf1 Update convert-llama-h5-to-gguf.py
| | | * | b19c1175 ggml.c : add gguf_get_arr_n
| | | * | b4676ee4 ggml.h : increase GGML_MAX_NAME to 64
| | | * | ccd81a75 gguf.py : add layer norm eps and merges
| | | * | 0790c121 constants.py : add layer norm eps
| | | * | 87c34e4d gguf : update convert-llama-h5-to-gguf.py
| | | * | 32e037ff gguf : fix set is not subscriptable
| | | * | 06c3e4a1 Update convert-llama-h5-to-gguf.py
| | | * | 95778214 gguf.py : support any type
| | | * | 2c22e3bc ggml.c : get arr str and f32
| | | * | 34469b9e ggml.h : get array str and f32
| | | * | 0f5e57f0 gguf : handle already encoded string
| | | * | 8ad7cd49 Update convert-llama-h5-to-gguf.py
| | | * | 0317c41d gguf : upd gguf conversion script
| | | * | cc3dd7f0 gguf : write tokenizer data
| | | * | 8a76dd8a gguf : write tensors one by one
| | | * | c861e234 gguf : write tensors one by one
| | | * | 0c219fb5 gguf : fix writing gguf arrays
| | | * | 93f7f7ae gguf : write tensors one by one and code reuse
| | | * |   aa99562d Merge branch 'gguf' of https://github.com//ggerganov/llama.cpp into gguf
| | | |\ \  
| | | | * | 999431c4 quick and dirty conversion example
| | | * | | ea5f9ad2 gguf : fix writing gguf arrays
| | | |/ /  
| | | * | d54f53ca gguf : add tokenization constants
| | | * | 06f423a8 gguf : write sample tensors to read
| | | * | 08dc8fd8 gguf : do not hardcode tensor names to read
| | | * |   9475cdb7 Merge branch 'gguf-write-tokenization' into gguf
| | | |\ \  
| | | | * | 1495735a gguf : fix writing tensors
| | | * | | 3492f848 gguf : add gguf_find_key (#2438)
| | | |/ /  
| | | * | 11ef380c GGUF : write tensor (#2426)
| | | | | * fc60a276 ci: add linux binaries to release build
| |_|_|_|/  
|/| | | |   
* | | | | 9ca4abed Handle `ENABLE_VIRTUAL_TERMINAL_PROCESSING` more gracefully on earlier versions of Windows.
* | | | | e59fcb2b Add --n-predict -2 for stopping generation on full context (#2565)
* | | | | 16387577 Fix grammar-based sampling issue in server (#2566)
* | | | | 916a9acd ggml-alloc: Don't try to re-use buffers of external tensors (#2562)
* | | | | ea04a4ca add log_callback to llama_context_params for custom logging. (#2234)
* | | | | 25d43e0e CUDA: tuned mul_mat_q kernels (#2546)
* | | | | f5bfea05 Allow passing grammar to completion endpoint (#2532)
|/ / / /  
* | | | acfc5478 CUDA: tighter VRAM scratch size for 65b/70b (#2551)
* | | | 7ed8d1fe llm.vim : multiline autocompletion, get rid of "^@" (#2543)
* | | | e7f94d6f vim : bring back simple llm.vim example
* | | | 2d7baaf5 vim : streaming and more (#2495)
* | | | f3c3b4b1 Add --rope-scale parameter (#2544)
| |_|/  
|/| |   
* | | 93356bdb ggml : mul mat tweaks (#2372)
* | | 60baff7c ggml : pad result of ggml_nbytes()
* | | 9082b5df ggml : change params pointer (style change) (#2539)
* | | 99d29c00 ggml : sync (custom ops) (#2537)
* | | 3d9a5518 Fixed mmap prefetch for GPU offloading (#2529)
* | | f6f9896a metal : fix out-of-bounds access + inc concurrency nodes (#2416)
* | | 34a14b28 [Makefile] Move ARM CFLAGS before compilation (#2536)
* | | 7297128d [Zig] Rewrite build for Zig 0.11 (#2514)
* | | 86c32198 console : fix issue related to Windows 11 PowerShell console mode persistence (#2521)
* | | 2e8265ae convert.py : add missing abstract methods for quantized data (#2491)
* | | f514d1b3 CUDA: faster k-quant mul_mat_q kernels (#2525)
* | | 33231123 fix firefox autoscroll (#2519)
* | | 182af739 server: regenerate completion.js.hpp (#2515)
* | | 4329d1ac CUDA: use min compute capability of GPUs actually used (#2506)
* | | 02f9d96a CUDA: check if event is NULL before cudaStreamWaitEvent (#2505)
* | | 3498588e Add --simple-io option for subprocesses and break out console.h and cpp (#1558)
* | | 5f631c26 Fixing race condition in server and partial stream handling in frontend. (#2391)
* | | 415e99fe Stream save llama context data to file instead of allocating entire buffer upfront (#2488)
* | | ff966e7c build : fix several cast and printf warnings (#2499)
* | | 8183159c examples : generate JSON according to schema (#1887)
| | | *   28046d1e Merge and update
| | | |\  
| |_|_|/  
|/| | |   
* | | | 468ea24f CUDA: faster non k-quant mul_mat_q kernels (#2483)
* | | | 4f6b60c7 CUDA: Fix models with output size != 32000 (#2480)
* | | | 220d9318 readme : add Aquila-7B model series to supported models (#2487)
* | | | 81844fbc tests : Fix compilation warnings (Linux/GCC) (#2451)
* | | | a312193e readme : Add Chinese LLaMA-2 / Alpaca-2 to supported models (#2475)
* | | | c574bddb fix a typo in examples/server/README.md (#2478)
* | | | 86aeb277 server : Support dark mode (#2414)
* | | | 1873ff58 metal : add gqa8 kernel to allow llama-2-70B on metal (#2459)
* | | | 49e7cb5b CUDA: fixed LLAMA_FAST compilation option (#2473)
* | | | b772bba4 CUDA: fixed cmake F16 option (#2471)
* | | | 0728c5a8 CUDA: mmq CLI option, fixed mmq build issues (#2453)
* | | | 1215ed7d CUDA: Implemented row flattening for non-glm RoPE (#2468)
* | | | 2dbf5189 CUDA: fewer memory bank conflicts for mul_mat_q (#2458)
* | | | 9d2382b3 Fix Metal backend broken from the allocator changes (#2455)
* | | | a1136895 ggml : add graph tensor allocator (#2411)
* | | | 11f3ca06 CUDA: Quantized matrix matrix multiplication (#2160)
* | | | 9baf9ef3 CUDA: faster multi GPU synchronization (#2448)
* | | | 8a88e585 perplexity : add Hellaswag calculation (#2389)
* | | | a9559bf7 ggml : workaround for missing _mm256_setr_m128i in GCC < 8 in k_quants.c (#2405)
* | | | ee1b497c llama : support more diverse tokenizers? (#2420)
* | | | d73b8d48 examples : fix whitespace
* | | | 34ae1caf examples : server chat mode with llama2 (#2400)
* | | | d91f3f0c readme : fix the description of the Tail free sampling (TFS) method (#2431)
* | | | 65cdf34b llama : use n_embd_gqa instead of n_embd to handle llama-2 70B (#2433)
* | | | edcc7ae7 Obtaining LLaMA 2 instructions (#2308)
* | | | 7c529ced convert.py : Update to support 70B HF format model files (#2427)
|/ / /  
* | | 1a941869 metal : disable graph concurrency optimization due to bug (#2413)
* | | b5472ea0 ggml : fix assert in ggml_set_unary_op (#2410)
* | | 6df1f594 make : build with -Wmissing-prototypes (#2394)
| | * ca2467d1 chat css
| | *   f77972f9 Merge remote-tracking branch 'origin/master' into server-cfg
| | |\  
| | * | e4db7072 [wip] chat now has parameter and cfg
| | * | 082dd812 [wip] chat improvements
| | * | 43694ca8 consistent semicolons
| | * |   890d1b84 Merge master into server-cfg
| | |\ \  
| | * | | dd3cf576 last n tokens done
| | * | | 42591a0a remove "smooth factor"
| | * | | 2cb8469e refactor evaluation logic
| | * | | 4cae9f56 Port CFG to server.
| | * | | 3a13d1e8 Apply formatting.
| | | | | * 51105572 undo formatting
| | | | | * 0c43a3b7 gitignore *.gguf
| | | | | * 8e62d2b2 rm example.gguf
| | | | | * 62f4926b fix : fix errors upd writing example
| | | | | * 94112505 refactor : rm unused import and upd todos
| | | | | * bb54d170 GGUF : Support writing tensors in Python
| | | | | * 464192b9 WIP: Write tensor
| | |_|_|/  
| |/| | |   
| * | | | d2bb3ac1 convert.py : remove GGML vocab + other obsolete stuff
| * | | | 68f53485 convert.py : start a new simplified implementation by removing old stuff
| * | | | 158be8f7 gguf.py : some code style changes
| * | | | d2b6ca13 gguf : add array support
| * | | | d89533df gguf : expose the gguf_type enum through the API for now
| * | | | c85d3178 refactor : reduce code duplication and better API (#2415)
| | | | | * af1c9966 gguf : start write tensor info
| | | | | * 8332d261 refactor: reduce code duplication and better API
| | |_|_|/  
| |/| | |   
| * | | | d8491fc7 gguf : add comments
| * | | | 5628ec71 gguf : read / write sample models
| * | | | e46870f5 gguf : gguf.c is now part of ggml.c
| * | | | d313c0fa gguf : simplify gguf_get_val
| * | | | cb871fa0 gguf : do not support passing existing ggml_context to gguf_init
| * | | | 860c9c63 gguf : add gguf_get_tensor_name()
| * | | | 78b226a9 gguf : initial model loading - not tested
| * | | | d91b985d gguf : read tensor info
| * | | | 8d6acfec gguf : read header + meta data
| * | | | 68731487 gguf : first API pass
| * | | | 7e82d25f ci : disable CI temporary to not waste energy
| * | | | bae6b125 wip : implement GGUF (#2397)
| * | | | 4d698495 gguf : init
|/ / / /  
* | | | 5488fb78 ggml : allocate graphs in a context (#2392)
* | | | eb542d39 Add LLAMA_DEFAULT_RMS_EPS so we can change the default (#2384)
* | | | 07aaa0f6 ggml : fix ggml_flash_attn to use op_params (#2387)
* | | | fce48caf convert.py : support bpe tokenizer (#2228)
* | | | 875086bd ggml : relax contiguous constraints in activation function (#2371)
* | | | da188983 ggml : improve graph build time via hash table lookup (#2329)
* | | | 82552b7f build : fix line breaking error in build-info.sh (#2349)
* | | | 0c06204f main : add `--in-prefix-bos` to prefix BOS to user inputs; keep EOS (#2304)
* | | | 1fed755b ci : add non-AVX scalar build/test (#2356)
* | | | be2301bc k_quants : add AVX support to dot functions with QK_K as 64 (#2339)
* | | | 1aa18ef9 metal : concurrently dispatch commands (#2358)
* | | | 9a08eaf3 Another speed gain for Q4_0 and Q4_1 on Metal (#2375)
* | | | 129d844c Fix Q4_K and Q5_K for QK_K = 64 on CUDA (#2359)
* | | | d5512b78 server: add rms_norm_eps parameter (#2380)
* | | | c798308e [Server] Escape HTML in webchat (#2368)
* | | | 41c67416 make rms_norm_eps a parameter (#2374)
* | | | b3f138d0 Chat UI extras (#2366)
* | | | 5b2b2dc6 ggml : sync (unary ops refactor, static-correctness) (#2370)
| |_|/  
|/| |   
* | | 42f70cb2 Fix scalar version of Q5_K when QK_K = 64 (#2362)
* | | 84e09a7d llama : add grammar-based sampling (#1773)
* | | 2f9cf974 Some more Q4_K and Q5_K speedup on CUDA (#2346)
* | | 4f06592c Add gqa parameter support to the server (#2351)
* | | 70d26ac3 Fix __dp4a documentation (#2348)
* | | 57921ca6 common : n_threads == -1 uses std::thread::hardware_concurrency() (#2347)
* | | 3602ac42 fix n_tasks (#2342)
* | | 95a6c595 ggml: move op parameters from tensors to ggml_tensor::op_params (#2333)
* | | e76d630d llama : grouped-query attention + LLaMAv2 70B support (#2276)
* | | 1d0824b2 llama : print help to stdout (#2338)
* | | bc3ec2cd flake : support `nix build '.#opencl'` (#2337)
* | | a940458e llama : print max tensor size to stderr (#2336)
* | | 91171b80 make : fix CLBLAST compile support in FreeBSD (#2331)
* | | 355c80f4 examples : simplify vim plugin (#2327)
* | | 83a00ce6 metal : support bcast add & dup & cont op (#2323)
* | | d2a43664 Speed up Q4_K (#2322)
* | | b9b7d94f CUDA: Fixed 7b q3_K_S with mul_mat_vec_q (#2313)
* | | b47b8a9c llama : optimize memory buffers (#2325)
* | | b5fe67f8 Perplexity: Compute scores correlated to HellaSwag (#2312)
* | | 24baa54a examples : basic VIM plugin
* | | dd6c67d3 ci : fix args
* | | 5d500e8c ci : add 7B CUDA tests (#2319)
* | | 7d5f1846 examples : add easy python script to create quantized (k-bit support) GGML models from local HF Transformer models (#2311)
* | | d924522a Custom RoPE + bettter memory management for CUDA (#2295)
* | | 4d76a5f4 Faster Q3_K implementation on Metal (#2307)
* | | 0db14fef ggml : fix the rope fix (513f8619535a64fa9ace808cdcbcf66211535f5c)
* | | 03e56697 examples :  fix typo in minigpt4.py (#2298)
* | | 513f8619 ggml : fix rope args order + assert (#2054)
* | | 3973b25a gitignore : fix final newline
* | | ab0e26bd llama : remove cfg smooth factor as it is only a reparameterization of the guidance scale (#2280)
* | | 73643f5f gitignore : changes for Poetry users + chat examples (#2284)
* | | a814d04f make : fix indentation
* | | 4c013bb7 ci : fix MNT realpath usage (#2250)
* | | 42c7c2e2 make : support customized LLAMA_CUDA_NVCC and LLAMA_CUDA_CCBIN (#2275)
* | | 78a3d134 flake : remove intel mkl from flake.nix due to missing files (#2277)
* | | ae178ab4 llama : make tensor_split ptr instead of array (#2272)
* | | 54e3bc76 make : add new target for test binaries (#2244)
* | | 019fe257 MIKU MAYHEM: Upgrading the Default Model for Maximum Fun 🎉 (#2287)
* | | e68c96f7 Faster Q2_K on Metal (#2297)
* | | 9cf022a1 make : fix embdinput library and server examples building on MSYS2 (#2235)
* | | e782c9e7 Faster Q5_K and Q6_K on Metal (#2294)
* | | 785829df Faster Q4_K on Metal (#2290)
* | | fff0e0ea llama : fix regression from #2000 - could not load no-mmap models
* | | 417a85a0 metal: minor q4 optimization and reduce code size (#2248)
| |/  
|/|   
* | 294f4245 llama : extend API to get max devices at runtime (#2253)
* | 45a1b07e flake : update flake.nix (#2270)
* | b1f42909 cmake : install targets (#2256)
* | d01bccde ci : integrate with ggml-org/ci (#2250)
* | 6cbf9dfb llama : shorten quantization descriptions
* | 7568d1a2 Support dup & cont ops on CUDA (#2242)
* | b7647436 llama : fix t_start_sample_us initialization warning (#2238)
* | 672dda10 ggml : fixed runtime bugs and compile errors related to GGML_PERF and GGML_DEBUG (#2219)
* | 27ab66e4 py : turn verify-checksum-models.py into executable (#2245)
| | * d273bfd2 allocator: cleanup, more comments
| | * 5141472e llama.cpp: print input/output buffers size
| | * e2b95759 allocator cleanup
| | * 7de78825 allocator: fix partial offloading
| | * e87840f9 allocator: automatic inplace operations
| | * 3d679827 improved memory management fixes
| | * 56e9ae06 llama.cpp: partially restore state support, graph export
| | * 37d3f6a2 remove unused code
| | * cd6f5dec improved memory management
| | * de69f8f2 initial implementation of delayed graph allocation
| | * cb205c0d automatically calculate compute buffer sizes (without graph allocator)
| | * 77ac8dea llama.cpp: remove backend-specific code where possible
| | | * d45c1631 metal : rewrite to fit new backend interface correctly (WIP)
| | | * cb82adad metal : first working version of the inference without prompt processing
| | | * 290cb700 metal : map the CPU buffers to Metal buffers (WIP)
| | | *   f38433ef Merge remote-tracking branch 'origin/ggml-backends' into ggml-backends-metal
| | | |\  
| | | |/  
| | |/|   
| | * | 295f8565 allocators wip renamed ggml_backend functions changed ggml_buffer and ggml_backend to always be used as pointers rename ggml_tensor::params -> op_params
| | | * 70c55c17 metal : create backend, mostly reuse CPU backend interface
| | | * ed960fa1 llama : separate compute buffer for metal
| | | * 652c8496 ggml : add is_ram_shared to ggml_backend
| | | * 90503f15 llama : init metal backend as CPU backend for now
| | | * 0a3861c4 metal : adapting to ggml_backend (WIP)
| | |/  
| | * 1102ff56 fix double-free with --no-mmap
| | * 4e94af30 improve layer backend printing with ranges
| | * c2beeb8e only allocate as much memory as is required in each backend for the model
| | * 9c72e7e9 rebase to master (except ggml-cuda)
| | * 33ab185d fix NVCC version on Makefile, __halves2half2 -> make_half2
| | * 24cc6f00 minor fixes
| | * 5765d7a5 restore simple.cpp for now
| | * 0d2b66c6 ggml backend interface wip
| |/  
|/|   
* | 6e7cca40 llama : add custom RoPE (#2054)
* | a6803cab flake : add runHook preInstall/postInstall to installPhase so hooks function (#2224)
* | 7dabc66f make : use pkg-config for OpenBLAS (#2222)
* | 7cdd30bf cuda : allocate all temporary ggml_tensor_extra_gpu from a fixed-size buffer (#2220)
* | e8035f14 ggml : fix static_assert with older compilers #2024 (#2218)
* | 7513b7b0 llama : add functions that work directly on model (#2197)
* | de834242 build.zig : install config header (#2216)
* | c48c525f examples : fixed path typos in embd-input (#2214)
* | 206e01de cuda : support broadcast add & mul (#2192)
* | 4304bd3c CUDA: mul_mat_vec_q kernels for k-quants (#2203)
* | 229aab35 make : fix combination of LLAMA_METAL and LLAMA_MPI (#2208)
* | 69796668 ggml : sync (ggml_conv_2d, fix mul_mat bug, CUDA GLM rope)
* | 27ad57a6 Metal: faster Q4_0 and Q4_1 matrix x vector kernels (#2212)
|/  
* 32c54116 Revert "Support using mmap when applying LoRA (#2095)" (#2206)
* ff5d58fa Fix compile error on Windows CUDA (#2207)
* b782422a devops : add missing quotes to bash script (#2193)
* 1cbf5614 metal : new q4_0 matrix-vector kernel (#2188)
* 975221e9 ggml : broadcast mul_mat + conv batch support (#2199)
* 4523d10d ggml : add ggml_pool_1d and ggml_pool_2d
* 680e6f91 cuda : add gelu support
* 4e7464ef FP16 is supported in CM=6.0 (#2177)
* 2b5eb72e Fixed __dp4a compute capability: 6.0 -> 6.1 (#2189)
* f7d278fa ggml : revert CUDA broadcast changes from #2183 (#2191)
* 20d7740a ggml : sync (abort callback, mul / add broadcast, fix alibi) (#2183)
* 5bf2a277 ggml : remove src0 and src1 from ggml_tensor and rename opt to src (#2178)
* c9c74b4e llama : add classifier-free guidance (#2135)
* 3ec7e596 docker : add '--server' option (#2174)
* 917831c6 readme : fix zig build instructions (#2171)
* 23474632 Support using mmap when applying LoRA (#2095)
* bbef2821 Possible solution to allow K-quants on models with n_vocab!=32000 (#2148)
* 5656d105 mpi : add support for distributed inference via MPI (#2099)
| * 04923631 mpi : fix after master merge
| *   81c5ddd5 Merge branch 'mpi' into refactor-mpi
| |\  
| | * 03cc12be [mpi] continue-on-error: true
| | * 4a9a4748 Add OpenMPI to GH action
| | *   0f557c2a Merge branch 'master' into mpi
| | |\  
| |_|/  
|/| |   
* | | 1d163099 llama : remove "first token must be BOS" restriction (#2153)
* | | db4047ad main : escape prompt prefix/suffix (#2151)
* | | 18780e0a readme : update Termux instructions (#2147)
* | | 3bbc1a11 ggml : fix buidling with Intel MKL but ask for "cblas.h" issue (#2104) (#2115)
* | | 2492a53f readme : add more docs indexes (#2127)
* | | 64639555 Fixed OpenLLaMA 3b CUDA mul_mat_vec_q (#2144)
| * | 9da9d26c mpi : minor
| * | beadbf33 mpi : fix inference
| * | ef37dd14 mpi : fix output tensor after MPI compute (still not working)
| * | c717c518 mpi : various fixes - communication now works but results are wrong
| * | 01abb3b3 mpi : move all MPI logic into ggml-mpi
| * | e339d355 mpi : add names for layer inputs + prep ggml_mpi_graph_compute()
| * | 3232db62 mpi : trying to move more MPI stuff into ggml-mpi (WIP) (#2099)
| |/  
| * ef61acfb Add info to README
| * 55207ba2 Add GH workflow, fix test
| * 1f0a2cfe Update CMakeLists.txt
| * 06a23934 PR comments
| *   32deabfd Merge branch 'master' into mpi
| |\  
| * | 042c5b27 wrap includes
| * | 668ba5fe fixes
| * | d05ca74d fix warnings, update README
| * | f85785f6 MPI support, first cut
| | | * e3445406 ci: add linux binaries to release build
| |_|/  
|/| |   
* | | 061f5f8d CUDA: add __restrict__ to mul mat vec kernels (#2140)
* | | 84525e79 docker : add support for CUDA in docker (#1461)
* | | a7e20edf ci : switch threads to 1 (#2138)
* | | 1d656d63 ggml : change ggml_graph_compute() API to not require context (#1999)
* | | 72421402 ggml : remove sched_yield() call in ggml_graph_compute_thread() (#2134)
* | | 3e08ae99 convert.py: add mapping for safetensors bf16 (#1598)
* | | 481f793a Fix opencl by wrap #if-else-endif with \n (#2086)
| |/  
|/|   
* | dfd9fce6 ggml : fix restrict usage
* | 36680f6e convert : update for baichuan (#2081)
* | a17a2683 alpaca.sh : update model file name (#2074)
* | 31cfbb10 Expose generation timings from server & update completions.js (#2116)
| | * 26cc1bd7 llama : uniform variable names + struct init
| | * 4fe95c69 update readme, update baked includes
| | * 30d973dc export llama_timings as struct and expose them in server
| | * a76ce02a use javascript generators as much cleaner API
| |/  
|/|   
* | 983b555e Update Server Instructions (#2113)
* | ec326d35 ggml : fix bug introduced in #1237
* | 1b6efeab tests : fix test-grad0
* | 1b107b85 ggml : generalize `quantize_fns` for simpler FP16 handling (#1237)
* | 8567c76b Update server instructions for web front end (#2103)
* | 924dd22f Quantized dot products for CUDA mul mat vec (#2067)
* | 051c70dc llama: Don't double count the sampling time (#2107)
* | 9e4475f5 Fixed OpenCL offloading prints (#2082)
* | 7f0e9a77 embd-input: Fix input embedding example unsigned int seed (#2105)
* | b472f3fc readme : add link web chat PR
| | * ff6e39f1 use javascript generators as much cleaner API
| | * efa86bf2 export llama_timings as struct and expose them in server
| | * c19daa4e basic response formatting
| | * eee6d69e fix mobile, fix missing prompt cache
| | * fedce007 rework state management into session, expose historyTemplate to settings
| | * 98e612ce slightly nicer css
| | * dd1df3f3 add /completion.js file to make it easy to use the server from js
| | * 8e1b04d3 enable server in Makefiles
| | * dc7dd088 let's try this with the xxd tool instead and see if msvc is happier with that
| | * 34fc3c7e remove need for @microsoft/fetch-event-source dep (-7kb)
| | * e192f950 revert log format changes
| | * 0f95689c improvements
| | * 7a389564 allow server to multithread
| | * a30d4b2a switched to fprintf logging and to access_log
| | * c8cedf56 newline police
| | * 022bf2bb embed index and add --path for choosing static dir
| | * e3fba85d minor aesthetic fixes
| | * c1cb0e1d server : clear trailing whitespace
| | * b07b2713 tighter
| | * 627d3ba8 expose simple web interface on root domain
| | | * f46db27e ci : disable FMA on Mac OS
| | | * 83efc920 ci : print mac os "sysctl -a"
| | | *   8e9af803 Merge branch 'master' into HEAD
| | | |\  
| |_|_|/  
|/| | |   
* | | | ed9a54e5 ggml : sync latest (new ops, macros, refactoring) (#2106)
* | | | f257fd25 Add an API example using server.cpp similar to OAI. (#2009)
* | | | 7ee76e45 Simple webchat for server (#1998)
| |/ /  
|/| |   
* | | acc111ca Allow old Make to build server. (#2098)
* | | 23c7c6fc Update Makefile: clean simple (#2097)
* | | 698efad5 CI: make the brew update temporarily optional. (#2092)
* | | 14a2cc71 [ggml] fix index for ne03 value in ggml_cl_mul_f32 (#2088)
* | | 1cf14cce fix server crashes (#2076)
* | | cc45a7fe Fix crash of test-tokenizer-0 under Debug build (#2064)
* | | 55dbb915 [llama] No need to check file version when loading vocab score (#2079)
|/ /  
* | d7d2e6a0 server: add option to output probabilities for completion (#1962)
| * 81f28f25 Remove call to ggml_cuda_mul_mat_get_wsize
| * f9c585f0 Generalize quantize_fns for simpler FP16 handling
|/  
* 46088f72 ggml : fix build with OpenBLAS (close #2066)
* 0bc2cdfc Better CUDA synchronization logic (#2057)
* befb3a35 Test-based VRAM scratch size + context adjustment (#2056)
* b2132270 cmake : don't force -mcpu=native on aarch64 (#2063)
* 2f8cd979 metal : release buffers when freeing metal context (#2062)
* 471aab6e convert : add support of baichuan-7b (#2055)
* 463f2f4c llama : fix return value of llama_load_session_file_internal (#2022)
* cb44dbc7 llama : catch llama_load_session_file_internal exceptions (#2022)
* 79f634a1 embd-input : fix returning ptr to temporary
* 04606a15 train : fix compile warning
* b1ca8f36 ggml : disable GGML_TASK_INIT and GGML_TASK_FINALIZE by default (#1995)
* b8c8dda7 Use unsigned for random seed (#2006)
* 96a712ca Porting the improved K-Quant CUDA kernels to OpenCL (#1966)
* d3494bb8 llama : replacing auto &kv with const auto &kv (#2041)
* 5b351e94 cuda : remove nchannels_x argument from mul_mat_vec_nc_f16_f32 (#2028)
* 6432aabb cuda : fix missing const qualifier in casts (#2027)
* b922bc35 llama : remove shards weight file support (#2000)
* 7f9753fa CUDA GPU acceleration for LoRAs + f16 models (#1970)
* cfa0750b llama : support input embeddings directly  (#1910)
* 9d23589d fix pthreads setaffinity usage on android (#2020)
* 0be54f75 baby-llama : fix build after ggml_rope change (#2016)
* 181e8d97 llama : fix rope usage after ChatGLM change
* d9779021 ggml : add support for ChatGLM RoPE
* d38e4515 readme : add Scala 3 bindings repo (#2010)
* eaa6ca5a ggml : increase max tensor name + clean up compiler warnings in train-text (#1988)
* aa777abb readme : LD_LIBRARY_PATH complement for some Android devices when building with CLBlast inside Termux (#2007)
| * 5cc672a9 metal : try to utilize more of the shared memory using smaller views
|/  
* c824d2e3 ggml : avoid conv 2d kernel round up
* b853d456 ggml : add NUMA support (#1556)
* 9225baef k-quants : fix indentation
* a84ab1da tests : fix quantize perf (#1990)
* 5743ca80 k-quants : add AVX support to dot functions (#1916)
* 412c60e4 readme : add link to new k-quants for visibility
* 6769e944 k-quants : support for super-block size of 64 (#2001)
* cbebf61c Fix assert when free invalid cuda pointer (#2005)
| * 78fafcaf ggml : do not use _GNU_SOURCE gratuitously
|/  
* 447ccbe8 readme : add new roadmap + manifesto
* bd34cdde ggml : sync latest ggml (custom operators)
* c2a08f87 fix server sampling: top k sampler first (#1977)
* 66a2555b readme : add Azure CI discussion link
* e65ca7e1 zig : upgrade build system support (#1981)
* 5ec8dd5a #1869 Fix null reference errors when training from scratch with CUDA (#1907)
* 65bdd52a tests : sync test-grad0 from ggml
* fdd18609 flake : fix ggml-metal.metal path and run nixfmt (#1974)
* c943d823 convert : fix invalid params in write_vocab_only (#1975)
* f2c754e1 ggml : improve ggml_graph_dump_dot, add ggml_format_name (#1978)
* 11da1a85 readme : fix whitespaces
* 235b610d readme : fixed termux instructions (#1973)
* b061ba9e llama : fix top-p sampling to match the canonical definition (#1953)
* 527b6fba llama : make model stateless and context stateful (llama_state) (#1797)
* d7b7484f Add OpenLLaMA instructions to the README (#1954)
* 74871372 rework convert.py to read hyper-parameters from config.json (#1958)
* bbca06e2 cmake: revert CUDA arch default to 52, 61 if f16 (#1959)
* fb98254f Fix typo in README.md (#1961)
* 049aa16b readme : add link to p1
* 2322ec22 Fix typo (#1949)
* aacdbd40 llama : fix params struct slignment (#1936)
* 20568fe6 [Fix] Reenable server embedding endpoint (#1937)
* 18b35625 ggml : fix bug in LBFGS optimizer (found by ggml tests)
* ba4e85a8 llama : use aligned memory during ggml_init call from loading saved sessions (#1934)
* 23fc5c21 cmake : fix trailing whitespaces
* cb40dfca llama : only use Q6_K for output weights if tensor size is multiple of 256 (#1932)
* ca7c3f4d cuda : faster k-quants on older GPUs (#1930)
* b97ca431 ggml : sync latest ggml repo (#1924)
* 1e3abfce cmake : fix build shared ggml when CUDA is enabled (#1929)
* 16b9cd19 Convert vector to f16 for dequantize mul mat vec (#1913)
* b24c3049 Added tokens per second to info prints (#1928)
* 0ede372a Fixed incorrectly applying RMS norm twice (#1925)
* 8596af42 ggml : fix bug in ggml_compute_forward_add_q_f32 (#1918)
* e1886cf4 readme : update Android build instructions (#1922)
* 8ab8ba62 llama : prevent usage of k-quants when tensor size is not a multiple of 256 (#1921)
* 90cc59d6 examples : fix examples/metal (#1920)
* ce2c7d72 metal : handle buffers larger than device's maxBufferLength (#1826)
* 57cd6946 cmake : add CUDA_ARCHITECTURES to new target ggml_static (#1917)
* b2416493 make : do not print help for simple example
* 4f9c43e3 minor : warning fixes
* 2c9380dd Only one CUDA stream per device for async compute (#1898)
* 051e1b0e llama : fix kv_cache `n` init (close #1903)
* 86c75718 make : update for latest Arch (#1701)
* 3d59ec59 ggml : fix warnings under MSVC (#1908)
* 0711a5f6 metal : add norm, cpy f16->f16, alibi kernels (#1823)
* fc45a81b exposed modules so that they can be invoked by nix run github:ggerganov/llama.cpp#server etc (#1863)
* 794db3e7 Server Example Refactor and Improvements (#1570)
* 5ddf7ea1 hooks : setting up flake8 and pre-commit hooks (#1681)
* bac19927 readme :  alternative way to build for Android with CLBlast. (#1828)
* b4c6f46f Allow cmake to build ggml as a library (#1896)
* 92f20d99 train : get raw text instead of page with html (#1905)
* d411968e opencl : support k-quants (#1836)
* b41b4cad examples : add "simple" (#1840)
* 13fe9d2d cmake : add auto detection of BLAS_INCLUDE_DIRS (#1886)
* ac3b8869 llama : fix embd when offloading non-repeating layers (#1891)
* 5b9ccaf1 Fixed possible macro redefinition (#1892)
* 9cbf50c0 build : fix and ignore MSVC warnings (#1889)
* 3d011226 CUDA : faster k-quant dot kernels (#1862)
* 602c7488 gitignore : add several entries specific to Visual Studio (#1888)
* a09f9195 Fixed CUDA runtime version check (#1879)
* bed92756 cmake : remove whitespaces
* c36e81da examples : add chat-vicuna.sh (#1854)
* 3559433f cmake : set include path for OpenBlas (#1830)
* 69b34a0e swift : Package compile breaks due to ggml-metal.metal (#1831)
* cf267d1c make : add train-text-from-scratch (#1850)
* 9dda13e5 readme : server compile flag (#1874)
* 37e257c4 make : clean *.so files (#1857)
* 64cc19b4 Fix the validation of main device (#1872)
* 4bfcc855 metal : parallel command buffer encoding (#1860)
* 6b8312e7 Better error when using both LoRA + GPU layers (#1861)
* 254a7a7a CUDA full GPU acceleration, KV cache in VRAM (#1827)
| * d9f38465 ci: add linux binaries to release build
|/  
* 92549202 baby-llama : fix operator!= (#1821)
* e32089b2 train : improved training-from-scratch example (#1652)
* 2347e45e llama : do a warm-up eval at start for better timings (#1824)
* 74d4cfa3 Allow "quantizing" to f16 and f32 (#1787)
* 74a6d922 Metal implementation for all k_quants (#1807)
* e4caa8da ci : run when changing only the CUDA sources (#1800)
* 58970a4c Leverage mmap for offloading tensors to GPU (#1597)
* 8c0a10e6 metal : fix failure to load model (#1817)
* fa84c4b3 Fix issue where interactive mode crashes when input exceeds ctx size (#1789)
* 12b063f0 Fixed WSL cuda's OOM error (#1594)
* 31d2b5f4 Update SHA256SUMS with current hashes for models quantized using q4_0 (#1798)
* 4de0334f cmake : fix Metal build (close #1791)
* 3f122315 k-quants : GCC12 compilation fix (#1792)
* 303f5809 metal : fix issue with ggml-metal.metal path. Closes #1769 (#1782)
* 059e9906 doc : fix wrong address of BLIS.md (#1772)
* 17c10acf ggml : force no_alloc == false when creating opt tensors (close #1699)
* e9b66ee9 metal : add Q4_1 implementation (#1785)
* 4f0154b0 llama : support requantizing models instead of only allowing quantization from 16/32bit (#1691)
* ef3171d1 ggml : workaround for missing _mm256_setr_m128i in GCC < 8 (#1638)
* 555275a6 make : add SSSE3 compilation use case (#1659)
* 98ed1655 OpenCL: Add release memory (#1741)
* ae9663f1 Windows nvcc workaround (#1753)
* b33dee28 metal : fix build "tanhf" -> "tanh"
* 92f44ff7 metal : add GELU implementation (#1770)
* 245fc3c3 metal : faster q4_0 (#1775)
* 72ff5282 metal : add Q2_K implementation (#1762)
* 0bf7cf1b Revert "ggml : load data into int8x16x4_t using vld4q_s8 on arm64 (#1738)"
* 8432d4d9 ggml : load data into int8x16x4_t using vld4q_s8 on arm64 (#1738)
* 0f291e1f metal : Q6_K implementation (#1752)
* 8fc81799 Add llama.cpp docker support for non-latin languages (#1673)
* b50b570e ggml : fix fprintf warnings (#1720)
* 53aba3f3 clang-tidy : restore dot file from accidental deletion
* 4161bdc0 metal : add Q4_K implementation (#1733)
* 00358582 k-quants : add missing compile definition to CMakeLists (#1748)
* 5c64a095 k-quants : allow to optionally disable at compile time (#1734)
* 5b57a5b7 flake : update to support metal on m1/m2 (#1724)
* 4dc62c54 readme : add June roadmap
* 35a84916 main: add the possibility to open the prompt cache read-only (#1640)
* 2d7bf110 llama : fix vram_scratch var
* 2a4e41a0 llama : fix compile warnings
* 17366df8 Multi GPU support, CUDA refactor, CUDA scratch buffer (#1703)
* 44f906e8 metal : add f16 support
* d5b111f5 Clblast fixes + enhancements to save VRAM and offload more layers (#1675)
* 2d43387d ggml : fix builds, add ggml-quants-k.o (close #1712, close #1710)
* 7ad7750c gitignore : add .clang-tidy
* 7a74dee6 llama : temporary disable Q6_K output quantization (#1711)
* 590250f7 metal : add checks for buffer size (#1706)
* f4c55d3b docs : add performance troubleshoot + example benchmark documentation (#1674)
* f1465624 readme : fix typo (#1700)
* c2df36d6 llama : consistently catch and throw only exceptions deriving from std::exception (#1599)
* 9d0693bc metal : use shared buffers between CPU and GPU (#1696)
* efe05076 ggml : fix internal overflow in ggml_time_us on Windows (#1702)
* e7fe66e6 ci : disable auto tidy (#1705)
* 99009e72 ggml : add SOTA 2,3,4,5,6 bit k-quantizations (#1684)
* 5220a991 Increase 3B scratch buffers. (#1698)
* d1f563a7 llama : fix Metal KV cache sync (close #1695)
* 827f5eda readme : update hot topics
* ecb217db llama : Metal inference (#1642)
* dcb2ed48 OpenCL: Fix duplication of layers in VRAM and RAM, add GPU mul kernel (#1653)
* d8bd0013 Add info about CUDA_VISIBLE_DEVICES (#1682)
* b5c85468 Docker: change to calling convert.py (#1641)
* 136476e8 Fix prompt cache saving and chat-persistent rollover (#1678)
* ffb06a34 OpenLLaMA 3B support (#1588)
* 7552ac58 ggml : sync cgraph import / export API
* 5d1830b9 ggml : fix bug in ggml_alibi
* 24836760 Work around for recalculating logits in cached prompts (Fixes #1585) (#1609)
* 0e730dd2 Adding git in container package dependencies (#1621)
* 3b126f65 LLAMA_DEBUG adds debug symbols (#1617)
* 1b78ed20 Only show -ngl option when relevant + other doc/arg handling updates (#1625)
* 337aea11 examples : add --alias option to gpt_params to set use friendly model name (#1614)
* bb051d97 opencl : no need to allocate cl_mem on heap (#1612)
* ca74884f opencl : use strstr to check if fp16 supported (#1611)
* a6704643 ggml : add support for the RISCV architecture (#1616)
* 0df7d63e Include server in releases + other build system cleanups (#1610)
* 97c9b77c Add documentation about CLBlast (#1604)
* 0ecb1bbb [CI] Fix openblas (#1613)
* 93618031 ggml : add ggml_tensor_overhead()
* 83c54e6d [CI] CLBlast: Fix directory name (#1606)
* bdbda1b1 ggml : sync ggml core (minor additions, e.g. ggml_get_tensor_by_name())
| * 20054a38 Fix directory name
|/  
* 66874d4f Some improvements to loading the session with --prompt-cache (#1550)
* 1fcdcc28 cuda : performance optimizations (#1530)
* ac7876ac Update CLBlast to 1.6.0 (#1580)
* c31bbe93 readme : add docs for chat-persistent.sh (#1568)
* 1359b6ab chat-persistent.sh : use bracket expressions in grep (#1564)
* 7d873811 Fix handling of "invalid property" when creating OpenCL command queue (#1565)
* 2e6cd4b0 OpenCL Token Generation Acceleration (#1459)
* 7e4ea5be examples : add server example with REST API (#1443)
* 7780e4f4 make : .PHONY clean (#1553)
* 265db983 ggml : output 3d sizes in ggml_graph_dump_dot()
* fab49c68 ggml : update WASM SIMD
* b8ee340a feature : support blis and other blas implementation  (#1536)
* 9ecb30f9 OpenCL: Fixes for older devices. (#1435)
* 29cf5596 llama : define magic numbers as integer constants (#1518) (#1520)
* 3de84b26 ggml : add ggml_clamp() (#1539)
* affc76ed cuda : loading models directly into VRAM, norm calculation on GPU, broadcasting for ggml_mul (#1483)
* ea600071 Revert "feature : add blis and other BLAS implementation support (#1502)"
* 07e9ace0 feature : add blis and other BLAS implementation support (#1502)
* ec2e10c4 llama : add llama_init_backend() API (close #1527)
* d2c59b8b Fix for mingw (#1462)
* 503db288 llama : fix name shadowing and C4146 (#1526)
| * a1cdd29c ggml : rms_norm in chunks
| * 5a317898 ggml : process mul mat rows in chunks
|/  
* 8a203f9f llama : fix compile warnings in llama_set_state_data()
* 4fd3e292 ggml : fix scalar implementation of Q4_1 dot
| *   95dc4d72 Merge 'origin/master' into steering
| |\  
| |/  
|/|   
* | 2d5db483 ggml : use F16 instead of F32 in Q4_0, Q4_1, Q8_0 (#1508)
* | 6986c783 tests : add missing header
* | 943e6081 examples : add persistent chat (#1495)
* | 7694b52b main : make reverse prompt option act as a stop token in non-interactive mode (#1032)
* | 79e3efb0 readme : adds WizardLM to the list of supported models (#1485)
* | 4b7e245a minor : fix compile warnings
* | 5ea43392 make kv_f16 the default for api users (#1517)
* | ee965413 Fixes #1511 lambda issue for w64devkit (mingw) (#1513)
* | dc271c52 Remove unused n_parts parameter (#1509)
| * da3d60f1 turning off
| * 5c9b45c2 Fix a very noobish C mistake
| * 7df9ab96 clean up
| * 7f59af52 Steer with inpSA instead of with inpL
| * 1b0ff2cf Update examples/common.cpp
| * c90059fb separate source layer for steering vector.
| * 8388aaa6 cleanup and stuff
| * 021e6d99 Steering
| | * 40ec4882 ggml : use F16C conversion when available
| |/  
|/|   
* | c238b587 benchmark-matmul: Print the average of the test results (#1490)
* | 2b264693 convert.py: Support models which are stored in a single pytorch_model.bin (#1469)
* | 42627421 ~7% faster Q5_1 AVX2 code (#1477)
* | 95606554 define default model path once, sync path with readme (#1366)
* | 2a5ee023 Add alternate include path for openblas (#1476)
|/  
* 63d20469 fix get_num_physical_cores() (#1436)
* b5c9295e benchmark-matmul: fix clang-tidy issues, report results in GFLOPS (#1458)
* eb363627 cuda : deduplicated dequantization code (#1453)
* 79b2d5b6 ggml : alternative fix for race condition bug in non-inplace ggml_compute_forward_diag_mask_f32 (#1454)
* 13c351ad ggml : various fixes (#1450)
* 60f8c361 ggml : add AVX support based on AVX2 code (#1430)
* 601a0334 ggml : add GGML_QNT_VERSION to track quantization format changes
* 08737ef7 cuda : fix convert function (#1412)
* bda4d7c2 make : fix PERF build with cuBLAS
* 5a5aeb1e llama : fix unused warning
* 66841fdb ggml : multi-thread mul and diag_mask ops (#1428)
* 905d87b7 ggml : GPU-accelerated token generation (#1412)
* f954edda ggml : implement backward pass for llama + small training-llama-from-scratch example (#1360)
* f048af02 ggml : sync alibi fix from ggml repo
* ac0cd259 Adding SSE instructions to ggml_vec_dot_q4_0_q8_0 (#1413)
* 0cd22e19 llama : fix various warnings
* 6456a4eb embedding : remove unused code (#1426)
* cdd53508 readme : update Q4_0 perplexities
* 738ace39 llama : free ggml context in set / copy state data (close #1425)
* 699b1ad7 opencl : fix kernels for the new formats (#1422)
* fb62f924 llama : fix --mtest option (close #1414)
* 773ee249 CLI args use - instead of _, backwards compatible (#1416)
* 553fd4d4 Add clang-tidy reviews to CI (#1407)
* 089b1c93 readme : add C#/.NET bindings repo (#1409)
* b9fd7eee ggml : remove bit shuffling (#1405)
* b608b55a prompts : model agnostic DAN (#1304)
* cf348a60 main : add option to save full output to session (#1338)
* e6a46b0e Locale fix for Windows (#1379)
* 9f8dbc47 use pause asm insn in busyloop to run the CPU (13600K) 10 °C cooler (#1314)
| * a3e6d622 cuda : alternative q4_q8 kernel
| * e7b9d97b More int mult, less float mult, worse performance
| * d882d1c2 Performance no longer terrible
| * 4b128813 WAKE ME UP
| * 8a9d7ce6 fixup! Store layers in VRAM
| * 3ed4588e Store layers in VRAM
| * d052a0ed Faster than CPU without 80% runtime memcpy
| * 229aa1f5 Works but slower than CPU
|/  
* 41654efe Interface improvements and `--multiline-input` (previously `--author-mode`) (#1040)
* 56551bc1 readme : add notice about upcoming breaking change
| * e116eb63 ggml : speed-up Q5_0 + Q5_1 at 4 threads
| * ffd76e18 llama : fix model magic/version write
| * 4201fa5c llama : produce error upon loading old model files
| * 0e48eb6f ggml : uniform 5th bit extraction
| * 948d1248 AVX implementations (#1370)
| * d155f0f8 scripts : add script for measuring the time per token
| * 8fbf7777 ggml : fix Q5_0 quantization
| * 60f62bbc ggml : minor formatting
| * 7cdc08a5 ggml : remove Q4_2 mode
| * b47bd287 ggml : update cuBLAS + normalize variable names
| * c2166569 ggml : fix Q4_1 quantization
| * 4991499a ggml : remove WASM SIMD bit shuffling + remove vzip for ARM 32-bit
| * ba953d6e ggml : simplify scalar dot
| * c7af9042 ggml : remove Q5_1 bit shuffling (ARM NEON + scalar)
| * 39bb8e7d ggml : 2x faster scalar implementations
| * 796f8ae2 ggml : remove Q5_0 bit shuffling (ARM NEON)
| * a6a1d96c ggml : remove Q4_2 bit shuffling (WIP, BROKEN)
| * 086cfea1 ggml : nibbles_from_floats() + bytes_from_nibbles() (ARM NEON)
| * edb6c8bb ggml : remove Q4_1 bit shuffling (ARM NEON + reference)
| * a546dc6d ggml : remove Q4_0 bit shufling (ARM NEON)
|/  
* fe60904e readme : add TOC and Pygmalion instructions (#1359)
* 003ba2fb llama : fix hparams shadow (#1367)
* f9a63649 llama : require first token to be BOS (#1303)
* 95078cc5 convert: add ability to convert safetensors files (#1276)
* 1f48b0ab Documented CUDA reproducibility, added warning (#1346)
* e1295513 CI: add Windows CLBlast and OpenBLAS builds (#1277)
* 1b0fd454 ggml : Allow usage of CLBlast alongside Accelerate.framework (#1336)
* 39240885 Remove default arguments from sampling functions (#1343)
| * 4baa8563 Fix build
| * 2dc7fc94 + ggml-opencl.c
| * dc61b7c6 spm: link with CLBlast
| * 0e3d7fd4 Remove default arguments from sampling functions (#1343)
| * 1fa3128d Allow usage of CLBlast alongside Accelerate.framework
|/  
* 173d0e64 makefile: automatic Arch Linux detection (#1332)
* a3b85b28 ci : add cublas to windows release (#1271)
* 921dcee0 readme: add missing info (#1324)
* 2d13786e Fix for OpenCL / clbast builds on macOS. (#1329)
* a90e96b2 Convert.py @staticmethod (#1327)
* 94c5652f quantize: make output filename optional, default to ggml-model-<ftype>.bin (#1301)
* 34d9f22f Wrap exceptions in std::exception to verbose output on exception. (#1316)
* d3e8093e convert: support DT_BF16 tensors (#1309)
* 360cfe5b readme : add OpenBuddy link (#1321)
* 2edbdb0f main : add --in-suffix option (#1318)
* 20fbf2a2 ggml : change immintrin.h to intrin.h for compatibility (#1307)
* db108087 Only escape prompts when used with `-e` (#1311)
* c65a7fbf Update main's README.md with new features (#1296)
* f647ce04 fix #1224 reverse prompt and multi line (#1297)
* 799fdc1b ggml : vectorize Q8_0 quantization
| * 31ff9e2e ci : add cublas to windows release
|/  
| * 45d94c8f ci : add cublas to windows release
|/  
| * 44286d3b ci : add cublas to windows release
|/  
* 6daa09d8 examples : read chat prompts from a template file (#1196)
* bca9ad93 minor : fix whitespaces (#1302)
* e2a937ca minor : fix trailing whitespaces
* b0c71c7b scripts : platform independent script to verify sha256 checksums (#1203)
* a8a2efdc examples : various prompt and example fixes (#1298)
* e216aa04 llama : only copy used KV cache in get / set state (#1272)
* 2485d7a4 Process escape sequences given in prompts (#1173)
* 13b0c68e Handle signals properly on Windows (#1123)
* 55bc5f09 Call sh on build-info.sh (#1294)
* 9daff419 fix build-info.h for git submodules (#1289)
* bf4b22ff fix missing parameters in `llama_init_from_gpt_params` (#1293)
* 67c77799 examples : add llama_init_from_gpt_params() common function (#1290)
* 0e6cbff1 llama : fix compile warnings
* 5d5817ca ggml : fix 32-bit ARM
* 8c9be35f examples : improve vertical alignment of a few variables (#1286)
* cc0bb723 ggml : fix ppc64le build error and make cmake detect Power processors (#1284)
* 2bb992f0 llama : allow 0 as a seed number. (#1275)
* e2cd5069 main : switch input_noecho to input_echo to remove negation (#979)
* 2d099e51 ggml: add names to tensors (#1268)
* f4cef87e Add git-based build information for better issue tracking (#1232)
* 58b367c2 cuBLAS: refactor and optimize f16 mat mul performance (#1259)
* ea3a0ad6 llama : update stubs for systems without mmap and mlock (#1266)
* 2bdc0964 ggml : fix ggml_used_mem() (#1264)
* 70269cae llama : fix session load / save (#1263)
* b925f1f1 cuBLAS: fall back to pageable memory if pinned alloc fails (#1233)
* 90b19bd6 llama : let context be const when accessing const data (#1261)
* 7ff0dcd3 ggml : fix UB (int << 31)
* 6f796992 build: add armv{6,7,8} support to cmake (#1251)
* a5d30b1f common : better default number of threads (#934)
* 76a88492 ggml : add CLBlast q5_0, q5_1, q8_0 dequant kernels (#1225)
* 6bc4400e ggml : add Q5 WASM SIMD + GGML_FTYPE
* f0d70f14 Various fixes to mat_mul benchmark (#1253)
* 3e5aa8a1 ggml : fix labels for GGML_OP_ALIBI
* c3ca7a5f ggml : fix 32-bit ARM NEON
* e8c05161 ggml : use vzip instead of vuzp for consistency
* 0b5a9350 ggml : fix visibility and unused warnings
* ec728e44 ggml : fix #if for f32_f32 mul_mat (CLBlast) (#1229)
* 214b6a35 ggml : adjust mul_mat_f16 work memory (#1226)
* 305eb5af build : fix reference to old llama_util.h
* 84ca9c2e examples : fix save-load-state + rename llama-util.h
* 334637e4 common : change default parameters to pre-#1126 (#1223)
* dd7eff57 llama : new sampling algorithms (#1126)
* 7fc50c05 cuBLAS: use host pinned memory and dequantize while copying (#1207)
* b1ee8f59 cuBLAS: non-contiguous tensor support (#1215)
* 36d19a60 Remove Q4_3 which is no better than Q5 (#1218)
* 7f15c5c4 readme : update hot topics
* 55390bca ggml : sync ggml (ggml_alibi)
* 5fba3c01 examples : add Jeopardy example (#1168)
* 1481a9cf llama : add session file format and saved sessions in main (#1169)
* 11d90236 ggml : add helper debug printf in soft_max
* 7296c961 ggml : add CLBlast support (#1164)
* 78ec5437 Correcting link to w64devkit (#1214)
* 92a6e13a Add Manjaro CUDA include and lib dirs to Makefile (#1212)
* 04aaae1d add avx2 for dot_q8_0_q8_0, 2x faster than scalar (#1211)
* 0b2da205 ggml : slightly faster AVX2 implementation for Q5 (#1197)
* f9be42ad readme : add quantization info
* 574406dc ggml : add Q5_0 and Q5_1 quantization (#1187)
* 87a6f846 Allow setting the rng seed after initialization. (#1184)
* ea3ad7eb Updating build instructions to include BLAS support (#1183)
* 859fee6d quantize : use `map` to assign quantization type from `string` (#1191)
* 4afcc378 Update SHA256SUMS after quantization change (#1181)
* 667c5013 py : cast lora_alpha to int in convert-lora-to-ggml (#1170)
* bb98e77b nix: use convert.py instead of legacy wrapper convert-pth-to-ggml.py (#981)
* 7a32fcb3 ggml : add Q8_0 quantization format (rename the old one to Q8_1) (ARM NEON) (#1179)
* dd0eabc0 ggml : use full range for Q4_0 and Q4_2 quantization (#729)
* 54bb60e2 ggml : fix bug in ggml_compute_forward_sum_f32 (#1162)
* 8a0f8673 ggml : export symbols (#1155)
* 0c569234 examples : add save_load_state example (#1150)
* 957c8ae2 llama : increase scratch buffer size for 65B (ref #1152)
* 9b0a4d42 examples/main README improvements and some light refactoring (#1131)
* 2ec83428 Fix build for gcc 8 and test in CI (#1154)
* e4cf982e Fix cuda compilation (#1128)
* c4fe84fb llama : refactor get / set state + remove redundant kv cache API (#1143)
* 1d78fecd Fix LoRA acronym (#1145)
* 284685f1 scripts : add helper scripts to synch ggml repo
* edce63ba Added README.md for main with examples and explanations (#1139)
* ec9cdb67 ggml : do not print perf ops that have not been used at all
* e4422e29 ggml : better PERF prints + support "LLAMA_PERF=1 make"
* 53c84343 Improve AVX2 for vec_dot_q4_3_q8_0 (#1138)
* c6524f46 readme : update gpt4all instructions (#980)
* c9e2c26f A better `packNibbles` and `mul_sum_i8_pairs_float` implementation using AVX512 (#1119)
| * 102cd980 ggml : Q4_3c using 2x "Full range" approach
| * 71e6ae37 ggml : continue from #729 (wip)
| * bd166f7f Fix type error in quantize_row_q4_1 for Arm NEON
| * 4282f9b0 Update quantize_row_q4_1 for PowerPC
| * 93c95fcc Update quantize_row_q4_0 for Arm NEON
| * b7e70465 Update quantize_row_q4_0 for WASM
| * 5d5f2b2e Update quantize_row_q4_0 for AVX/AVX2
| * 3698f79e Use full range for q4_0 quantization
|/  
| * a0242a83 Minor, plus rebase on master
| * e435bfd9 RMSE-optimized quants for all quantization types
|/  
* 0e018fe0 ggml : fix Q4_3 cuBLAS
* 857308d1 ci : trigger CI for drafts, but not most PR actions (#1125)
* c50b6288 Fix CI: ARM NEON, quantization unit tests, editorconfig (#1122)
* 5f939498 ggml : unit test for quantization functions (#953)
* 36b4f7e0 llama : print timings on ctrl+c exit (#1021)
| * 4b8d5e38 llama : quantize attention results
|/  
* 10f19c11 llama : have n_batch default to 512 (#1091)
* 7e312f16 cmake : fix build under Windows when enable BUILD_SHARED_LIBS (#1100)
* 872c365a ggml : fix AVX build + update to new Q8_0 format
* 955ef9a5 ggml : alternative Q4_3 implementation using modified Q8_0 (#1109)
* c5aa5e57 ggml : AVX2 optimization for vec_dot_q4_3_q8_0 and refactoring (#1099)
* e9a9cb0c examples : Improve Alpaca Default Repeat Penalty: Better Match Alpaca.cpp Experience (#1107)
* b6e7f9b0 llama : add api for getting/setting the complete state: rng, logits, embedding and kv_cache (#1105)
* 50cb666b Improve cuBLAS performance by using a memory pool (#1094)
* 25d7abbd llama : fixed rlimit error message (#888)
* 018f2279 cmake : link threads publicly to ggml (#1042)
* 94112882 main : evaluate tokens in batches after swapping context (#1014)
* 8687c1f2 llama : remember and restore kv cache data pointers (#1104)
* 1bfc153e ggml : a faster version for Q4_1 x Q8_0 dot products (#1083)
* 3d59769c Show perplexity ETA in hours and minutes (#1096)
* d40fded9 llama : fix comment for "output.weight" tensor
* 2510c183 Add ggml-model-*.bin checksums for 7B, 13B, 30B, 65B (#1088)
* 12b5900d ggml : sync ggml (add GPT-NeoX RoPE implementation)
* 9ff334f3 ggml : fix bug in ggml_compute_forward_dup_f32()
* 2005469e Add Q4_3 support to cuBLAS (#1086)
* 8a1756ab ggml : do not break cuBLAS build (Q4_3 is not yet implemented)
* 66aab460 ggml : fix Q4_3 quantization
* 38de86a7 llama : multi-threaded quantization (#1075)
* e0305ead ggml : add Q4_3 quantization (#1082)
* 6a9661ea ci : remove the LLAMA_ACCELERATE matrix dimension from Ubuntu builds in the CI (#1074)
* 5addcb12 fix: LLAMA_CUBLAS=1 undefined reference 'shm_open' (#1080)
* c8c2c524 AVX2 optimization for vec_dot_q4_2_q8_0 (#1068)
* 02d69881 Improve cuBLAS performance by dequantizing on the GPU (#1065)
* 834695fe Minor: Readme fixed grammar, spelling, and misc updates (#1071)
* f7d05095 Q4_2 quantization with rmse-optimized scale and quants (#1062)
* 884e7d7a ggml : use 8-bit precision for Q4_1 intermediate results (#1047)
* 7cd5c4a3 readme : add warning about Q4_2 and Q4_3
* f3d4edf5 ggml : Q4 cleanup - remove 4-bit dot product code (#1061)
* 8944a132 Add NVIDIA cuBLAS support (#1044)
* 66674012 Multi-threaded ggml_cpy (#1035)
* 77a73403 ggml : add new Q4_2 quantization (ARM only) (#1046)
* 50a8a2af ggml : scratch that - vmlaq_n_f32 is always better
* 4caebf6d gitignore : vdot
* dcdd65e2 ggml : optimize ggml_vec_dot_q4_0_q8_0() using vectorized accumulators
* 5ecff351 Adding a simple program to measure speed of dot products (#1041)
* 7faa7460 readme : update hot topics about new LoRA functionality
* 5af8e322 ci : do not run on drafts
* 42747220 Do not close file after mmap (Windows version) (#1034)
* e9298af3 readme : add Ruby bindings (#1029)
* 4ad73137 add 4_0 to default outfile namestr dict (#1031)
* 315a95a4 Add LoRA support (#820)
* efd05648 llama : well-defined static initialization of complex objects (#927)
* eb17a026 quantize-stats : fix bug in --type argument
* 69b74028 ggml : avoid using ggml_fp16_to_fp32() and ggml_fp32_to_fp16() in ggml.c
* f266259a Speedup the AVX-512 implementation of ggml_vec_dot_q4_0() (#933)
* 47f61aaa Fix: do not close file on mmap (#1017)
| * 15067374 Add mmap pages stats (disabled by default)
|/  
* 3173a62e stdout : vertical align outputs for better readibility
* 489537e6 examples: add missing <ctime> include for time() (#1011)
* 2d3481c7 Fix msys2 build error and warnings (#1009)
* 74f5899d convert.py: Fix loading safetensors and ggml format on Windows (#991)
* 2f7c8e01 Fix potential int8 overflow in non-SIMD vec_dot (#986)
* 0ad96463 Refactor ggml.c for future tensor types (#1001)
* e95b6554 ggml : add Q8_0 quantization for intermediate results (#951)
* aa485cee ggml : use posix_memalign on non-Windows env
* c12b14b7 benchmark : fix result validation in benchmark-q4_0-matmult (#987)
* 106faaf2 cmake : add finding the OpenBLAS header file (#992)
* c85e03d1 Revert "main : alternative instruct mode (Vicuna support, etc.) (#863)" (#982)
* 48909354 py : bump sentencepiece to 0.1.98 to support Python 3.11 (#976)
* 93265e98 make : fix dependencies, use auto variables (#983)
* c56b7152 Expose type name from ggml (#970)
* f4d277ae main : alternative instruct mode (Vicuna support, etc.) (#863)
* c9a59b70 ggml : add unary and binary map operations (#874)
* a32f7acc py : cleanup dependencies (#962)
* 43ffdefb py : fix flake8 and isort nitpicks (#960)
* 1623a6e9 ggml : minor
* c14e0d2f ggml : always allocate buffers with size multiple of GGML_MEM_ALIGN
* 723dac55 py : new conversion script (#545)
* 0f07cacb ggml : fix q4_1 dot product types
* c5d70f5c ggml : optimize rope function to avoid call powf in the tight loop (#807)
* be87b6ed perplexity : add support for batch size to `--perplexity` (#407)
* 0e07e6a8 common : remove unnecessary includes (#947)
* a3a2a0ed ggml : add GGML_DEFAULT_N_THREADS
* d990e3ff ggml : speed-up ggml_vec_dot_q4_1() ARM_NEON + 32-bit ARM support (#900)
* 9190e8ea llama : merge llama_internal.h into llama.h
* c85980ac gitignore : benchmark
* 6232f2d7 ggml : optimize non-SIMD Q4_0 vector dot product (#703)
* 6c248707 ggml : introduce GGML_ALIGNED_MALLOC/GGML_ALIGNED_FREE macros (#884)
* 8cda5c98 fix whitespace (#944)
* ec292721 readme : remove python 3.10 warning (#929)
* 7e941b95 readme : llama node binding (#911)
* c729ff73 flake.nix: add all binaries from bin (#848)
* 4579af95 zig : update build.zig (#872)
* 8c3ffc2f ggml : update cblas_sgemm columns var to be more reasonable (#838)
* 107980d9 examples : add -n to alpaca and gpt4all scripts (#706)
* 585d91a1 cmake : add explicit F16C option (x86) (#576)
* 95ea26f6 benchmark : add tool for timing q4_0 matrix multiplication (#653)
* 82d146df do not force the prompt file to end with a new line (#908)
* e7f6997f Don't crash on ftype (formerly f16) == 4 (#917)
* f76cb3a3 readme : change "GPU support" link to discussion
* 78243807 readme : update hot topics with link to "GPU support" issue
* 4dbbd407 readme: link to sha256sums file (#902)
* 8b679987 Fix whitespace, add .editorconfig, add GitHub workflow (#883)
* 3e6e70d8 Add enum llama_ftype, sync ggml_type to model files (#709)
* 2663d2c6 Windows fixes (#890)
* a0caa34b Add BAIR's Koala to supported models (#877)
* 461ba9e6 ggml : fix WASM build
* c3ac702e ggml : add ggml_cont() + optimize ggml_cpy() for contiguous dst
* 9d634ef4 ggml : remove trailing whitespaces
* d9a239c4 Simplify to include lower-case windows.h always, fix compile on mingw32 (#747)
* 684da259 ggml : fix quantize_row_q4_1() ARM_NEON (close #876)
* 180b693a Print model version.
* f963b63a Rewrite loading code to try to satisfy everyone:
* aaf3b23d fix for windows utf-8 input (#840)
* f2d1c472 cmake should link openblas properly with -lopenblas like how it's done in the makefile (#839)
* 317fb12f Add new binaries to flake.nix (#847)
* 62cfc54f Add quantize-stats command for testing quantization (#728)
* 698f7b5d make : add libllama.so target for llama-cpp-python (#797)
* c1950c34 zig : don't link examples/common.cpp for non-example (#814)
* 4953e900 llama : always sort logits before nucleus sampling (#812)
* cc9cee8e Do not crash when it has nothing to say. (#796)
* d2beca95 Make docker instructions more explicit (#785)
* eeaa7b04 ggml : multi-thread ggml_rope() (~3-4 times faster on M1) (#781)
| * 36ddd129 llama : add flash attention (demo)
|/  
* 986b6ce9 ggml, llama : avoid heavy V transpose + improvements (#775)
* 34162989 Update README.md
* 5a8c4f62 llama : define non-positive top_k; top_k range check (#779)
* ff05d05c miku.sh : add executable bit (#780)
* 62b3e81a media : add logos and banners
* 8d10406d readme : change logo + add bindings + add uis + add wiki
* ed1c214e zig : add build.zig (#773)
* 0c44427d make : missing host optimizations in CXXFLAGS (#763)
* 594cc95f readme : update with CMake and windows example (#748)
* 88ed5761 examples : add Miku.sh (#724)
* 58c438cf Add Accelerate/BLAS when using Swift (#765)
* 53dbba76 Windows: reactive sigint handler after each Ctrl-C (#736)
* 437e7785 10+% performance improvement of ggml_vec_dot_q4_0 on AVX2 (#654)
* cd7fa956 Define non-positive temperature behavior (#720)
* a0c05164 Remove torch GPU dependencies from the Docker.full image (#665)
* d8d4e865 Add a missing step to the gpt4all instructions (#690)
* e986f948 Added api for getting/setting the kv_cache (#685)
* c0bb1d3c ggml : change ne to int64_t (#626)
* 6e7801d0 examples : add gpt4all script (#658)
* 81040f10 llama : do not allocate KV cache for "vocab_only == true" (#682)
* c4f89d8d make : use -march=native -mtune=native on x86 (#609)
* 5b70e7de fix default params for examples/main (#697)
* a717cba8 py: huggingface -> Hugging Face (#686)
* d0a7f742 readme: replace termux links with homepage, play store is deprecated (#680)
* 0d054e29 Show error message when -f fails
* 35258992 Enable -std= for cmake builds, fix warnings (#598)
* 1d08882a Optimize AVX2 ggml_vec_dot_q4_0 (#642)
* 02c5b27e Add AVX acceleration (#617)
* cbef5428 py : cleanup the code
* 9733104b drop quantize.py (now that models are using a single file)
* 3df890ae readme : update supported models
* ee0c40dd Introduce GGML migration tool for new file format
* 6f23ba5e Ensure --mlock works properly with mmap() support
* 78ca9838 Make loading weights 10-100x faster
* a0173903 Initial windows support (untested)
* ac184d51 Always initialize mm_addr and mm_length in llama_model
* 276e5b78 Unmap the file in llama_free
* d68c5dc4 Make mmap_file static
* 64bde3ff Fix ggml_init_params in quantize
* c03ae8dc Add mmap support for model files
* 3bcc129b cmake : properly invoke CTest (#629)
* a4755cf2 Remove unused variable (#607)
* 1f0414fe make : fix darwin f16c flags check (#615)
* 77efdf5a ggml : fix NEON signs (close #620, #622)
* ed3c680b Fix GGML_F32Cx8_STORE in AVX without F16C path (#619)
* 9cbc404b ci : re-enable AVX512 testing (Windows-MSVC) (#584)
* b51c717d ggml : init time on first ggml_init() call
* 0ba76c1e llama : fix compile warnings when reading the vocab
* cea1c859 ggml : add ARM_NEON dequantize_row_q4_1()
* f202ada1 ggml : add ARM_NEON quantize_row_q4_1()
* 3b44d30d ggml : add ARM_NEON ggml_vec_dot_q4_1()
* 61cbfff5 rename convert_ggml_to_pth.py -> convert-ggml-to-pth.py (#600)
* d9ad1044 Create chat-13B.bat (#592)
* b467702b readme : fix typos
* 516d88e7 readme : add GPT4All instructions (close #588)
* 53635c08 py : add GPT4All conversion script
* 41318d70 llama : use the same threshold for OpenBLAS and ggml thread limiting (#577)
* a6956b25 add example of re-act pattern (#583)
* 83df5639 Fix GCC warning about binary literal (#595)
* a5c42c4b Fix typo in llama.h (#593)
* 5a5f8b15 Enable Fused-Multiply-Add (FMA) and F16C/CVT16 vector extensions on MSVC (#375)
* f1217055 CI: fix subdirectory path globbing (#546)
* 7f4c5c66 llama : fix linkage with mingw (#551)
* 2a98bc18 ggml : add AVX2 implementation of quantize_row_q4_1 (#515)
* d0aaff57 py : add temporary script to convert old ggml files to newer version (#539)
* d0330fd7 py : add capabiliy to convert from ggml back to torch or hf format for further consumption/training/finetuning (#403)
* 99c5b276 ggml : refactor quantized processing functions (#509)
* 692ce316 py : removed unused `model` variable and verified that the code functions correctly with `vocab_only` setting. Also confirmed that the code works as expected after running with reduced memory usage due to deletion of no-longer-needed variable. (#547)
* 96f9c050 ci : make ctest verbose, hopefully we see what is wrong with the sanitizer
* d502bc7c tests : free llama context at the end of the test
* 436e5619 all : be more strict about converting float to double (#458)
* 20e1e848 deploy : add a Package.swift for SwiftPM support (#393)
* c1f88506 ggml : introduce structs for the q4 data blocks (#356)
* e0670260 gitignore : add "embedding"
* 28ba975a Check the existence of f16_model_path_base in quantize.py (#574)
* a6bdc47c Fix usage of F16C intrinsics in AVX code (#563)
* 7b8dbcb7 main.cpp fixes, refactoring (#571)
* 4b8efff0 Add embedding example to Makefile (#540)
* 7e539557 Fix missing ggml link in cmake for examples/* on w64-mingw32 (#542)
* 34c1072e ci: add debug build to sanitizer build matrix (#527)
* 939ad2d3 Fix undefined variables in debug build, remove unused variables (#531)
* 8c2ec5e2 Add support for linux/arm64 platform during Docker Builds (#514)
* b391579d Update README and comments for standalone perplexity tool (#525)
* 7a87d31f [main] fix infinite generation (-n == -1) (#523)
* 348d6926 Add logo to README.md
* 33e35b8f Exit from interactive mode if input stream is bad (#491)
* 19726169 CI: Run other sanitizer builds even if one fails (#511)
* f732695c Clarify console output in convert-pth-to-ggml.py (#512)
* 2f7bf7dd CMake / CI additions (#497)
* 34ab5268 (Windows) Set console to UTF-8 on init (#420)
* c2b25b69 Fix colors enabling on WIN32
* 79b2b266 If n_predict == -1, generate forever
* e2d490da Inifinite generation via context swapping (#71)
* 03f7e335 Cleanup STL headers + fix embedding examples + minor stuff
* 55ad42af Move chat scripts into "./examples"
* 459e93cc Add AVX2 implementation of dequantize_row_q4_1 (#505)
* a316a425 Overhaul the examples structure
* ecbe466a Retire the ggml_mul_mat() branch for transposed src0 (#500)
* 502a4001 Disable prompt verbosity by default and add option to enable (#480)
* 09aecbf6 Add AVX2 implementation of dequantize_row_q4_0 (#467)
* 4640eff2 Don't interefe with BLAS for large prompts by running only 1 thread
* ab77d763 Add longer DAN prompt for testing big batch numbers
* 29b7baab Add timings for the prompt evaluation (#478)
* 4a7129ac Remove obsolete information from README
* 6b6dbc89 Remove obsolete assert and fix compiler warning
* 2a2e63ce Fix nasty bug in ggml_compute_forward_mul_mat_f32() and reenable BLAS
* e899bf54 bounds checking for input prefix (#492)
* fbd4d38c feat: '--in-prefix STRING' option (#426)
* 58e6c9f3 Add support for file load progress reporting callbacks (#434)
* 36d07532 Add missing struct annotation (#483)
* 6f1ee4b6 Fix crash for 65B model with pre-allocated memory (#485)
* 8520fc31 Disable BLAS altogether - the bug is not just for qunatized mat mul
* b3f460e9 Disable BLAS branch in mul_mat - seems there is a bug
* 04c6f5ed Immediately start processing the prompt before user input has been provided (#476)
* 7a9b6c3a Reduce memory usage and allocate enough memory for largest context (#473)
| * c9c820ff Added support for _POSIX_MAPPED_FILES if defined in source (#564)
| * 1a5ee113 Restore old -std= flags
| * 16312984 Remove -std=foo compiler flags
| * cbddf466 Get mmap() working with WIN32 MSVC
| * e4881686 Make WIN32 mmap() improvements (#341)
| * 0b5448a3 Implement system polyfill for win32 / posix.1
| * 5b8023d9 Implement prototype for instant mmap() loading
| * 2788f373 Get the build working
| | * 4aeee216 Regroup q4_1 dot addition for better numerics.
| | * 580991bb Squeeze out about 5% more performance in Q4_1 inference
| |/  
|/|   
* | 31572d96 Temporary bump the memory buffer size - hopefully fix issues from 483bab2e
* | f4f5362e Update README.md (#444)
* | 863f65e2 fix instruct mode (#445)
* | afd220d9 Properly free llama_context on failure
* | 481044d5 additional optimizations for POWER9 (#454)
* | 563cdc39 Support calling mlock() on loaded model data on Linux and macOS (#453)
* | 8d4a855c Add embedding mode with arg flag. Currently working (#282)
* | b6b268d4 Add link to Roadmap discussion
* | 3cd8dde0 Revert "Fix memory allocation issues and seg faults"
* | 4870e455 Fix memory allocation issues and seg faults
* | 483bab2e Avoid the transposed X branch in the Z = X * Y matrix multiplication (#439)
* | 404e1da3 Fix quantize script not finding models in parent directory (#428)
* | 4cc053b6 Remove oboslete command from Docker script
* | 0ba5a3a9 Obsolete
* | 2e17dfd8 Replace EOS with newline to prevent context/memory being flushed by EOS in interactive mode (#333)
* | 20a1a4e0 Fix GPTQ converter (#423)
* | ad072fc5 Generate library with CMake (#430)
* | ea10d3de Command line args bounds checking (#424)
* | a18c1925 Fix Nix build
* | a50e39c6 Revert "Delete SHA256SUMS for now" (#429)
* | a140219e Fix Makefile echo escape codes (by removing them). (#418)
* | 8a3e5ef8 Move model section from issue template to README.md (#421)
* | 8eea5ae0 Delete SHA256SUMS for now (#416)
* | 93208cfb Adjust repetition penalty ..
* | 03ace14c Add link to recent podcast about whisper.cpp and llama.cpp
* | e4412b45 CI: CMake: Separate build and test steps (#376)
* | f7dc43bc Fix instruct mode broken by PR #354 (#409)
| | * 66ea164e Kahan summation on Q4_1
| | | * 71122470 Break up loop for numeric stability
| | |/  
| | * 69071d3b Squeeze out about 5% more performance in Q4_1 inference
| |/  
|/|   
* | ee8a7887 Update issue template so people will use it (#404)
| | * 3a0dcb39 Implement server mode.
| | * bf44faa0 Remove direct access to std streams from "run"
| | * b7f1fa6d Move llama_context setup + perplexity back to main.cpp
| | * d7d53b84 Add main.cpp back and invoke "run" from it
| | * 90175ee1 Move main.cpp to run.cpp
| |/  
|/|   
* | 69c92298 Deduplicate q4 quantization functions (#383)
* | 97940520 fix: add POSIX functionality for Linux compilation (#51)
* | 305ba6f0 Don't force immediate interactive without `-i` (#354)
* | 4122dfff cmake: make llama an actual library (#392)
* | 56e659a0 fix perplexity after c-api refactor (#390)
* | 40ea807a Add details on perplexity to README.md (#395)
* | d5850c53 Add missing header for memcpy (#386)
* | ae44e23e When seed <= 0 - use the clock to generate one
* | 928480ef Init llama_context_params properly from CLI (#370)
* | 56817b1f Remove temporary notice and update hot topics
* | f5a77a62 Introduce C-style API (#370)
* | da0e9fe9 Add SHA256SUMS file and instructions to README how to obtain and verify the downloads
* | e6c9e098 Fix bin dir for win ci
* | 01a297b0 specify build type for ctest on windows (#371)
* | 3366853e Add notice about pending change
* | 3f9c6135 fix typo in chatLLaMa (#368)
* | 0f613527 Update issue templates
* | 353ec251 We could use std::unordered_map over std::map (#305)
* | 89d5d90f Fix color codes emitting mid-UTF8 code. (#312)
* | 16ffc013 Importer for GPTQ quantized LLaMA models (#301)
* | 486ae645 Compute perplexity over prompt (#270)
* | 3ab3e658 Add chatLLaMa script (#198)
* | f157088c makefile: Fix CPU feature detection on Haiku (#218)
* | c86ba036 Enable ANSI colors on Windows 10+ (#311)
* | 1daf4dd7 Minor style changes
* | dc6a845b Add chat.sh script
* | 6a612959 Check for reverse prompt by characters instead of tokens (#292) (#330)
* | d5f56a5e Check for reverse prompt by characters instead of tokens (#292) (#330)
* | 3bfa3b43 Fix convert script, warnings alpaca instructions, default params
* | 715d292e Add OpenBSD support (#314)
* | c98ae026 fix typo in comment (#318)
* | c3b2306b Makefile: slightly cleanup for Mac Intel; echo instead of run ./main -h (#335)
* | 975d2ceb cmdline option for custom amount of model parts (--n_parts N) (#348)
* | e0ffc861 Update IPFS links to quantized alpaca with new tokenizer format (#352)
* | 8f644a0a Change default repeat_penalty to 1.0
* | eb34620a Add tokenizer test + revert to C++11 (#355)
* | 2e664f1f Add initial AVX512 support for dot product on Linux (#320)
* | 8cf9f34e Adding missing features of CMakeLists.txt & Refactoring (#131)
* | bd4b46d6 Nix flake: set meta.mainProgram to llama
* | 6b6d5b50 Fixed tokenizer.model not found error when model dir is symlink (#325)
* | a791a68b move file magic/version to header, print expected version (#319)
* | 0f1b21cb Docker - Fix publish docker image in GitHub Registry (#235)
* | 074bea2e sentencepiece bpe compatible tokenizer (#252)
* | 5cb63e24 Add tqdm to Python requirements (#293)
* | da5303c1 bugfix: default should not be interactive (#304)
* | 4545539d Rename script
* | edeba283 Add temporary helper script for Alpaca chat
* | 5c19c70b fix coloring of last `n_batch` of prompt, and refactor line input (#221)
* | 24568371 Support for multiple reverse prompts. (#299)
* | 7392f1cd Improved quantize script (#222)
* | ad5fd5b6 Make prompt randomization optional. (#300)
* | 368d0c8a Respect the maximum number of tokens in interactive. (#298)
* | 50fae10d Add --ignore-eos parameter (#181)
* | 084e2f0e interactive mode: print '\n' in sigint_handler, this flush stdout thus ensure color reset. (#283)
* | 0b366e73 Command line switch to use F16 for memory_k and memory_v (refactor of #154) (#294)
* | 160bfb21 Update hot topics to mention Alpaca support
* | c494ed5b Fix off-by-one bug (#115)
* | c1c7026b Fix python stuff (#109)
* | 467b1497 Refactoring `convert-pth-to-ggml.py`: more concise and readable (#109)
* | 70f01cb8 Drop trailing new line from file prompts (#80)
* | a4e63b73 Add instruction for using Alpaca (#240)
* | 9e170721 Add "--instruct" argument for usage with Alpaca (#240)
* | 22213a17 Change RMSNorm eps to 1e-6 (#173)
* | d7def1a7 Warn user if a context size greater than 2048 tokens is specified (#274)
* | 6f61c18e Fix typo in readme
* | 1e5a6d08 Add note about Python 3.11 to readme
* | 554b5415 Add memory/disk requirements to readme
* | d3f202d5 Remove unused code since n_vocab is model.hparams.n_vocab (#262)
* | e03e3597 fixed warning with std::ignore about unused function result (#151)
* | a81d0c2a Fix n^2 loop in tokenization (#254)
* | b2de7f18 CI Improvements (#230)
* | a2927478 Nix flake (#40)
* | c9f670a1 Implement non-greedy tokenizer that tries to maximize token lengths (#242)
* | 4f546091 Default to 4 threads (#243)
* | e81b9c81 Update Contributing section
* | 367946c6 Don't tell users to use a bad number of threads (#243)
* | 6b0df5cc add ptread link to fix cmake build under linux (#114)
* | 2af23d30 🚀 Dockerize llamacpp (#132)
* | 904d2a8d Q4_1 quantization (#193)
* | 72131107 Update README.md
* | ac15de78 Expand "Contributing" section
* | 273abc47 Update hot topics - RMSnorm
* | 9b4a15b1 Fix RMS norm in GGML (#191)
* | 6eac39ba Add RMS norm and use it (#187)
* | 27944c42 fixed typo (#178)
* | 2d15d6c9 add SIGINT support for _WIN32 environments (#120)
* | 2d64715a added ctx_size parameter (#148)
* | 16b2c61a fixed color reset on exit (#149)
* | 977295c7 Fix potential licensing issue (#126)
* | 956dfda8 Use `tokenizer.vocab_size()` instead of hardcoding 32000 in convert-pth-to-ggml.py (#142)
* | 113e685d inline -> static inline for "bytesFromNibbles" (#161)
|/  
* 47857e56 Don't use vdotq_s32 if it's not available (#139)
* 60f819a2 Add section to README on how to run the project on Android (#130)
* 97ab2b25 Add Misc section + update hot topics + minor fixes
* 2f700a27 Add windows to the CI (#98)
* c09a9cfb CMake build in Release by default (#75)
* 7ec903d3 Update contribution section, hot topics, limitations, etc.
* 4497ad81 Print system information
* ed6849cc Initial support for CMake (#75)
* 41be0a3b Add NetBSD support. (#90)
* 671d5cac Use fprintf for diagnostic output (#48)
* 84d9015c Use vdotq_s32 to improve performance (#67)
* 63fd76fb Reduce model loading time (#43)
* 2a20f48e Fix UTF-8 handling (including colors) (#79)
* d1f22471 Add quantize script for batch quantization (#92)
* 1808ee05 Add initial contribution guidelines
* a169bb88 Gate signal support on being on a unixoid system. (#74)
* 460c4825 Fix token count accounting
* c80e2a8f Revert "10% performance boost on ARM"
* 54a0e66e Check for vdotq_s32 availability
* 543c57e9 Ammend to previous commit - forgot to update non-QRDMX branch
* 113a9e83 10% performance boost on ARM
* 404fac0d Fix color getting reset before prompt output done (#65)
* 1a0a7430 Update README.md
* 96ea727f Add interactive mode (#61)
* 96619548 Fix typo in README (#45)
* f385f8de Allow using prompt files (#59)
* 02f0c6fe Add back top_k (#56)
* eb062bb0 Windows fixes (#31)
* 7027a978 Update README.md
* 2d555e5b Add CI (#60)
* 7c9e54e5 Revert "weights_only" arg - this causing more trouble than help
* b9bd1d01 python/pytorch compat notes (#44)
* 129c7d1e Add repetition penalty (#20)
* 702fddf5 Clarify meaning of hacking
* 7d86e25b README: add "Supported platforms" + update hot topics
* a9312023 use weights_only in conversion script (#32)
* 6a9a67f0 Add LICENSE (#21)
* da1a4ff0 Update README.md
* 6b2cb630 Fix a typo in model name (#16)
* 4235e3d5 Update README.md
* f1eaff47 Add AVX2 support for x86 architectures thanks to @Const-me !
* a9e58529 Fix un-initialized FP16 tables on x86 (#15, #2)
* 7d9ed7b2 Bump memory buffer
* 0c680332 Update README.md
* f60fa9e5 .gitignore models/
* 7211862c Update Makefile var + add comment
* a5c5ae2f Update README.md
* ea977e85 Update README.md
* 007a8f6f Support all LLaMA models + change Q4_0 quantization storage
* 5f2f970d Include Python dependencies in README (#6)
* 73c6ed5e Update README.md
* 01eeed8f Update README.md
* 6da2df34 Update README.md
* 9dcf4dba Add missing headers for memcpy and assert (#3)
* 920a7fe2 Update README.md
* 3a57ee59 Update README.md
* b8502852 Update README.md
* 8a01f565 Update README.md
* 70bc0b8b Fix a bug in the rope calculation
* 18ebda34 Update README.md
* 319cdb3e Final touches
* 77532806 Create README.md
* 26c08466 Initial release
